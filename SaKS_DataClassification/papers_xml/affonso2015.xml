<article>
  <title>Biological image classification using rough-fuzzy artificial neural network</title>
  <abstract>
    <sentence>This paper presents a methodology to biological image classification through a Rough-Fuzzy Artificial Neural Network (RFANN).</sentence>
    <sentence>This approach is used in order to improve the learning process by Rough Sets Theory (RS) focusing on the feature selection, considering that the RS feature selection allows the use of low dimension features from the image database.</sentence>
    <sentence>This result could be achieved, once the image features are characterized using membership functions and reduced it by Fuzzy Sets rules.</sentence>
    <sentence>The RS identifies the attributes relevance and the Fuzzy relations influence on the Artificial Neural Network (ANN) surface response.</sentence>
    <sentence>Thus, the features filtered by Rough Sets are used to train a Multilayer Perceptron Neuro Fuzzy Network.</sentence>
    <sentence>The reduction of feature sets reduces the complexity of the neural network structure therefore improves its runtime.</sentence>
    <sentence>To measure the performance of the proposed RFANN the runtime and training error were compared to the unreduced features.</sentence>
  </abstract>
  <keywords>
    <keyword>Image identification</keyword>
    <keyword>Feature selection</keyword>
    <keyword>Rough sets</keyword>
    <keyword>Fuzzy sets</keyword>
    <keyword>Artificial neural network</keyword>
  </keywords>
  <section name="Introduction">
    <sentence>In complex problems as biological cells image classification, the capture of the essential features must be carried out without a priori knowledge of the image.</sentence>
    <sentence>The increased amount of attributes requires computational complexity and runtime even bigger.</sentence>
    <sentence>Moreover, due to noise in the database caused by excessive image features can cause a reduction in capacity of representation.</sentence>
    <sentence>According to Shang and Qiang (2008), the employment of Rough-Fuzzy features selection mechanism allows the reduction for a low dimensionality features sets from samples descriptions.</sentence>
    <sentence>For these complex cases from the real life the use of Rough Sets (RS) in the pre-processing of the database has been efficient, since only the most relevant features are used as input parameters for the neural network.</sentence>
    <sentence>The RS has recently emerged as another major mathematical approach for managing uncertainty that arises from inexact, noisy, or incomplete information.</sentence>
    <sentence>It is found to be particularly effective in the area of knowledge reduction (Petrosino &amp; Salvi, 2006).</sentence>
    <sentence>In these cases, Fuzzy Set theory (FS) and RS represent two different approaches to vagueness.</sentence>
    <sentence>FS addresses gradualness of knowledge, expressed by the fuzzy membership, whereas rough set theory addresses granularity of knowledge, expressed by the indiscernibility relation (Affonso &amp; Sassi, 2010).</sentence>
    <sentence>An option to simplify the structure of the Artificial Neural Network (ANN) and reduce the noise caused by non-significant features is to use the Rough Set (RS) approach in order to select the most important features.</sentence>
    <sentence>The present paper proposes a new algorithm to realize the feature selection, with the intention to use RS as a tool for structuring the ANN.</sentence>
    <sentence>The methodology consisted of generating rules from training examples by rough-set learning, and mapping the dependency factors of the rules into the connection weights of a four-layered neural network.</sentence>
    <sentence>The advantage of the Rough-Fuzzy Artificial Neural Network (RFANN) approach consists in the synergy achieved by combining two or more technical capabilities to achieve a more powerful system regarding to learning and generalization (Gomide, Figueiredo, &amp; Pedrycz, 1998).</sentence>
    <sentence>A sequential architecture is used in this work, in which RS and the FS have distinct functions: RS identifies the most critical features, while the FS generates the surface response (input, output) since the Neuro Fuzzy Network (NFN) has Learnability and can adapt itself to the real world.</sentence>
    <sentence>The paper is organized as follows: Section 2 presents the Literature review, Section 3 presents the Experimental Methodology and Section 4 presents the Conduct of Experiments.</sentence>
    <sentence>The Conclusion is presented in Section 5.</sentence>
  </section>
  <section name="Literature review">
    <sentence>Techniques can be combined to obtain a more powerful system in terms of interpretation, learning, parameter estimation, generalization, and less disability as well.</sentence>
    <sentence>Thus, various combinations have been applied in different papers generating systems based on: Fuzzy Min–Max Neural Network, Regression Tree and the Random Forest model as a decision support tool for medical data classification (Seera &amp; Lim, 2014), an hybrid evolutionary dynamic Neural Network for stock market trend analysis and prediction using unscented Kalman filter (Bisoi &amp; Dash, 2014), credit risk evaluation using Multi-Criteria Optimization classifier with Kernel, Fuzzification and Penalty Factors (Zhang, Gao, &amp; Shi, 2014), a novel Support Vector Machine model combining Kernel Principal Component Analysis with Genetic Algorithm is proposed for intrusion detection (Kuang, Xu, &amp; Zhang, 2014), two independent hybrid mining algorithms to improve the classification accuracy rates of Decision Tree and Naïve Bayes classifiers for the classification of multi-class problems (Farid, Zhang, Rahman, Hossain, &amp; Strachan, 2014), a novel Fuzzy Hybrid Quantum Artificial Immune Clustering algorithm based on cloud model (Zhang, Shan, Liu, &amp; Zhang, 2014), an optimization approach based on the Ordinal Optimization Philosophy and Particle Swarm Optimization is used to search in the continuous space of the operational variables (Zhang, Chiang, &amp; Wu, 2014) and a Local Least-Squares Support Vector Machines-Based Neuro-Fuzzy Model for Nonlinear and Chaotic Time Series Prediction (Miranian, &amp; Abdollahzade, 2013).</sentence>
    <sentence>Combined techniques can also be applied to the identification, treatment and processing of images, in generating systems based on: extreme Learning Machine and Sparse Representation based classification method, have attracted significant attention due to their respective performance characteristics in computer vision and pattern recognition (Luo &amp; Zhang, 2014), a Neural-AdaBoost based facial expression recognition system (Owusu, Zhan, &amp; Mao, 2014), Artificial Bee Colony approach to information granulation-based Fuzzy Radial Basis Function Neural Networks for image fusion (Yu &amp; Duan, 2013), a novel Multi-Instance Learning algorithm based on Multiple-Kernels Framework has been proposed for image classification (Li, Wang, Zhao, Liu, &amp; Wang, 2014), Fuzzy-Rough feature selection aided Support Vector Machines for Mars image classification (Shang &amp; Barnes, 2013), Rough Sets and Near Sets in Medical Imaging (Hassanien, Abraham, Peters, Schaefer, &amp; Henry, 2009), Implementation and comparative analysis of Rough Set, Artificial Neural Network and Fuzzy-Rough classifiers for Satellite image classification (Juneja, Walia, Sandhu, &amp; Mohana, 2009) and an Analysis of Clustering Algorithms for MR Image Segmentation using IQI (Patel &amp; Patnaik, 2012).</sentence>
    <sentence>Hybrid techniques have been applied to biological images, generating systems based on: Expert System Approach to the Identification and Clustering of Features of Biological Images (Jordan &amp; Perkins, 1988), Artificial Neural Networks for Classification and Identification of Data of Biological Tissue Obtained by Mass-Spectrometry Imaging (Xiong et al., 2012), Multi-objective Nature-Inspired Clustering and Classification Techniques for image segmentation (Bong &amp; Rajeswari, 2011), Evolutionary Artificial Neural Network Design and Training for wood veneer classification (Castellani &amp; Rowlands, 2009), Image Segmentation Algorithms applied to wood defect detection (Funck, Zhong, Butler, Brunner, &amp; Forrer, 2003), a new Neuro-Fuzzy method to investigate the characteristics of the facial images (Diago, Kitaoka, Hagiwara, &amp; Kambayashi, 2011), Rough Sets combined with various other methodologies such as Neural Networks, Wavelets, Mathematical Morphology, Fuzzy Sets, Genetic Algorithms, Bayesian Approaches, Swarm Optimization and Support Vector Machines in the image processing domain (Hassanien, Abraham, Peters, &amp; Schaefer, 2008); Rough Set frameworks hybridized with other Computational Intelligence Technologies that include Neural Networks, Particle Swarm Optimization, Support Vector Machines and Fuzzy Sets (Hassanien et al., 2009).</sentence>
    <sentence>Image identification It takes a long time to train a person to be competent in wood identification.</sentence>
    <sentence>Furthermore, manual examination of the wood sample can be very subjective.</sentence>
    <sentence>In addition to the macroscopic features of wood, physical features such as weight (different moisture content), color (variation), odour, hardness, texture, and surface appearances are also considered.</sentence>
    <sentence>For unknown specimen, usually dichotomous keys are used on a systematic analytical procedure for the examination of the wood structure.</sentence>
    <sentence>The identity of the tree in the forest can be easily known by examining their flowers, fruits and leaves.</sentence>
    <sentence>However, once the tree is felled, the identification of the tree becomes very difficult and has to rely on their physical, macroscopic and microscopic features for identification.</sentence>
    <sentence>In this research, an intelligent recognition system using low cost equipment for the identification of wood species based on the macroscopic features of wood has been designed (Pham, Soroka, Ghanbarzadeh, &amp; Koc, 2006).</sentence>
    <sentence>The image processing techniques are widely used for classification and clustering of plant cells.</sentence>
    <sentence>In most cases, the biological classification is performed by trained operators, but this solution suffers significant disadvantages, so the literature contains several papers in which neural networks are used in image processing plant cells (He, 1997; Khalid, Lee, Yusof, &amp; Nadaraj, 2008; Marzuki, Eileen, Rubiyah, &amp; Miniappan, 2008; Pham et al., 2006; Topalova &amp; Tzokev, 2011), also for prediction of fracture toughness (Dassanayake, 2000; Samarasinghe, Kulasiri, &amp; Jamieson, 2007).</sentence>
    <sentence>Rough set theory (RS) RS was proposed by Zdzislaw Pawlak in 1982 (Pawlak, 1982) as a mathematical model to represent knowledge and to treat uncertainty.</sentence>
    <sentence>An important concept in RS is the reduct.</sentence>
    <sentence>Fig 1.</sentence>
    <sentence>Full process of the Rough-Fuzzy Artificial Neural Network (RFANN).</sentence>
    <sentence>A reduct is a minimal set of attributes that can represent an object with the same accuracy as the original set of attributes.</sentence>
    <sentence>Elimination of redundant attributes can help in the identification of strong, non-redundant classification rules.</sentence>
    <sentence>A reduct of B – RED(B) – on information system (IS) is a set of attributes B’⊆ B such that all attributes a ∈ (B – B’) are dispensable.</sentence>
    <sentence>Thus, U/INDs(B’) =U/INDs(B), where INDs(B) is called the Indiscernibility Relation.</sentence>
    <sentence>Computing the reduct is an n-p hard problem, and processing the reduct for large databases requires high computational processing.</sentence>
    <sentence>The reduct is generated by discernibility from the Discernibility Matrix.</sentence>
    <sentence>The Discernibility Matrix of information systems S, denoted DM(B), is a symmetric n × n matrix with: mD(i, j) = {a ∈ B | a(Ei) ≠ a(Ej)} for i,j =1,2,…,n.</sentence>
    <sentence>with 1≤i, j≤n e n=| U / INDs(B)}.</sentence>
    <sentence>Thus, the elements of the Discernibility Matrix mD(i,j) are a set of conditional attributes of B that differentiate the elements of classes in relation to their nominal values.</sentence>
    <sentence>The reducts of S are generated through the simplification methods of Boolean functions for the Fs(B) function (1).</sentence>
    <sentence>This simplification is an algebraic approximation of the logical functions, with the goal of reducing the number of attributes.</sentence>
    <sentence>(1) With: The discernibility function Fs(B) is obtained as follows: for all attributes represented by an element in the Discernibility Matrix MD(B), apply the sum operator (“or” or “∨”) and, for each pair of cells in this matrix, apply the “product” element (“and” or “∧”), which results in a Boolean expression of “sum of products”.</sentence>
    <sentence>Fuzzy Sets concern membership among elements from the same class, while RS concerns the relationship between groups of elements in different classes.</sentence>
    <sentence>However, the theory of RS does not compete with the Fuzzy Sets Theory but rather complements it.</sentence>
    <sentence>In fact, RS theory and Fuzzy Sets theory are two independent approaches for the treatment of imprecise knowledge.</sentence>
    <sentence>The knowledge acquisition bottleneck is a significant problem that hinders the building of intelligent monitoring systems.</sentence>
    <sentence>The generation of good knowledge bases for this task is notoriously difficult.</sentence>
    <sentence>This problem is particularly prevalent where experts are not readily available.</sentence>
    <sentence>Machine learning techniques (especially rule induction methods) can be of great benefit to this area by providing strategies to automatically extract useful knowledge, given enough historical data.</sentence>
    <sentence>Fuzzy set (FS) In 1965, Zadeh (1964) assigned a number to every element in the universe, which indicates the degree (grade) to which the element belongs to a Fuzzy set.</sentence>
    <sentence>To formulate this concept of Fuzzy set mathematically, we present the following definition.</sentence>
    <sentence>Let X be the universe.</sentence>
    <sentence>A mapping A: X→ [0,1] is called a Fuzzy set on X.</sentence>
    <sentence>The value µ(x) of A, at x∈ X stands for the degree of membership of x in A.</sentence>
    <sentence>The set of all Fuzzy sets on X will be denoted by F(X).</sentence>
    <sentence>μ(x) = 1 means full membership, μ(x) = 0 means non-membership, and intermediate values between 0 and 1 mean partial membership.</sentence>
    <sentence>μ(x) is referred to as a membership function as x varies in X.</sentence>
    <sentence>Based on the database, membership functions for all the variables were defined, and the criterion for defining the deviation and center of each of these functions was developed with support from the injection experts.</sentence>
    <sentence>We considered levels of sensitivity (linguistic labels) α for each membership function with respective centers cα and standard deviations σ.</sentence>
    <sentence>Considering the components x ℮ X, to model the membership functions, we used the Gaussian function (2).</sentence>
    <sentence>(2) The premises of all rules are compared with controlled entries to determine which rules apply to a situation; the outputs are compared with the established rules that have been determined.</sentence>
    <sentence>In this paper, we used the T-norm applied to x = (x1, x2,..,xq) as suggested by Jensen, 2005 , where the value of Φj,j=1,...,p rules of inference is calculated (3): (3) 2.4.</sentence>
    <sentence>Artificial neural network (ANN) The ANN architecture Multilayer Perceptron (MLP) typically consists of a specification of the number of layers, the type of activation function of each unit, and the weights of connections between the different units, and should be established for the construction of the neural architecture (Haykin, 2001).</sentence>
    <sentence>This paper presents a supervised feedback ANN architecture in three layers: the input layer represents the values of the reduced rule base.</sentence>
    <sentence>The neurons of the network hidden layer are trained from the set of inference rules (reduced).</sentence>
    <sentence>The algorithm used in training the MLP is the error back propagation that works as follows: first, a standard is presented.</sentence>
    <sentence>In this work, a standard will be a prototype vector and its label - the input layer of the network.</sentence>
    <sentence>This pattern is processed layer by layer until the output layer provides the response rendered, fMLP, as calculated in Eq (4): (4) where wlj are synaptic weights; bl and b0 are the biases; φ is the activation function, usually specified as the sigmoid function.</sentence>
    <sentence>ANN has the ability to solve problems in complex systems such as image classification, due to its ability to generalize, however, has difficulty working with redundant information, or with very large data set.</sentence>
    <sentence>Rough-fuzzy artificial neural network (RFANN) In this paper the pre-processing through RS serves to identify which attributes are most relevant for the classification of image.</sentence>
    <sentence>Rough selection provides a means by which discrete or real-valued noisy data (or a mixture of both) can be effectively reduced without the need for user-supplied information.</sentence>
    <sentence>In addition, this technique can be applied to data with continuous or nominal decision attributes, and, as such, can be applied to regression as well as classification datasets.</sentence>
    <sentence>The only additional information required is in the form of Fuzzy partitions for each feature, which can be automatically derived from the data (Jensen, 2005).</sentence>
    <sentence>The attributes selection for problems such as image classification is a complex problem, and its complexity grows exponentially in the number of attributes to be discretized.</sentence>
    <sentence>In this paper a new algorithm was proposed to do the discretization.</sentence>
  </section>
  <section name="Experimental methodology">
    <sentence>This work should be viewed as an application of Artificial Intelligence techniques, in particular, ANN to determine a surface response.</sentence>
    <sentence>The hardware platform used in the experiments was a Core i5-4200 U 1.6 GHz, 8 G RAM and 500GB hard drive.</sentence>
    <sentence>Establishment of new algorithm The images processing and the RFANN routines were written on C Program Language.</sentence>
    <sentence>Fig 1 illustrates the full process of the Rough-Fuzzy Artificial Neural Network (RFANN).</sentence>
    <sentence>The RS pre-processes the Fuzzy rules and generates the input vectors with most critical attributes.</sentence>
    <sentence>This reduced rule base is used to train the MLP ANN.</sentence>
    <sentence>Fig 2.</sentence>
    <sentence>Image from tree radial cuts.</sentence>
    <sentence>Fig 3.</sentence>
    <sentence>Mesh representing the image topography.</sentence>
    <sentence>The image processing system operates as follows: The features are extracted from colour images by treating each channel of colour image (Red–Yellow–Green) as a monochrome imageand transforming its shape information in pixels surfaces through a C-language program.</sentence>
    <sentence>In the second step, a matrix is created with the numeric values of color intensity corresponding to each pixel (between 0 and 255).</sentence>
    <sentence>Later it performs normalization into the numerical matrix, where it is assigned a unit value for pixels maximum intensity and zero to minimal intensity.</sentence>
    <sentence>The images were collected from axial cut of trees, each one with 1756 × 1326 pixels size.</sentence>
    <sentence>The following is presented in Fig 2, a dataset of images composed of radial cuts in different samples.</sentence>
    <sentence>From this normalized basis, vectors are created to identify patterns for each image.</sentence>
    <sentence>These standards will be used as the basis of training data for RFNAN, which after its training; will be able to identify the specimen, even if it exactly feature vector does not belong to the database training.</sentence>
    <sentence>The computer program converts the images into a 3D mesh through his monochromatic image.</sentence>
    <sentence>At this stage the algorithm creates a mesh representing the topography of the image, where the point is associated to a set of spatial coordinates as shown in Fig 3.</sentence>
    <sentence>An image = {y(s)} is assumed to be a Gaussian random field on a M × N lattice Ω where y(s) denotes the gray level of a pixel at location s(i,j), where y(s) ℮ [0;ymax] The space will be subdivided into q-parts as follows (5) (Stepaniuk, 2008).</sentence>
    <sentence>(5) Considering the b samples in the image dataset, given a non-empty set Y, a possible partition Ψ is a collection of non-empty subsets of Y,such that (6): (6) For the finite set Ψi, cardinality, denoted by card(Ψi), is the number of set elements, as shown in Eq (7): (7) The feature vectors as defined above will be used as the basis of training data for a RFANN, which after its training; will be able to classify the image.</sentence>
    <sentence>It is possible to establish a dataset with respective feature vectors for all images pattern, the values obtained are displayed below in Table 1.</sentence>
    <sentence>Table 1.</sentence>
    <sentence>Datasets: All set features for each sample.</sentence>
    <sentence>Feature Sample 0 Sample 1 Sample 2 Sample 3 Sample 5 Sample 6 ×1 3636 8 29 11 15 119 ×2 3284 0 108 0 6 444 ×3 1842 1 186 0 25 932 ×4 1160 11 286 1 59 1254 ×5 796 80 413 0 137 1759 ×6 655 433 574 4 248 1937 ×7 681 1390 749 67 319 2050 ×8 549 3144 998 370 443 2311 ×9 677 5431 1278 1276 482 2472 ×10 775 7383 1585 3012 610 2509 ×11 1051 4516 2176 5006 1039 2409 ×12 1484 1110 2653 5916 1806 1950 ×13 1999 235 2779 4196 2878 1583 ×14 2214 61 2978 2580 3971 871 ×15 1757 14 3344 1140 5046 468 ×16 967 2 2786 341 4646 149 ×17 320 3 1052 106 1950 40 ×18 87 5 300 22 421 7 ×19 16 0 29 2 56 0 Table 2.</sentence>
    <sentence>Rough inclusion function - ν(i,j).</sentence>
    <sentence>feature 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ∑ 1 1,00 0,50 0,33 0,25 0,25 0,17 0,17 0,14 0,14 0,14 0,14 0,14 0,14 0,17 0,17 0,20 0,20 0,25 0,50 5,01 2 0,50 1,00 0,50 0,33 0,25 0,17 0,17 0,14 0,14 0,14 0,14 0,14 0,14 0,20 0,20 0,17 0,20 0,20 0,33 5,07 3 0,33 0,50 1,00 0,33 0,25 0,17 0,17 0,14 0,14 0,14 0,14 0,17 0,17 0,25 0,25 0,17 0,20 0,20 0,33 5,05 4 0,25 0,33 0,33 1,00 0,50 0,25 0,25 0,20 0,20 0,17 0,17 0,14 0,14 0,20 0,20 0,20 0,33 0,25 0,25 5,37 5 0,25 0,25 0,25 0,50 1,00 0,33 0,33 0,25 0,25 0,20 0,20 0,17 0,17 0,17 0,17 0,20 0,50 0,25 0,25 5,68 6 0,17 0,17 0,17 0,25 0,33 1,00 0,50 0,33 0,33 0,25 0,25 0,20 0,20 0,14 0,14 0,17 0,33 0,25 0,17 5,35 7 0,17 0,17 0,17 0,25 0,33 0,50 1,00 0,33 0,33 0,25 0,25 0,17 0,17 0,14 0,14 0,17 0,33 0,25 0,17 5,29 8 0,14 0,14 0,14 0,20 0,25 0,33 0,33 1,00 0,50 0,25 0,33 0,17 0,17 0,14 0,17 0,20 0,25 0,20 0,14 5,06 9 0,14 0,14 0,14 0,20 0,25 0,33 0,33 0,50 1,00 0,25 0,25 0,17 0,17 0,14 0,17 0,20 0,25 0,20 0,14 4,98 10 0,14 0,14 0,14 0,17 0,20 0,25 0,25 0,25 0,25 1,00 0,33 0,20 0,25 0,17 0,14 0,20 0,20 0,17 0,14 4,60 11 0,14 0,14 0,14 0,17 0,20 0,25 0,25 0,33 0,25 0,33 1,00 0,25 0,20 0,17 0,14 0,20 0,20 0,17 0,14 4,68 12 0,14 0,14 0,17 0,14 0,17 0,20 0,17 0,17 0,17 0,20 0,25 1,00 0,50 0,20 0,17 0,17 0,20 0,14 0,14 4,43 13 0,14 0,14 0,17 0,14 0,17 0,20 0,17 0,17 0,17 0,25 0,20 0,50 1,00 0,20 0,17 0,17 0,20 0,14 0,14 4,43 14 0,17 0,20 0,25 0,20 0,17 0,14 0,14 0,14 0,14 0,17 0,17 0,20 0,20 1,00 0,25 0,25 0,17 0,17 0,17 4,29 15 0,17 0,20 0,25 0,20 0,17 0,14 0,14 0,17 0,17 0,14 0,14 0,17 0,17 0,25 1,00 0,20 0,17 0,17 0,17 4,17 16 0,20 0,17 0,17 0,20 0,20 0,17 0,17 0,20 0,20 0,20 0,20 0,17 0,17 0,25 0,20 1,00 0,20 0,20 0,20 4,45 17 0,20 0,20 0,20 0,33 0,50 0,33 0,33 0,25 0,25 0,20 0,20 0,20 0,20 0,17 0,17 0,20 1,00 0,25 0,20 5,38 18 0,25 0,20 0,20 0,25 0,25 0,25 0,25 0,20 0,20 0,17 0,17 0,14 0,14 0,17 0,17 0,20 0,25 1,00 0,33 4,79 19 0,50 0,33 0,33 0,25 0,25 0,17 0,17 0,14 0,14 0,14 0,14 0,14 0,14 0,17 0,17 0,20 0,20 0,33 1,00 4,92 3.2.</sentence>
    <sentence>Approximation spaces To obtain the network output value at the generalization phase, it is necessary to classify the values from inference rules.</sentence>
    <sentence>Approximation spaces can be treated as granules used for concept approximation.</sentence>
    <sentence>They are some special parameterized relational structures.</sentence>
    <sentence>Tuning of parameters makes it possible to search for relevant approximation spaces relative to given concepts (Komorowski, Polkowski, &amp; Skowron, 2002).</sentence>
    <sentence>The Rough inclusion function P (U)×P (U) →[0, 1] defines the degree of inclusion of ΦI in Φj, where Φi, Φj⊆ U.</sentence>
    <sentence>In the simplest case the standard rough inclusion function can be defined by (8): (8) The Rough inclusion function ν(i,j) algorithm is given as follows: ν[]: Rough inclusion function Φ[]: Fuzzy rules of inference (1) k,i,j,ν[]:←0 (2) do (3) if (Φ [i,k]≠ Φ [j,k]) ν [i,j]= ν [i,j]+1 (4) until k &lt; p (5) until i&lt;q and j&lt;q (6) return ν []←1/(ν [] +1) This measure is widely used by the data mining and RS communities.</sentence>
    <sentence>It is possible to use this idea to estimate the probability of implications.</sentence>
    <sentence>The Table 2 presents the values of the Rough inclusion function.</sentence>
  </section>
  <section name="Conduct of experiments">
    <sentence>RFANN applied to image identification The experiment compares the knowledge generated by the RFANN with the data obtained from an Image dataset in two cases: the unreduced dataset and the reduced one by the RS.</sentence>
    <sentence>Fig 4.</sentence>
    <sentence>Error comparison: RFANN vs. randomly selected features.</sentence>
    <sentence>Since the processing times were near to zero, the criterion for selection was the simplicity of the architecture of the network.</sentence>
    <sentence>The choice of MLP parameters of the reduced dataset were: number of input neurons: 4, initial learning rate η = 0.7; Initial momentum μ = 0.7; the stop criterion was maximum 1000 epochs.</sentence>
    <sentence>The configuration of the number of neurons and layers are accomplished by choosing the architecture that has the lowest permissible error.</sentence>
    <sentence>The choice of parameters for the MLP of the full dataset were: input neurons 19, number of hidden neurons 12, and learning constant η = 0.7, momentum μ = 0.7; the stop criterion was maximum 1000 epochs.</sentence>
    <sentence>The rule bases reduced by the RFANN exhibit good behavior during the generalization phase, indicating a promising way for to associate RS with FS to replace the human expert in the construction of inference rules.</sentence>
    <sentence>The RFANN returns four features as the most significant ones, x10, x11, x12, x13, out of the original full set of features.</sentence>
    <sentence>We can check that using the features selection does not significantly reduce the accuracy of the classification compared to the use of the full set of features.</sentence>
    <sentence>Table 3 shows the error produced for the RFANN.</sentence>
    <sentence>Table 3.</sentence>
    <sentence>RFANN error: comparing fuzzy-rough selection versus full features.</sentence>
    <sentence>Features Dim.</sentence>
    <sentence>Topology Error Training Testing RF selection 4 4-12-1 0.00307 0.00435 x1–x7 7 7-12-1 0.01786 0.01299 x1–x10 10 10-12-1 0.00519 0.00724 x1–x13 13 13-12-1 0.00212 0.00301 x1–x16 16 16-12-1 0.00176 0.00250 Full features 19 19-12-1 0.00208 0.00295 It is very interesting to note that the error rate when using four selected features is very near the error when using the full feature set.</sentence>
    <sentence>Furthermore, this performance improvement is obtained via a network structurally much simpler.</sentence>
    <sentence>This is indicative of the power resource selection to help reduce redundant feature, not only the measures but also the noise associated with this measurement.</sentence>
    <sentence>Comparison with the randomly selected features The above comparison ensured that no information loss is incurred due to RFANN feature reduction.</sentence>
    <sentence>It is possible to compare the performance of different attributes, reduced according to RFANN criteria and randomly selected features, as shown in Table 4 and Fig 4.</sentence>
    <sentence>Table 4.</sentence>
    <sentence>RFANN versus randomly selected features.</sentence>
    <sentence>Features Dim.</sentence>
    <sentence>Topology Error Training Testing RF selection 4 4-12-1 0.00307 0.00435 7,10,11,19 4 4-12-1 0.00394 0.00557 2,8,14,19 4 4-12-1 0.00501 0.00706 3,6,9,18 4 4-12-1 0.00556 0.00715 1,5,8,12 4 4-12-1 0.00670 0.00899 4,6,7,17 4 4-12-1 0.00685 0.00978 1,2,3,4 4 4-12-1 0.09206 0.03213 2,10,16,17 4 4-12-1 0.02783 0.03885 16,17,18,19 4 4-12-1 0.11604 0.11777 The average error classifiers, using four randomly selected characteristics, higher than that achieved by the classifier that uses the resources selected by RFANN, considering the same dimensionality.</sentence>
    <sentence>This implies that on these features randomly selected occur losses of information during reduction.</sentence>
    <sentence>The technique proposed in this paper uses a histogram analysis as feature selection criteria, on this way, there is a computational gain considering the model's simplicity.</sentence>
    <sentence>This approach presents advantages comparing to the traditional ANN ones, since the running time is considerably reduced, and the ANN processed only 21% (4/19) of the original dataset.</sentence>
    <sentence>The figures shown that with a simple neural network topology, namely, one hidden layer containing 12 neurons, and a sensitive layer (4 neurons), the network was capable to classify biological images.</sentence>
    <sentence>However, this method was applied to a restricted dataset, therefore we suggest its enlargement to another databases.</sentence>
  </section>
  <section name="Conclusions">
    <sentence>In this paper, an automatic visual inspection system for the recognition of tropical Wood species based on artificial intelligence techniques has been proposed.</sentence>
    <sentence>The system was objectively designed to be cost-effective and as a means to replace wood inspectors due to difficulty in recruiting them as the job is rather laborious.</sentence>
    <sentence>ANN has the ability to solve problems in complex systems such as image classification, due to its ability to generalize.</sentence>
    <sentence>However, has difficulty working with redundant information, or with very large dataset.</sentence>
    <sentence>Therefore, the technique proposed in this paper uses a histogram analysis as criteria to feature selection, on this way, there is a computation gain considering the model simplicity.</sentence>
    <sentence>The figures shown on Section 4.2 demonstrates this hypothesis, once with a simple neural network topology, was capable to classify biological images.</sentence>
    <sentence>Additionally, one possible weakness of the method, may be the restricted dataset and the fact the method was been used only as classification.</sentence>
    <sentence>The application of RFANN showed a great ability to generalize, to identify behavior patterns, and to allow the creation of an inference mechanism in high complex systems.</sentence>
    <sentence>When applied to a real world dataset, the RFANN was able to identify the significant features as defined by a human expert.</sentence>
    <sentence>The main advantage of using the RFANN is the reduction of dependence on a human expert for the choice and construction of the rules of inference mechanism.</sentence>
    <sentence>This gain is important, considering that one of the weaknesses of the approach using the Fuzzy Sets is its dependence on the human expert.</sentence>
    <sentence>If there is a reasonable number of attributes and a structured database, it may be even possible to eliminate the need for support from the human expert for the construction of inference rules, instead using his support only in the construction of membership functions.</sentence>
    <sentence>We suggest the RFANN in modeling other problems, such as dynamic routing or business datasets, in order to assess the impact of this approach on the dependence of the human expert in building the inference mechanism.</sentence>
  </section>
</article>
