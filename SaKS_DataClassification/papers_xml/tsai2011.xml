<article>
  <title>Modular neural network programming with genetic optimization</title>
  <abstract>
    <sentence>This study proposes a modular neural network (MNN) that is designed to accomplish both artificial intelligent prediction and programming.</sentence>
    <sentence>Each modular element adopts a high-order neural network to create a formula that considers both weights and exponents.</sentence>
    <sentence>MNN represents practical problems in mathematical terms using modular functions, weight coefficients and exponents.</sentence>
    <sentence>This paper employed genetic algorithms to optimize MNN parameters and designed a target function to avoid over-fitting.</sentence>
    <sentence>Input parameters were identified and modular function influences were addressed in manner that significantly improved previous practices.</sentence>
    <sentence>In order to compare the effectiveness of results, a reference study on high-strength concrete was adopted, which had been previously studied using a genetic programming (GP) approach.</sentence>
    <sentence>In comparison with GP, MNN calculations were more accurate, used more concise programmed formulas, and allowed the potential to conduct parameter studies.</sentence>
    <sentence>The proposed MNN is a valid alternative approach to prediction and programming using artificial neural networks.</sentence>
  </abstract>
  <keywords>
    <keyword>Artificial intelligence</keyword>
    <keyword>Genetic programming</keyword>
    <keyword>High order neural network</keyword>
    <keyword>Concrete</keyword>
  </keywords>
  <section name="Introduction">
    <sentence>Soft-computing approaches have been refined into specialized branches that include neural networks (NNs), fuzzy logic, support vector machines, genetic algorithms (GAs) and genetic programming (GP), among others.</sentence>
    <sentence>Each has its particular merits in comparison with others.</sentence>
    <sentence>NNs are the most familiar soft-computing approach for learning tasks.</sentence>
    <sentence>The tools categorized into NNs, including back propagation networks (BPNs), radial basic functions, self-organizing mapping and learning vector quantization, are already applied in many fields for data mining and knowledge learning.</sentence>
    <sentence>BPNs are the most widely applied, accounting for over 70% of all NN applications.</sentence>
    <sentence>A neuron in the BPN model calculates net value based on associated inputs and multiplicative weights.</sentence>
    <sentence>As they lack hidden layers, BPNs are not able to address complicated problems.</sentence>
    <sentence>To enhance BPN capacity, hidden layers are employed between input-output mapping.</sentence>
    <sentence>High order neural networks (HONNs) were developed around 1990.</sentence>
    <sentence>HONN’s nonlinear combination of inputs (Zurada, 1992), allows for easy capture of high order correlations and efficient nonlinear mapping (Abdelbar &amp; Tagliarini, 1996; Tsai, 2009).</sentence>
    <sentence>HONNs have been applied widely in various research domains, especially to problems related to pattern recognition and function approximation (Artyomov &amp; Yadid-Pecht, 2005; Foresti &amp; Dolso, 2004; Rovithakis, Chalkiadakis, &amp; Zervakis, 2004; Wang &amp; Lin, 1995).</sentence>
    <sentence>References show the HONN as a powerful soft-computing approach.</sentence>
    <sentence>HONN designs can further be specially adapted to represent problems in more meaningful ways.</sentence>
    <sentence>Since it was first proposed by Koza (1992), GP has received much attention with regard to modeling nonlinear relationships in input-output mappings.</sentence>
    <sentence>GP creates solutions as programs (formulas) to solve problems with an operation tree.</sentence>
    <sentence>As some HONN models provide such programming functions as well, HONN is considered a branch of NN that, in contrast to GP, programs nonlinear relationships for input-output mappings.</sentence>
    <sentence>This study proposes a modular neural network (MNN) able to accomplish both artificial intelligent prediction and programming.</sentence>
    <sentence>The proposed MNN comprises HONN models and modular functions, with each HONN model containing weight and exponent coefficients able to transfer inputs into meaningful polynomials.</sentence>
    <sentence>The remaining sections of this paper include Section 2: Proposed MNN and GA optimization; Section 3: Programming high-strength concrete parameters to study MNN capacities in comparison with GP results; Section 4: Suggestions for further studies and future work and Section 5: Conclusions.</sentence>
  </section>
  <section name="Modular neural network optimized by genetic algorithm">
    <sentence>Modular neural network The output (O) of the modular neural network (MNN) is a combination of several modular elements (M), with relationships wK, where w represents weight coefficients and K denotes the module index from 0 to NK (meaning that there are NK function selections and a bias term).</sentence>
    <sentence>Hence, O can be mathematically represented as (see Fig 1): (1) where M0 is a bias unit with a value of 1.</sentence>
    <sentence>Modular neural network Fig 1.</sentence>
    <sentence>Modular neural network.</sentence>
    <sentence>Each MK is activated by a weighted summation operator and a setting function type FK.</sentence>
    <sentence>MK is a derivative of a HONN (Abdelbar &amp; Tagliarini, 1996) designed for programming simplicity.</sentence>
    <sentence>This HONN has three layers, i.e., an input layer, a hidden layer and an output layer, with neuron numbers NI-NJ-1.</sentence>
    <sentence>The eventual layer has only one neuron, i.e., modular element MK.</sentence>
    <sentence>There are NI input neurons XIK and NJ hidden neurons YJK indexed of I and J respectively in each modular element MK (see Fig 2).</sentence>
    <sentence>(2) (3) MNN module Fig 2.</sentence>
    <sentence>MNN module.</sentence>
    <sentence>The three layers are connected through two connections functioned, respectively, by an exponent-product (pIJK-Π) connection and weight-summation (wJK-Σ) connection.</sentence>
    <sentence>Coefficients wK, wJK, and pIJK activate the proposed MNN.</sentence>
    <sentence>Besides, in contrast with traditional NNs, MNN has no activation function (using an identity function) for the first two layers.</sentence>
    <sentence>Activation function FK is associated with modular output MK, and each FK is assigned a user-selected function.</sentence>
    <sentence>Such function assignments depend on user settings to identify problems within multiple attributes.</sentence>
    <sentence>(4) While there are NP input parameters P, MNN adopts NI inputs for an MNN module.</sentence>
    <sentence>Each XI selects a P as: (5) This input XI selection is dominated by optimization.</sentence>
    <sentence>Therefore, the MNN parameters that require optimized include: wK, wJK, and pIJK and XI.</sentence>
    <sentence>Parameters that should be input include: NP, NI, NJ, NK, FK, and training / testing patterns.</sentence>
    <sentence>The final MNN output O can be formed as: (6) For instance, an example O could be: (7) where f may serve as the user defined function.</sentence>
    <sentence>Eq (7) represents the original concept and primary target of this study.</sentence>
    <sentence>With the proposed MNN, NN learning will be able to both predict and program.</sentence>
    <sentence>As mentioned above, optimization remains a challenge to MNN programming.</sentence>
    <sentence>Genetic algorithm optimization The Genetic Algorithm (GA), which imitates parts of the natural evolution process, was first proposed by (Holland, 1975).</sentence>
    <sentence>GA is a stochastic search approach inspired by natural evolution that involves crossover, mutation, and evaluation of survival fitness.</sentence>
    <sentence>Genetic operators work from initial generation to offspring in order to evolve an optimal solution through generations.</sentence>
    <sentence>Each individual of a generation generates a result for the problem, which is represented as a string-named chromosome.</sentence>
    <sentence>This relatively straightforward and simple implementation procedure gives GA exceptional flexibility to hybridize with domain-dependent heuristics to create effective implementation solutions tailored to specific problems.</sentence>
    <sentence>Based on these merits, the potential to use of GA in optimization techniques has been studied intensively (Gen &amp; Cheng, 1997).</sentence>
    <sentence>However, simple GA is difficult to apply directly and successfully to a large range of difficult-to-solve optimization problems (Michalewicz, 1996).</sentence>
    <sentence>MATLAB was employed in this study because it is a powerful tool that incorporates various function sets, including genetic algorithm (ga function).</sentence>
    <sentence>To study how the MATLAB ga function works is essential due to the numerous parameter settings that must be set properly in order to ensure correct results.</sentence>
    <sentence>The five basic steps to using GA are: 1.</sentence>
    <sentence>Initialize population - Initial individuals in a population are randomly generated, with each composed of a binary string (i.e., a chromosome) containing NK × NI MNN input parameter selections (XI), NK × NI × NJ exponent coefficients (pIJK), NK × (NJ + 1) weight coefficients (wJK), and NK + 1 weight coefficients (wK).</sentence>
    <sentence>Value boundaries are set as integer 1 ∼ NP for XI selections and float −10 ∼ 10 for all coefficient constants.</sentence>
    <sentence>Evaluate individuals - Fitness is a major index used to evaluate chromosome status, with decreasing fitness values correlated to increasing degrees of achievement of the model objective.</sentence>
    <sentence>A large fitness value indicates a strong individual.</sentence>
    <sentence>In this study, a target function that is an inverse of the fitness function will be discussed later.</sentence>
    <sentence>Perform crossover - A positive scalar should be set for parts of the population to perform crossover.</sentence>
    <sentence>To create crossover children, a function must be selected.</sentence>
    <sentence>This study performed a scattered (function of @corssoverscattered in MATLAB) crossover with a crossover rate of 0.8.</sentence>
    <sentence>Perform mutation - A mutation produces spontaneous random changes in chromosomes.</sentence>
    <sentence>The MATLAB function @mutationuniform with a mutation rate at 0.05 was used herein.</sentence>
    <sentence>Select individuals - The @selectionstochinif was used herein to select parents of crossover and mutation children.</sentence>
    <sentence>In additional, five elitist individuals were guaranteed to survive to the next generation herein.</sentence>
    <sentence>The author took four quantities as the target function (TF).</sentence>
    <sentence>One was the training root mean square error (RMSEtr) and the others were the effects of pIJK, wJK, and wK coefficients (Tsai, 2010).</sentence>
    <sentence>Individual health (fitness function) correlates inversely to TF quantity.</sentence>
    <sentence>(8) (9) (10) (11) where C1, C2, and C3, are hyper-parameters leveraged to balance the bilateral impact between errors and coefficients.</sentence>
    <sentence>VPIJ, VWJ and VWK encourage all coefficients to approach zero and become normalized by their respective numbers of coefficients.</sentence>
    <sentence>With this designed TF, MNN always identifies optimal results under simple linkages.</sentence>
    <sentence>Such a criterion prevents an over-fitting of resultant models.</sentence>
    <sentence>This was proposed previously for a Bayesian neural network (MacKay, 1995), but only for weights that were not exponents.</sentence>
    <sentence>Settings for the hyper-parameter were determined under conditions in which the effects of RMSEtr on TF exceeded 70%.</sentence>
    <sentence>Although particular settings for hyper-parameters may be meaningful, this study adopted the same value for all coefficients currently.</sentence>
  </section>
  <section name="MNN programming for high-strength concrete parameters">
    <sentence>Baykasoglu, Oztas, and Ozbay (2009) gathered 104 high strength concrete datasets.</sentence>
    <sentence>Six parameters were selected as input parameters, including water-binder ratio (W/B), water (W), fine aggregate (s/a), fly ash (FA), air entraining agent (AE), and super-plasticizer (SP).</sentence>
    <sentence>Output targets focused on concrete compressive strength (Strength), cost (Cost) and slump (Slump) (see Table 1).</sentence>
    <sentence>Parameter settings are listed in Table 2.</sentence>
    <sentence>The referenced RMSEs for GP were 2 MPa, 0.2 $/m3, and 20.7 cm for compressive strength, cost, and slump, respectively, with GP (Baykasoglu et al., 2009).</sentence>
    <sentence>The parameter settings for MNN and GA are listed in Table 2.</sentence>
    <sentence>NK = 5 indicates that five modular functions have been adopted (five functions in Eq (4)).</sentence>
    <sentence>Nine sets of NI and NJ combinations were selected for MNN topology.</sentence>
    <sentence>Each MNN model was executed ten times to obtain optimal results depending on summation of final training and testing RMSEs.</sentence>
    <sentence>Table 1.</sentence>
    <sentence>Lower and upper bound for inputs and targets.</sentence>
    <sentence>Factors Lower bound Upper bound P1: W/B,% 30 45 P2: W, kg/m3 160 180 P3: s/a,% 37 53 P4: FA,% 0 20 P5: AE, kg/m3 0.036 0.078 P6: SP, kg/m3 1.89 8.5 Strength, MPa 38 74 Cost, $/m3 35.02 66.88 Slump, cm 95 260 Table 2.</sentence>
    <sentence>Summary of MNN parameter settings.</sentence>
    <sentence>Setting MNN parameter NI 2, 4, 6 NJ 2, 4, 6 NK 5 NP 6 wk, wjk, and pijk ranges −10 ∼ 10 Datasets 80 of 104 for training 24 for testing GA parameter Population size 200 Crossover rate 0.8 Mutation rate 0.05 Elitist 5 Iterations 2,000 C1, C2, and C3 0.08 for strength 0.02 for cost 1.5 for slump 3.1.</sentence>
    <sentence>Compressive strength The best analyzed results for concrete compressive strength are listed in Table 3, which show training RMSE, training R-square (R2), testing RMSE, and testing R2.</sentence>
    <sentence>Average computation time for the ten trials is also listed.</sentence>
    <sentence>Table 3.</sentence>
    <sentence>Results of concrete compressive strength analysis.</sentence>
    <sentence>Training RMSE (MPa); Training R2; Testing RMSE (Mpa); Testing R2; Avg.</sentence>
    <sentence>CPU time (min) NJ 2 4 6 NI 2 1.519 1.579 1.676 0.974 0.972 0.969 1.945 1.555 1.927 0.953 0.970 0.954 1727 1743 1858 4 1.336 1.429 1.259 0.980 0.977 0.982 1.646 1.190 1.215 0.966 0.982 0.982 1721 2004 2142 6 1.354 1.343 1.236 0.980 0.980 0.983 1.426 1.806 1.828 0.975 0.959 0.958 1982 2208 2406 The referenced GP result is listed in Eq (12), with RMSE at 2 MPa (Baykasoglu et al., 2009).</sentence>
    <sentence>(12) All MNN model results perform better than the GP’s in terms of RMSE and are attached with good R2.</sentence>
    <sentence>In result sets where the NI equals 2, a large NJ does not improve result accuracy.</sentence>
    <sentence>Such indicates that an inadequate number of input parameters were available for strength programming.</sentence>
    <sentence>Results obtained with NI up to 4 improve with increasing NI or NJ.</sentence>
    <sentence>However, such increases processing time required and raises programmed formula complexity.</sentence>
    <sentence>While the proposed MNN was originally designed as a tool for artificial intelligence programming, this goal has not yet been achieved.</sentence>
    <sentence>In Table 3, the result set {NI = 4, NJ = 2, NK = 5} delivers outstanding results in terms of RMSE, R2 and model complexity.</sentence>
    <sentence>While a complex model topology may lead to accurate prediction results, it will form a complicated formula in terms of programming.</sentence>
    <sentence>When {NI = 4, NJ = 2, NK = 5} is adopted, MNN programs high-strength concrete Strength as an expression of: (13) The MNN learning process is shown in Fig 3.</sentence>
    <sentence>The curve of the target function decreases consistently during iterations and RMSEs trend toward minimization.</sentence>
    <sentence>The coefficient effect is shown in all terms except training RMSE in Eq (8) and encourages MNN to identify values close to zero for all coefficients (see Fig 4).</sentence>
    <sentence>Furthermore, in order to study the effects of modular functions, the modular function, y = log(x), dominates the final prediction, followed by y = x (see Fig 5).</sentence>
    <sentence>This paper tried to prune down some of the MNN modules, and tried modular functions {y = x, y = log(x)}, i.e., NK = 2.</sentence>
    <sentence>Unfortunately, the best trial result provided an RMSE at 2.2 MPa.</sentence>
    <sentence>This result may be inadequate.</sentence>
    <sentence>The reason may be attributable to the reflection by other modular functions for some of the sensitive variations under such high accuracy.</sentence>
    <sentence>Consequently, this paper added another modular function, y = sin(x).</sentence>
    <sentence>Results obtained by {NI = 4, NJ = 2, NK = 3} with FK = {y = x, y = log(x), y = sin(x)} were, 1.718 MPa, 0.967, 1.679 MPa, 0.965, 1301 min on training RMSE, training R2, testing RMSE, testing R2 and average CPU time, respectively.</sentence>
    <sentence>The modular function, y = log(x), contributed significantly to the final prediction, while remaining functions accounted for tuning results (see Fig 6).</sentence>
    <sentence>The final Strength formula was expressed as: (14) where {P1, P5, P6}, {P2, P6}, {P1, P3, P5, P6} contribute to modular outputs of y = x, y = sin(x), y = log(x), respectively, of which, P6 appears in all modular outputs and P4 is absent from the final Strength formula.</sentence>
    <sentence>No doubt, P6 (super-plasticizer) greatly impacts upon the compressive Strength of high-strength concrete.</sentence>
    <sentence>Parameter influences reflect input-output mapping or data attribute problems.</sentence>
    <sentence>Additional datasets may change the influence of parameters.</sentence>
    <sentence>It is hard to state unequivocally that any parameter does not contribute to final targets.</sentence>
    <sentence>Compared to StrengthGP, this paper demonstrated that MNN represents an alternative in artificial intelligence programming.</sentence>
    <sentence>Strength target values during iterations Fig 3.</sentence>
    <sentence>Strength target values during iterations.</sentence>
    <sentence>Coefficient values for strength Fig 4.</sentence>
    <sentence>Coefficient values for strength.</sentence>
    <sentence>Module impacts on strength Fig 5.</sentence>
    <sentence>Module impacts on strength.</sentence>
    <sentence>Final module impacts on strength Fig 6.</sentence>
    <sentence>Final module impacts on strength.</sentence>
    <sentence>Cost The best analyzed results of concrete cost are listed in Table 4, along with training RMSE, training R2, testing RMSE and testing R2.</sentence>
    <sentence>Average computation times for the ten trials are also listed.</sentence>
    <sentence>Table 4.</sentence>
    <sentence>Analyzed results for concrete cost.</sentence>
    <sentence>Training RMSE ($/m3); Training R2; Testing RMSE ($/m3); Testing R2; CPU time (min) NJ 2 4 6 NI 2 0.536 0.409 0.451 0.998 0.999 0.998 0.450 0.402 0.608 0.998 0.999 0.997 1821 1832 1969 4 0.347 0.428 0.296 0.999 0.998 0.999 0.271 0.357 0.274 0.999 0.999 0.999 1808 2156 2210 6 0.445 0.453 0.299 0.998 0.998 0.999 0.481 0.466 0.257 0.998 0.998 0.999 2047 2265 2469 Referenced GP results are listed in Eq (15), with an RMSE of 0.2 $/m3 (Baykasoglu et al., 2009).</sentence>
    <sentence>No MNN model result performs better than GPs in terms of RMSE.</sentence>
    <sentence>However MNN models present outstanding R2 values.</sentence>
    <sentence>Similar to previous statements for Strength, MNN results may be improved by increasing the NI or NJ.</sentence>
    <sentence>However, any increase in NI or NJ number will require greater processing time.</sentence>
    <sentence>(15) Under {NI = 4, NJ = 2, NK = 5}, MNN programs high-strength concrete Cost as an expression of: (16) Under the iteration process, parameters shown in Fig 7 all target on minimization.</sentence>
    <sentence>Two modular functions {y = x, y = log(x)} and the bias term {y = C} all affect Cost predictions significantly.</sentence>
    <sentence>(see Fig 8).</sentence>
    <sentence>Furthermore, this paper also tried modular functions of {y = x, y = log(x)}, i.e., NK = 2.</sentence>
    <sentence>The MNN successfully predicted Cost with training RMSE, training R2, testing RMSE, testing R2, and average CPU time at 0.501 $/m3, 0.998, 0.493 $/m3, 0.998, and 178.1 min, respectively.</sentence>
    <sentence>Final functional effects are shown in Fig 9, where both {y = x, y = log(x)} clearly contribute to this MNN Cost prediction.</sentence>
    <sentence>The final Cost formula is: (17) where {P1, P2} and {P3, P6} participate in the modular output of y = x and y = log(x), respectively.</sentence>
    <sentence>While this paper does not assert that P4 and P5 are not related to Cost, the influences of these two variables are minor.</sentence>
    <sentence>It may be that current datasets do not adequately reflect the contributions of P4 and P5.</sentence>
    <sentence>Further testing using other datasets can help clarify this issue.</sentence>
    <sentence>Compared to CostGP, while the studied Cost presented a somewhat disappointing level of accuracy, it delivered a reliable R2 and a beautiful formula.</sentence>
    <sentence>Certainly, this case study proves the abilities of MNN in terms of both artificial intelligence prediction and programming.</sentence>
    <sentence>Cost target values during iterations Fig 7.</sentence>
    <sentence>Cost target values during iterations.</sentence>
    <sentence>Module impacts on cost Fig 8.</sentence>
    <sentence>Module impacts on cost.</sentence>
    <sentence>Final module impacts on cost Fig 9.</sentence>
    <sentence>Final module impacts on cost.</sentence>
    <sentence>Slump The referenced GP Slump prediction has an RMSE of 20.7 cm in training without testing results (Baykasoglu et al., 2009).</sentence>
    <sentence>MNN Slump results are listed in Table 5 with training RMSE, training R2, testing RMSE, testing R2, and average CPU time.</sentence>
    <sentence>When an MNN of {NI = 4, NJ = 4, NK = 5} is adopted, result accuracy approximated that of GP’s Slump result in training.</sentence>
    <sentence>However, as none of the R2 results were adequate, this paper abandoned this case study for the MNN programming process.</sentence>
    <sentence>Civil engineers appreciate that concrete slump is a valuable index of concrete workability.</sentence>
    <sentence>Unfortunately, obtaining/learning this index is difficult both in terms of experimentation and prediction.</sentence>
    <sentence>Further research is needed, and the practical results of this case study reached their potential already.</sentence>
    <sentence>Table 5.</sentence>
    <sentence>Analyzed results of slump.</sentence>
    <sentence>Training RMSE (cm); Training R2; Testing RMSE (cm); Testing R2; CPU time (min) NJ 2 4 6 NI 2 23.797 23.691 23.587 0.350 0.356 0.361 25.618 23.468 24.842 0.536 0.611 0.564 1827 1817 1948 4 23.401 20.881 20.891 0.371 0.500 0.499 24.911 27.595 25.499 0.561 0.462 0.540 1806 2123 2237 6 21.804 20.760 18.206 0.454 0.505 0.620 26.806 25.514 23.741 0.492 0.540 0.602 2074 2298 2489</sentence>
  </section>
  <section name="Future studies">
    <sentence>Significant functions should be studied further in order for results to be applied to particular problems (like f in Eq (6)).</sentence>
    <sentence>The NI and NJ used in this study were fixed.</sentence>
    <sentence>This may be improved by using different numbers of MNN inputs to reduce programmed formula length.</sentence>
    <sentence>Besides, the present MNN is a multi-inputs-single-output (MISO) mapping tool.</sentence>
    <sentence>To improve MNN for multi-inputs-multi-outputs (MIMO) problems is essential in order to create a set of MNN formulas with modular relationship components (see Fig 10).</sentence>
    <sentence>Of course, an MIMO problem may be trained via repeated MISO learning executions.</sentence>
    <sentence>MISO and MIMO MNN Fig 10.</sentence>
    <sentence>MISO and MIMO MNN.</sentence>
    <sentence>As defined by (Tsai, 2009), the currently used MNN modules, which are three-layered HONN models, can be represented as a type of PW layer connection of which P indicates a high order layer connection and W denotes a linear layer connection.</sentence>
    <sentence>Certainly, some HONN derivatives (e.g., WP, PWW, WPW, WWP, polynomial neural networks (Fazel Zarandi, Türksen, Sobhani, &amp; Ramezanianpour, 2008), or other NN derivatives) may be employed.</sentence>
    <sentence>Such modifications will allow execution of different types of programmed formulas.</sentence>
    <sentence>In previous sections, each modular function type was used once.</sentence>
    <sentence>However the same function can be employed several times to program a function (O) such as: (18) Swarm intelligence approaches (e.g., particle swarm intelligence, fish swarm algorithms) may be employed to improve MNN optimization in terms of accuracy and computational efforts.</sentence>
    <sentence>Such improvements can facilitate the development of different programmed function types, which depend on various MNN designs.</sentence>
    <sentence>Work in this paper has focused only on developing the MNN programming concept.</sentence>
  </section>
  <section name="Conclusions">
    <sentence>This paper proposed a modular neural network (MNN) concept applicable to both artificial intelligent prediction and programming.</sentence>
    <sentence>The resulting programmed formulas are comprehensive in terms of their mathematic functions, weights and exponents.</sentence>
    <sentence>MNN greatly improves the abilities of neural networks (even high order neural network) to predict and program.</sentence>
    <sentence>Case studies focused on parameters specific to high-strength concrete.</sentence>
    <sentence>The merits of MNN in comparison to genetic programming include: 1.</sentence>
    <sentence>The modular approach of MNN makes it relatively easy to study the attributes of problems, and users may adopt any modular function type.</sentence>
    <sentence>After MNN learning, significant modular functions can be identified for particular problems.</sentence>
    <sentence>High order neural networks provide MNN programming to program problems in meaningful function types.</sentence>
    <sentence>Sequentially, MNN both improves on previous high order neural networks and creates an NN derivative against GP.</sentence>
    <sentence>To modify the number of NI, NJ, and NK, a simple formula can be programmed and parameter influence can be studied with good accuracy.</sentence>
    <sentence>Finally, this paper states that the proposed MNN is a useful branch of artificial intelligent programming which can be used for both problem prediction and programming problems with functions.</sentence>
    <sentence>1 Tel.</sentence>
    <sentence>: +886 2 27301073; fax: +886 2 27301074.</sentence>
  </section>
</article>
