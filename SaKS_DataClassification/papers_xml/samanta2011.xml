<article>
  <title>Prediction of chaotic time series using computational intelligence</title>
  <abstract>
    <sentence>In this paper, two CI techniques, namely, single multiplicative neuron (SMN) model and adaptive neuro-fuzzy inference system (ANFIS), have been proposed for time series prediction.</sentence>
    <sentence>A variation of particle swarm optimization (PSO) with co-operative sub-swarms, called COPSO, has been used for estimation of SMN model parameters leading to COPSO-SMN.</sentence>
    <sentence>The prediction effectiveness of COPSO-SMN and ANFIS has been illustrated using commonly used nonlinear, non-stationary and chaotic benchmark datasets of Mackey–Glass, Box–Jenkins and biomedical signals of electroencephalogram (EEG).</sentence>
    <sentence>The training and test performances of both hybrid CI techniques have been compared for these datasets.</sentence>
  </abstract>
  <keywords>
    <keyword>Time series prediction</keyword>
    <keyword>Single multiplicative neuron model</keyword>
    <keyword>Computational intelligence</keyword>
    <keyword>Particle swarm optimization</keyword>
    <keyword>Nonlinear time series</keyword>
    <keyword>Biomedical signal analysis</keyword>
  </keywords>
  <section name="Introduction">
    <sentence>Time series prediction involves predicting the system behavior in future based on information of the current and the past status of the system.</sentence>
    <sentence>Prediction of time series has widespread applications in the fields of science, engineering, medicine and econometrics, among others.</sentence>
    <sentence>Several methods have been used for prediction of real life complex, nonlinear time series commonly encountered in various such application domains (Box, Jenkins, &amp; Reinse, 1994; De Gooijer &amp; Hyndman, 2006; Mackey &amp; Glass, 1997).</sentence>
    <sentence>In recent years, there is also a growing interest in incorporating bio-inspired computational algorithms, commonly termed as computational intelligence (CI), in discovering knowledge from data, both in education and research (Haykin, 1999; Kennedy &amp; Eberhart, 1995; Kennedy, Eberhart, &amp; Shi, 2001; Poli, Kennedy, &amp; Blackwell, 2007; Samanta &amp; Nataraj, 2009, 2008).</sentence>
    <sentence>Among various CI techniques, artificial neural networks (ANNs) have been developed in form of parallel distributed network models based on biological learning process of the human brain.</sentence>
    <sentence>Among different types of ANNs, multi-layer perceptron (MLP) neural networks are quite popular (Haykin, 1999).</sentence>
    <sentence>Recently single multiplicative neuron (SMN) model has been proposed as an alternative to the general MLP type ANN.</sentence>
    <sentence>The SMN model derives its inspiration from the single neuron computation in neuroscience (Koch, 1997; Koch &amp; Segev, 2000).</sentence>
    <sentence>The SMN model is much simpler in structure than the more conventional multi-layer ANN and can offer better performances, if properly trained (Herz, Gollisch, Machens, &amp; Jaeger, 2006; Schmitt, 2001).</sentence>
    <sentence>However, the success of the SMN model depends on estimation of the model parameters in the training stage, similar to ANN.</sentence>
    <sentence>Another CI technique, namely, particle swam optimization (PSO) was proposed by Kennedy and Eberhart (1995) as a population based stochastic optimization technique inspired by the social behavior of bird flocking.</sentence>
    <sentence>PSO is a computationally simple algorithm based on group (swarm) behavior.</sentence>
    <sentence>The algorithm searches for an optimal value by sharing cognitive and social information among the individuals (particles).</sentence>
    <sentence>PSO has many advantages over evolutionary computation techniques like genetic algorithms in terms of simpler implementation, faster convergence rate and fewer parameters to adjust (Kennedy et al., 2001; Poli et al., 2007).</sentence>
    <sentence>The popularity of PSO is growing with applications in diverse fields of engineering, biomedical and social sciences, among others (Poli et al., 2007; Samanta &amp; Nataraj, 2009, 2008).</sentence>
    <sentence>In the present work, the SMN model parameters have been estimated using PSO (Yadav, Kalra, &amp; John, 2007; Zhao &amp; Yang, 2009).</sentence>
    <sentence>A variation of PSO with co-operative sub-swarms, COPSO, has been used in this work.</sentence>
    <sentence>The resulting combination is termed as COPSO-SMN.</sentence>
    <sentence>Fuzzy logic (FL) has been used in many practical engineering situations because of its capability in dealing with imprecise and inexact information (Yen &amp; Langari, 1999; Zadeh, 1965).</sentence>
    <sentence>The powerful aspect of fuzzy logic is that most of human reasoning and concept formation is translated into fuzzy rules.</sentence>
    <sentence>The combination of incomplete, imprecise information and the imprecise nature of the decision-making process make fuzzy logic very effective in modeling complex engineering, business, finance and management systems which are otherwise difficult to model.</sentence>
    <sentence>This approach incorporates imprecision and subjectivity in both model formulation and solution processes.</sentence>
    <sentence>The major issues involved in the application of FL or fuzzy inference system (FIS) are the selection of fuzzy membership functions (MFs), in terms of number and type, designing the rule base simulating the decision process as well as the scaling factors used in fuzzification and defuzzification stages.</sentence>
    <sentence>These parameters and the structures are, in general, decided based on multiple trials and expert knowledge.</sentence>
    <sentence>In adaptive neuro-fuzzy systems (ANFIS) proposed by Jang (1993), the advantages of FL and ANNs were combined for adjusting the MFs, the rule base and related parameters to fit the training dataset.</sentence>
    <sentence>In this paper, two CI techniques, COPSO-SMN and ANFIS, have been used for time series prediction.</sentence>
    <sentence>The prediction effectiveness of these techniques has been illustrated using commonly used nonlinear, non-stationary and chaotic benchmark datasets of Mackey–Glass, Box–Jenkins and biomedical signals of electroencephalogram (EEG) (http://www.cs.colostate.edu).</sentence>
    <sentence>The training and test performances of both hybrid CI techniques have been compared for these datasets.</sentence>
    <sentence>The rest of the paper is organized as follows.</sentence>
    <sentence>Section 2 briefly discusses the SMN model.</sentence>
    <sentence>In Section 3, the basic PSO algorithm is presented.</sentence>
    <sentence>A brief discussion on ANFIS is presented in Section 4.</sentence>
    <sentence>Section 5 presents the results and conclusions are in Section 6.</sentence>
  </section>
  <section name="Single multiplicative neuron (SMN) model">
    <sentence>Fig 1 shows the schematic of a general single multiplicative neuron (SMN) model with a learning algorithm for modeling a system with a single output y and the input vector x.</sentence>
    <sentence>The input vector x = {xi}with diagonal weight matrix W = [wii] and bias vector b = {bi} forms the intermediate vector p = {pi}, i = 1, n where n is the size of the input vector.</sentence>
    <sentence>The vector p goes through the multiplication node and gets transformed to y through the nonlinear function of logsig as follows: (1) (2) (3) (4) The aim of the SMN model is to minimize the error (e) between the target output yd and the model output for the same input vector.</sentence>
    <sentence>The model parameters wii and bi are adapted using the learning algorithm based on COPSO to minimize this error (e).</sentence>
    <sentence>Structure of single multiplicative neuron model with COPSO laerning (COPSO-SMN) Fig 1.</sentence>
    <sentence>Structure of single multiplicative neuron model with COPSO laerning (COPSO-SMN).</sentence>
  </section>
  <section name="Particle swarm optimization (PSO)">
    <sentence>Standard particle swarm optimization (PSO) In this section, a brief introduction to PSO algorithm is presented, for details text (Kennedy et al., 2001) can be referred to.</sentence>
    <sentence>Recent overviews of PSO and its variants are presented in Poli et al.</sentence>
    <sentence>(2007).</sentence>
    <sentence>For a problem with n-variables, each possible solution can be thought of as a particle with a position vector of dimension n. The population of m such individuals (particles) can be grouped as the swarm.</sentence>
    <sentence>Let xij and vij represent, respectively the current position and the velocity of ith particle (i = 1, m) in the jth direction (j = 1, n).</sentence>
    <sentence>The fitness of a particle is assessed by calculating the value of the target or the objective function for the current position of the particle.</sentence>
    <sentence>If the value of the objective function for the current position of the particle is better than its previous best value then the current position is designated as the new best individual (personal) location pbest, pbij.</sentence>
    <sentence>The best current positions of all particles are compared with the historical best position of the whole swarm (global or neighborhood) gbest, pbgj, in terms of the fitness function.</sentence>
    <sentence>The global best position is accordingly updated if any of the particle individual best (pbest, pbij) is better than the previous global best (gbest, pbgj).</sentence>
    <sentence>The current position and the velocity decide the trajectory of the particle.</sentence>
    <sentence>The velocity of the particle is influenced by three components, namely, inertial, cognitive and social.</sentence>
    <sentence>The inertial component controls the behavior of the particle in the current direction.</sentence>
    <sentence>The cognitive and the social components represent the particle’s memory of its personal best position (pbest) and the global best position (gbest).</sentence>
    <sentence>The velocity and the position of the particle are updated for the next iteration step (k + 1) from its values at current step k as follows: (5) (6) where U(0, 1) represents uniformly distributed random numbers in the range of (0, 1).</sentence>
    <sentence>These random numbers present the stochastic nature of the search algorithm.</sentence>
    <sentence>The constants c1 and c2 define the magnitudes of the influences on the particle velocity in the direction of the individual and the global optima.</sentence>
    <sentence>In this work, c1 = 2.0 and c2 = 2.0 were used.</sentence>
    <sentence>Co-operative particle swarm optimization (COPSO) In standard PSO, there is only one population (swarm).</sentence>
    <sentence>However, at times, especially for complex problems, it is advantageous to employ multiple co-operative swarms (sub-swarms).</sentence>
    <sentence>In this version, named as co-operative PSO (COPSO), multiple sub-swarms run in parallel to explore different segments of the search space and the particles exchange the gbest of all sub-swarms randomly in updating their velocity and position.</sentence>
    <sentence>The velocity updating Eq (5) is rewritten as follows: (7) where l = 1, … , s, s being the number of sub-swarms and r is a random integer between 1 and s, representing the random index of the sub-swarm whose gbest is selected in the velocity update.</sentence>
    <sentence>COPSO based learning of SMN model parameters The aim of the present approach is to select the SMN model parameters (wii and bi) such that an objective function representing the mean square error (MSE) is minimized.</sentence>
    <sentence>(8) where o is the observation (sample) index and N represents the total number of samples.</sentence>
    <sentence>In the present work, COPSO was used to select the SMN model parameters from a user-given range [−15, 15] for each minimizing the objective function (8).</sentence>
    <sentence>A population size of 30 individuals split equally in three sub-swarms was used starting with randomly generated particle positions and velocities.</sentence>
    <sentence>The objective function (8) was used as the fitness function.</sentence>
    <sentence>The maximum generation of 1000 was used as the termination criterion.</sentence>
  </section>
  <section name="Adaptive Neuro-Fuzzy inference system (ANFIS)">
    <sentence>In this section, the main features of ANFIS are briefly discussed.</sentence>
    <sentence>Readers are referred to Jang (1993) for details.</sentence>
    <sentence>A typical ANFIS structure for a system consisting of m inputs (x1, … , xm) each with n MFs, R rules and one output (y) is shown in Fig 2.</sentence>
    <sentence>In the case of the time-series prediction, the output is y = xt+r, i.e., the network is used to predict the series (y) r time steps ahead based on the current and the previous m values.</sentence>
    <sentence>For the present case of one step ahead prediction, r = 1.</sentence>
    <sentence>The network consisting of five layers is used for training Sugeno-type fuzzy inference system (FIS) through learning and adaptation.</sentence>
    <sentence>Number of nodes (N) in layer 1 is the product of numbers of inputs (m) and MFs (n) for each input, i.e., N = mn.</sentence>
    <sentence>Number of nodes in layers 2–4 is equal to the number of rules (R) in the fuzzy rule base.</sentence>
    <sentence>Basic structure of ANFIS Fig 2.</sentence>
    <sentence>Basic structure of ANFIS.</sentence>
    <sentence>It requires a training dataset of desired input/output pair (x1, x2, … , xm, y) depicting the target system to be modeled.</sentence>
    <sentence>ANFIS adaptively maps the inputs (x1, x2, … , xm) to the output (y) through MFs, the rule base and the related parameters emulating the given training dataset.</sentence>
    <sentence>It starts with initial MFs, in terms of type and number, and the rule base that can be designed intuitively.</sentence>
    <sentence>ANFIS applies a hybrid learning method for updating the FIS parameters.</sentence>
    <sentence>It utilizes the gradient descent approach to fine-tune the premise parameters that define MFs and applies the least-squares method to identify the consequent parameters that define the coefficients of each output equation in the Sugeno-type fuzzy rule base.</sentence>
    <sentence>The training process continues till the desired number of training steps (epochs) or the desired root mean squared error (RMSE) between the desired and the generated output is achieved.</sentence>
    <sentence>In the present work, two MFs of generalized bell type were used for each input variable.</sentence>
    <sentence>In this work, the maximum epoch and the RMSE target were set at 100 and 10−4, respectively.</sentence>
  </section>
  <section name="Results and discussions">
    <sentence>In this paper, the application of COPSO-SMN and ANFIS in time series prediction is illustrated using three datasets, namely, Mackey–Glass (MG) time series, Box–Jenkins gas furnace dataset and electroencephalogram (EEG) datasets (http://www.cs.colostate.edu).</sentence>
    <sentence>The prediction performances of the CI algorithms are compared.</sentence>
    <sentence>Mackey–Glass time series The dataset of chaotic, non-convergent time-series was generated using Mackey–Glass Eq (9) with initial condition of y(0) = 1.2 and delay time τ = 17.</sentence>
    <sentence>(9) The normalized response y(t) (within 1) of 950 data points after the initial transients was used to train and test both CI predictors.</sentence>
    <sentence>The aim was to predict y(k + 1) from the values of previous time steps y(k), y(k − 6), y(k − 12) and y(k − 18).</sentence>
    <sentence>The first 450 data points were used for training and the next 500 points were used for testing the generalization capability of the predictors.</sentence>
    <sentence>Fig 3 shows the variations of the performance index (J) for three sub-swarms over the generations for COPSO-SMN.</sentence>
    <sentence>All sub-swarms converged to a very small value within 500 generations.</sentence>
    <sentence>The first part of Table 1 shows the typical values of the SMN model weights and biases.</sentence>
    <sentence>Fig 4(a) and (b) shows the predicted time series for training and test using COPSO-SMN.</sentence>
    <sentence>Fig 5(a) and (b) shows the predicted time series tracking the target values quite closely in case of ANFIS.</sentence>
    <sentence>The prediction performance is represented in terms of normalized RMSE (NRMSE) which is the ratio of RMSE and the standard deviation of the target signal.</sentence>
    <sentence>The first part of Table 2 shows prediction NRMSE and the training time for both CI techniques.</sentence>
    <sentence>ANFIS performs better than COPSO-SMN both in terms of NRMSE and training time.</sentence>
    <sentence>Variation of performance index of sub-swarms Fig 3.</sentence>
    <sentence>Variation of performance index of sub-swarms.</sentence>
    <sentence>Table 1.</sentence>
    <sentence>Neural network parameters obtained from co-operative PSO module.</sentence>
    <sentence>Dataset Inputs Swarm 1 Swarm 2 Swarm 3 w b W b w b Mackey–Glass y(k) −0.275 −1.085 −0.261 −0.878 −0.563 −0.030 y(k − 6) 0.640 −0.140 −0.328 −1.062 −0.065 −0.720 y(k − 12) −2.387 −0.293 −4.409 1.093 −0.904 −6.045 y(k − 18) −10.920 5.524 −1.007 0.406 −9.120 4.869 Box Jenkins y(k − 1) −2.833 −3.518 −1.919 0.958 −0.676 0.337 u(k − 4) −0.857 0.399 −1.384 3.272 −2.508 7.972 EEG y(k − 1) 0.322 −0.477 −0.161 0.333 0.023 −0.327 y(k − 2) −0.245 0.517 −0.303 0.510 0.218 −0.511 y(k − 4) −0.945 −1.225 1.352 1.648 −0.035 1.777 y(k − 8) 15.000 −7.473 15.000 −7.515 15.000 −7.561 Prediction of Mackey–Glass time series using COPSO-MSN (a) training, (b) test Fig 4.</sentence>
    <sentence>Prediction of Mackey–Glass time series using COPSO-MSN (a) training, (b) test.</sentence>
    <sentence>Prediction of Mackey–Glass time series using ANFIS (a) training, (b) test Fig 5.</sentence>
    <sentence>Prediction of Mackey–Glass time series using ANFIS (a) training, (b) test.</sentence>
    <sentence>Table 2.</sentence>
    <sentence>Comparison of prediction performance between COPSO-SMN and ANFIS.</sentence>
    <sentence>Dataset Training/test data COPSO-SMN ANFIS Training time (s) NRMSE Training time (s) NRMSE Swarm 1 Swarm 2 Swarm 3 Mackey–Glass Training 52.86 0.3223 0.5121 0.3651 5.34 0.0064 Test 0.3243 0.5209 0.3621 0.0064 Box–Jenkins Training 18.33 0.2151 0.2150 0.2150 0.20 0.0374 Test 0.3416 0.3390 0.3416 0.0640 EEG Training 91.54 0.5378 0.5364 0.5357 8.63 0.1565 Test 0.5762 0.5724 0.5618 0.2189 5.2.</sentence>
    <sentence>Box –Jenkins gas furnace data The Box–Jenkins dataset represents the CO2 concentration as output, y(t), in terms of input gas flow rate, u(t), from a combustion process of a methane-air mixture (Box et al., 1994).</sentence>
    <sentence>From a total set of 296 data pairs, the first 140 data points were used for training and the next 140 data points were used for test.</sentence>
    <sentence>The aim is to predict y(k) in terms of y(k − 1) and u(k − 4).</sentence>
    <sentence>Table 1 presents the SMN parameters of the trained model.</sentence>
    <sentence>Table 2 shows the training and test results.</sentence>
    <sentence>Again, ANFIS outperforms COPSO-SMN for this dataset.</sentence>
    <sentence>Figs.</sentence>
    <sentence>6 and 7 show the predicted time series with COPSO-SMN and ANFIS, respectively.</sentence>
    <sentence>Prediction of Box Jenkins time series using COPSO-SMN (a) training, (b) test Fig 6.</sentence>
    <sentence>Prediction of Box Jenkins time series using COPSO-SMN (a) training, (b) test.</sentence>
    <sentence>Prediction of Box Jenkins time series using ANFIS (a) training, (b) test Fig 7.</sentence>
    <sentence>Prediction of Box Jenkins time series using ANFIS (a) training, (b) test.</sentence>
    <sentence>Electroencephalogram (EEG) dataset EEG dataset (http://www.cs.colostate.edu) was also used to illustrate the procedure.</sentence>
    <sentence>Both training and test datasets consist of 750 samples.</sentence>
    <sentence>The aim is to predict y(k) in terms of y(k − 1), y(k − 2), y(k − 4) and y(k − 8) using the CI predictors.</sentence>
    <sentence>Table 1 shows the COPSO-SMN parameters and Table 2 shows the prediction performance.</sentence>
    <sentence>The error levels are higher for both SMN and ANFIS compared to the first two datasets.</sentence>
    <sentence>ANFIS performs better than SMN also for this dataset.</sentence>
    <sentence>Figs.</sentence>
    <sentence>8 and 9 show the predicted EEG signals for COPSO-SMN and ANFIS, respectively.</sentence>
    <sentence>Prediction of EEG time series using COPSO-SMN (a) training, (b) test Fig 8.</sentence>
    <sentence>Prediction of EEG time series using COPSO-SMN (a) training, (b) test.</sentence>
    <sentence>Prediction of EEG time series using ANFIS (a) training, (b) test Fig 9.</sentence>
    <sentence>Prediction of EEG time series using ANFIS (a) training, (b) test.</sentence>
  </section>
  <section name="Conclusions">
    <sentence>Results are presented for prediction of nonlinear, chaotic and non-stationary time series using two bio-inspired computational intelligence techniques.</sentence>
    <sentence>The single multiplicative neuron model parameters were estimated using a learning algorithm based on a cooperative particle swarm optimization PSO.</sentence>
    <sentence>Though both techniques show reasonably good results, ANFIS performs better than COPSO-SMN for all three datasets.</sentence>
    <sentence>The role of bio-inspired CI techniques in time series prediction is illustrated using three well known benchmark datasets.</sentence>
  </section>
</article>
