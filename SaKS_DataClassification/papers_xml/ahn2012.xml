<article>
  <title>Using ridge regression with genetic algorithm to enhance real estate appraisal forecasting</title>
  <abstract>
    <sentence>This study considers real estate appraisal forecasting problem.</sentence>
    <sentence>While there is a great deal of literature about use of artificial intelligence and multiple linear regression for the problem, there has been always controversy about which one performs better.</sentence>
    <sentence>Noting that this controversy is due to difficulty finding proper predictor variables in real estate appraisal, we propose a modified version of ridge regression, i.e., ridge regression coupled with genetic algorithm (GA-Ridge).</sentence>
    <sentence>In order to examine the performance of the proposed method, experimental study is done for Korean real estate market, which verifies that GA-Ridge is effective in forecasting real estate appraisal.</sentence>
    <sentence>This study addresses two critical issues regarding the use of ridge regression, i.e., when to use it and how to improve it.</sentence>
  </abstract>
  <keywords>
    <keyword>Ridge regression</keyword>
    <keyword>Genetic algorithm</keyword>
    <keyword>Real estate market</keyword>
  </keywords>
  <section name="Introduction">
    <sentence>In recent years, interest in performance of real estate markets and real estate investment trusts (REITs) has grown up so fast and tremendously as they are usually required for asset valuation, property tax, insurance estimations, sales transactions, and estate planning.</sentence>
    <sentence>Conventionally, sales comparison approach has been widely accepted to forecast residential real estate.</sentence>
    <sentence>The sales comparison grid method, however, is often questioned for relying too much on subjective judgments for obtaining reliable and verifiable data (Wiltshaw, 1995).</sentence>
    <sentence>As a consequence, multiple linear regression (MLR) based on related predictors has been considered as a rigorous alternative enhancing predictability of real estate and property value, which immediately faces criticism such as nonlinearity within the data, multicollinearity issues in the predictor variables and the inclusion of outlier in the sample.</sentence>
    <sentence>As is often the case with other financial forecasting problems, this criticism has prompted researchers to resort to artificial neural network (ANN) as another logical alternative (Ahn, Lee, Oh, &amp; Kim, 2009; Chen &amp; Du, 2009; Dong &amp; Zhou, 2008; Lee, Booth, &amp; Alam, 2005; Lu, 2010; Oh &amp; Han, 2000; Versace, Bhatt, Hinds, &amp; Shiffer, 2004).</sentence>
    <sentence>The follow-up studies observe, however, that either ANN or MLR fails to report a dominating performance than the other, i.e., ANN excels MLR in some cases while MLR excels ANN in other cases (Dehghan, Sattari, Chehreh, &amp; Aliabadi, 2010; Hua, 1996; Nguyen &amp; Cripps, 2001; Worzala, Lenk, &amp; Silva, 1995).</sentence>
    <sentence>In this study, it will be shown that this confusing episode appears due to difficulty finding proper predictor variables and could be resolved quite successfully by a modified version of ridge regression, i.e., ridge regression coupled with genetic algorithm (GA-Ridge).</sentence>
    <sentence>Theoretically as well as practically, there has been widespread strong objection to arbitrary use of ridge regression.</sentence>
    <sentence>The main criticisms are twofold.</sentence>
    <sentence>Firstly, though it is well known that ridge regression is effective for the case where the unknown parameters (or the linear coefficients) are known a priori to have small modulus values, it is hard to obtain or implement such prior information.</sentence>
    <sentence>Secondly, blind use of ridge regression is likely to change any non-significant predictor variable into significant one easily.</sentence>
    <sentence>Our study addresses these two critical issues and proposes GA-Ridge as a measure that takes care of them nicely.</sentence>
    <sentence>The rest of the study is divided as follows.</sentence>
    <sentence>Section 2 discusses background of this article involving difficulty finding proper predictor variables in real estate forecasting.</sentence>
    <sentence>Section 3 is devoted to detailed description of the proposed GA-Ridge and discusses its effectiveness for handling the two critical issues of ridge regression.</sentence>
    <sentence>In Section 4, GA-Ridge is experimented in the Korean real estate market to demonstrate its effectiveness.</sentence>
    <sentence>Lastly, the concluding remarks are given in Section 5.</sentence>
  </section>
  <section name="Background">
    <sentence>Predictor variable for real estate forecasting Forecasting of asset pricing is a major issue in real estate practice (Bourassa, Cantoni, &amp; Hoesli, 2010; Chica-Olmo, 2007; McCluskey &amp; Anand, 1999; O’Roarty et al., 1997; Peterson &amp; Flanagan, 2009; Tay &amp; Ho, 1994; Wilson, Paris, Ware, &amp; Jenkins, 2002).</sentence>
    <sentence>Property development relies on prediction of expected costs and returns (Allen, Madura, &amp; Springer, 2000; Evans, James, &amp; Collins, 1993; Juan, Shin, &amp; Perng, 2006; McCluskey &amp; Anand, 1999; Pivo &amp; Fisher, 2010).</sentence>
    <sentence>Property and facilities managers need forecasts of supply and demand as well as of cost and return.</sentence>
    <sentence>Funds and investment managers rely on forecasts of the present and future values of real estate in terms of economic activities.</sentence>
    <sentence>In these real estate forecasting problems, there has been a hot controversy over superiority of ANN over MLR as proper tool since use of ANN for residential valuation was first suggested by Jensen (1990).</sentence>
    <sentence>Rossini (1997) seeks to assess the application of ANN and MLR to residential valuation and supports the use of MLR while Do and Grudnitski (1992) suggests ANN as s superior technique.</sentence>
    <sentence>Worzala et al.</sentence>
    <sentence>(1995) notices that while ANN slightly outperforms MLR in some cases, the difference between the two is insignificant.</sentence>
    <sentence>Hua (1996) and Brooks and Tsolacos (2003) support ANN over MLR with some cautionary note on predictor variables and McGreal, Berry, McParland, and Turner (2004) expresses skepticism about ANN.</sentence>
    <sentence>Noting that ANN is designed mainly for the purpose of modeling any functional relationship (or ANN is designed mainly to correct modeling bias or assumption), ANN is expected to excel MLR when there is significant modeling bias from linear model, while MLR is expected to excel ANN otherwise.</sentence>
    <sentence>Thus no clear-cut superiority between ANN and MLR implicitly suggests that other source of trouble might exist than incorrect modeling in real estate appraisal forecasting.</sentence>
    <sentence>One possible source of trouble is difficulty finding significant and reliable predictors as discussed by several authors.</sentence>
    <sentence>Rossini (1997) noticed that quantitative predictor variables such as past sale price, land area, rooms and year of construction tend to suffer from lack of qualitative measures, while qualitative predictor variables such as building style and environments are frequently rather simplistic and fail to capture sufficient information.</sentence>
    <sentence>Similar observations are made by Brooks and Tsolacos (2003) and McGreal et al.</sentence>
    <sentence>(2004).</sentence>
    <sentence>In particular, Brooks and Tsolacos (2003) noticed that significant predictors depend on the used methodology.</sentence>
    <sentence>These discussions altogether suggest clearly that finding a proper set of predictor variables is hard in real estate appraisal and it would be highly desirable to take care of this predictor selection problem technically.</sentence>
    <sentence>Ridge regression Ridge regression is known as a very useful tool for alleviating multicolinearity problem (Walker &amp; Birch, 1988).</sentence>
    <sentence>Its formal formulation is given as one of least squares subject to a specific type of restrictions on the parameters.</sentence>
    <sentence>The standard approach to solve an overdetermined system of linear equations: is known as linear least squares and seeks to minimize the residual: where Y is n × 1 vector, X is n × p matrix (), β is p × 1 vector and is Euclidean norm.</sentence>
    <sentence>However, the matrix X may be ill conditioned or singular yielding a non-unique solution.</sentence>
    <sentence>In order to give preference to a particular solution with desirable properties, the regularization term is included in this minimization: This regularization improves the conditioning of the problem, thus enabling a numerical solution.</sentence>
    <sentence>An explicit solution, denoted by , is given by (1) where k is a positive number.</sentence>
    <sentence>In applications, the interesting values of k usually lie in the range of (0, 1).</sentence>
    <sentence>This procedure is called ridge regression.</sentence>
    <sentence>It is well known that ridge regression can be regarded as an estimation of β from the data subject to prior knowledge that smaller values in modulus of the β s are more likely than larger values, and that larger and larger values of the β s are more and more unlikely.</sentence>
    <sentence>Thus ridge regression is quite useful when smaller values in modulus of the β s are expected more than larger values.</sentence>
    <sentence>In this context, one major drawback of ridge regression is the “unchecked arbitrariness” when it is implemented in practice.</sentence>
    <sentence>Indeed the characteristic effect of the ridge regression procedure is to change any non-significant estimated β to the significant estimated β and hence it is questionable that much real improvement can be really achieved by such a procedure.</sentence>
    <sentence>Refer to Draper and Smith (1981).</sentence>
    <sentence>Genetic algorithm GA is a stochastic search technique that can explore large and complicated spaces on the ideas from natural genetics and evolutionary principle (Goldberg, 1989; Holland, 1975; Oh, Kim, &amp; Min, 2005).</sentence>
    <sentence>It has been demonstrated to be effective and robust in searching very large spaces in a wide range of applications (Koza, 1993).</sentence>
    <sentence>GA is particularly suitable for multi-parameter optimization problems with an objective function subject to numerous hard and soft constraints.</sentence>
    <sentence>GA performs the search process in four stages: (i) initialization, (ii) selection, (iii) crossover, and (iv) mutation (Wong &amp; Tan, 1994).</sentence>
    <sentence>In the initialization stage, a population of genetic structures (known as chromosomes) that are randomly distributed in the solution space is selected as the starting point of the search.</sentence>
    <sentence>After the initialization stage, each chromosome is evaluated using a user-defined fitness function.</sentence>
    <sentence>The goal of the fitness function is to encode numerically the performance of the chromosome.</sentence>
    <sentence>For real-world applications of optimization methods such as GA, the choice of the fitness function is the most critical step.</sentence>
    <sentence>In this paper, GA is employed for finding optimal k and proper set of predictors simultaneously.</sentence>
  </section>
  <section name="GA-Ridge algorithm">
    <sentence>Procedure of GA-Ridge algorithm is described.</sentence>
    <sentence>A general multiple linear regression model is represented as follows: (2) where Dj (j = 1, … , p) equals to either 0 or 1 according to the inclusion of predictor variable Xj(j = 1, … , p) in the model (2).</sentence>
    <sentence>Then, for numerous combinations of and various values of 0 &lt; k &lt; 1, the optimal is searched by GA, i.e., (3) where , is a n × q matrix with and .</sentence>
    <sentence>Thus final GA-Ridge regression estimator is: (4) Remark 1 Ridge regression is preferred when one expects smaller β s in modulus.</sentence>
    <sentence>Note that insignificant β s might be included in smaller β s in modulus.</sentence>
    <sentence>Problem is that it is usually hard to determine such situation effectively because smaller β s in modulus requires a subjective judgment.</sentence>
    <sentence>To resolve this problem, we propose to use ridge regression “when neither ANN nor MLR excels the other that significantly”.</sentence>
    <sentence>This litmus rule for using ridge regression is based on the fact that the quoted situation arises when smaller β s in modulus are very likely or reliable predictor variables are hard to find.</sentence>
    <sentence>Note that this litmus rule is desirable because it does not depend on subjective judgment that much.</sentence>
    <sentence>Refer to Section 4 for how to implement the litmus rule in practice.</sentence>
    <sentence>Remark 2 One strong criticism against ridge regression is that in (1) arbitrarily changes the non-significant estimated β to the significant estimated β. of (4) prevents this unchecked arbitrariness effectively since the optimal k and the optimal predictor variables are searched “simultaneously”.</sentence>
    <sentence>Note that GA plays a key role for GA-Ridge algorithm because it is particularly suitable for multi-parameter optimization problems with an objective function subject to numerous hard and soft constraints.</sentence>
  </section>
  <section name="Empirical studies">
    <sentence>Experimental setting In this experimental study, forecasting of home sales index (HSI) and home rental index (HRI) in the Korean real estate market are considered.</sentence>
    <sentence>These monthly indexes are produced and maintained by the KB bank, one of the major banks in Korea, for the purpose of monitoring real estate market movement.</sentence>
    <sentence>In this study forecasting analysis of HSI and HRI covers a 14-year period from July 1996 to December 2009.</sentence>
    <sentence>In order to evaluate the forecasting accuracy of GA-Ridge algorithm under different experimental situations, “moving window scheme” is employed.</sentence>
    <sentence>Indeed moving window which is a block of time series data of size l comprising the first sub-block of size l1 and the second sub-block of size l2 (i.e., l = l1 + l2), moves by size l2 each time and thus each moving window of size l overlaps the next window by size l2.</sentence>
    <sentence>Here the latter sub-block of size l2 is held out for evaluation purpose while GA-Ridge algorithm is implemented for the former sub-block of size l1.</sentence>
    <sentence>The moving window scheme with 10 windows is illustrated in Table 1 and Fig 1.</sentence>
    <sentence>Refer also to Jang, Lai, Jiang, Pan, and Chien (1993) and Hwarng (2001).</sentence>
    <sentence>Table 1.</sentence>
    <sentence>Training and testing period for moving window scheme.</sentence>
    <sentence>Window number Training period Testing period 1 1996.07–2004.12 2005.01–2005.06 2 1997.01–2005.06 2005.07–2005.12 3 1997.07–2005.12 2006.01–2006.06 4 1998.01–2006.06 2006.07–2006.12 5 1998.07–2006.12 2007.01–2007.06 6 1999.01–2007.06 2007.07–2007.12 7 1999.07–2007.12 2008.01–2008.06 8 2000.01–2008.06 2008.07–2008.12 9 2000.07–2008.12 2009.01–2009.06 10 2000.01–2009.06 2009.07–2009.12 Moving window scheme Fig 1.</sentence>
    <sentence>Moving window scheme.</sentence>
    <sentence>For experimenting GA-Ridge algorithm with monthly HRI or monthly HSI as predicted variables, the predictor variables used for monitoring economic condition in Ahn, Oh, Kim, and Kim (2011) are employed as predictors here.</sentence>
    <sentence>Indeed the three major economic variables, foreign exchange rates, interest rates, and stock market index, and the key macroeconomic predictors such as GDP and trade balance with their derivations are included to obtain 22 predictors.</sentence>
    <sentence>Refer to Table 2.</sentence>
    <sentence>Note that all the predictors are monthly data and are developed for the purpose of monitoring the Korean economic conditions by Ahn et al.</sentence>
    <sentence>(2011).</sentence>
    <sentence>What is behind this selection of predictor is that economic condition itself is obviously quite influential on the real estate market but hard to quantify as a single predictor.</sentence>
    <sentence>Thus it is decomposed as the predictors in Table 2 instead.</sentence>
    <sentence>Table 2.</sentence>
    <sentence>List of predictor variables.</sentence>
    <sentence>Selected predictor Explanation X1 Note default rate (NDR) Using raw data X2 Size of the run of increasing X1 during the latest 12 months X3 Change rate of foreign exchange holdings (FEH) Ratio of the current month to the same month of the last year X4 Change rate of money stock Ratio of the current month to the same month of the last year X5 Change rate of producer price index Ratio of the current month to the same month of the last year X6 Change rate of consumer price index Ratio of the current month to the same month of the last year X7 Change rate of balance of trade Ratio of the current month to the same month of the last year X8 Change rate of index of industrial production Ratio of the current month to the same month of the last year X9 Size of the run of decreasing X8 during the latest 12 months X10 Change rate of index of producer shipment Ratio of the current month to the same month of the last year X11 Change rate of index of equipment investment Ratio of the current month to the same month of the last year X12 FEH per gross domestic products FEH/GDP X13 Size of the run of decreasing X12 during the latest 12 months X14 Size of the run of decreasing monthly change of X12 during the latest 12 months X15 Change rate of FEH per GDP Ratio of the current month to the same month of the last year X16 Balance of trade per GDP BOT/GDP X17 Size of the run of increasing X16 during the latest 16 months X18 Size of the run of negative X16 during the latest 16 months X19 Balance of payments (direct Investment) Use raw data X20 Balance of payments (securities investment) Use raw data X21 Other balance of payments Use raw data X22 Amount of foreigners’ investment in the stock market Use raw data In order to evaluate the forecasting accuracy, the following three distance metrics (5)–(7) are employed: root mean squared error (RMSE), mean absolute error (MAE), and mean absolute percentage error (MAPE): (5) (6) (7) where Yt and are respectively the actual and forecasted value of HSI or HRI at time t. While both MAE and RMSE are simply measures of discrepancies between the predicted values and the actual observations, MAPE measures scaled discrepancies at each t. 4.2.</sentence>
    <sentence>Experimental results Forecasting analysis is done for both HSI and HRI.</sentence>
    <sentence>Since forecasting analysis result of HRI is quite similar to HSI, detailed forecasting analysis of HSI is given first and then a brief summary of HRI forecasting analysis is given later.</sentence>
    <sentence>As a prior validity check for using GA-Ridge for HSI forecasting, we examined two things.</sentence>
    <sentence>Firstly, we compare performance of MLR and ANN, which is our litmus rule for using GA-Ridge (refer to Remark 1).</sentence>
    <sentence>Fig 2 shows that neither ANN nor MLR excels its counterpart uniformly throughout 10 windows.</sentence>
    <sentence>Recall that our experiments use moving window scheme with 10 windows as described in Fig 1.</sentence>
    <sentence>Secondly, significances of the 22 predictors are tested individually when they are employed for MLR at each window (see Table 3).</sentence>
    <sentence>Table 3 shows that most of the predictors are not significant and significant predictors found at each window vary.</sentence>
    <sentence>In addition it shows that the estimated coefficients are close to zero in their modulus.</sentence>
    <sentence>For editorial purpose, test results for only window 1 and 10 are given in Table 3 (others point out similar things).</sentence>
    <sentence>This is not really surprising because each predictor constituting the current economic condition together is expected to have indirect influence though the current economic condition itself has evidently great influence on HSI.</sentence>
    <sentence>As a result, it seems to be technically as well as intuitively desirable to employ GA-Ridge as appropriate method for forecasting on this particular problem.</sentence>
    <sentence>Performance comparison of ANN and MLR for HSI forecasting during evaluation… Fig 2.</sentence>
    <sentence>Performance comparison of ANN and MLR for HSI forecasting during evaluation period.</sentence>
    <sentence>Table 3.</sentence>
    <sentence>Significance test for 22 predictor variables when MLR is used for HSI forecasting.</sentence>
    <sentence>Predictor Coefficient p-Value (a) Window No.</sentence>
    <sentence>1 X1 −0.0014402 0.5826 X2 0.0010064 0.3458 X3 −0.058566 0.0553 X4 0.054909 0.2217 X5 −0.082096 0.0757 X6 −0.053088 0.655 X7 2.686E−05 0.411 X8 0.017776 0.7411 X9 −0.0008066 0.326 X10 −0.039748 0.4673 X11 0.012778 0.1496 X12 7.649E−05 0.1309 X13 0.0002201 0.7276 X14 0.003623 0.0005 X15 0.071041 0.0166 X16 −1.9552 0.0004 X17 0.0007475 0.396 X18 −0.0022285 0.008 X19 −1.94E−06 0.1587 X20 −4.56E−07 0.358 X21 −3.37E−07 0.4 X22 −1.54E−09 0.0455 (b) Window No.</sentence>
    <sentence>10 X1 −0.0047141 0.6146 X2 0.0004101 0.6759 X3 −0.076859 0.2488 X4 0.094025 0.048 X5 −0.11661 0.0647 X6 0.14433 0.3806 X7 −4.36E−05 0.46 X8 0.15811 0.0313 X9 0.0012202 0.2552 X10 −0.15055 0.0569 X11 0.012823 0.3028 X12 −5.252E−05 0.2384 X13 0 X14 0.0009064 0.4315 X15 0.09998 0.1568 X16 −0.23245 0.6975 X17 −0.0016523 0.0887 X18 −0.0006417 0.7232 X19 −3.88E−07 0.6875 X20 1.83E−07 0.642 X21 1.13E−07 0.654 X22 −2.76E−10 0.5506 For forecasting performance comparison with GA-Ridge, three other forecasting methods are considered: MLR, Pure Ridge regression and ANN.</sentence>
    <sentence>Pure ridge regression method is considered here in order to assess how effectively GA-Ridge resolves the unchecked arbitrariness of ridge regression mentioned in Remark 2.</sentence>
    <sentence>Fig 3 depicts the forecasting result when each method is employed during the testing periods of the moving window scheme.</sentence>
    <sentence>Note that the testing periods are connected continuously without any time break, starting from January 2005 (refer to Fig 1).</sentence>
    <sentence>Fig 3 is summarized by Table 4 numerically which calculates RMSE, MAE and MAPE values from Fig 3 for evaluating the performances of the four methods.</sentence>
    <sentence>It is easy to notice from Table 4 that GA-Ridge is superior to the other methods across the three distance metrics.</sentence>
    <sentence>To understand things better, mean difference tests (or paired t-tests) are done for 6 pairs out of the 4 methods.</sentence>
    <sentence>Indeed from calculation of MASE, a set of data for j = 1 (GA Ridge), j = 2 (MLR), j = 3 (Pure Ridge) and j = 4 (ANN) are obtained and paired tests are done for six pairs (W1, W2), (W1, W3), (W1, W4), (W2, W3), (W2, W4) and (W3, W4).</sentence>
    <sentence>Similar procedure is done for MAE and MAPE.</sentence>
    <sentence>Results of these paired tests given in Table 5 verify that the performances of the four methods are significantly different from each other except the pair (ANN, MLR).</sentence>
    <sentence>This together with Fig 2 and Table 4 confirms the superior performance of GA-Ridge and the insignificant difference between performances of MLR and ANN.</sentence>
    <sentence>Finally the predictors selected by GA-Ridge are examined for their significance at each window in Table 6, which shows that almost all the selected predictors are changed to significant ones in GA-Ridge method.</sentence>
    <sentence>Again for editorial purpose results for windows 1, 2 and 10 are given.</sentence>
    <sentence>Predicted vs actual HSI during testing period Fig 3.</sentence>
    <sentence>Predicted vs actual HSI during testing period.</sentence>
    <sentence>Table 4.</sentence>
    <sentence>Numerical comparison of four forecasting methods for HSI.</sentence>
    <sentence>Distance metric GA-Ridge Multiple regression Ridge regression ANN RMSE 0.0074 0.0104 0.0088 0.0110 MAE 0.0055 0.0086 0.0069 0.0088 MAPE 239.66 304.91 244.72 291.05 Table 5. p-Values of 6 paired t-tests for four forecasting methods on HIS.</sentence>
    <sentence>GA-Ridge Multiple regression Ridge regression ANN (a) MSE GA-Ridge – 0.000⁎ 0.000⁎ 0.000⁎ Multiple regression – 0.003⁎ 0.266 Ridge regression – 0.009⁎ ANN – (b) MAE GA-Ridge – 0.000⁎ 0.001⁎ 0.000⁎ Multiple regression – 0.000⁎ 0.421 Ridge regression – 0.009⁎ ANN – (c) MAPE GA-Ridge – 0.000⁎ 0.000⁎ 0.001⁎ Multiple regression – 0.011⁎ 0.425 Ridge regression – 0.043⁎ ANN – ⁎ Significant at 5%.</sentence>
    <sentence>Table 6.</sentence>
    <sentence>Significance test for the selected variables when GA-Ridge is used for HSI forecasting.</sentence>
    <sentence>Selecting variables Coefficient p-Value Ridge value (a) Window No.</sentence>
    <sentence>1 X1 −0.0037 0.0831 0.0019 X2 0.0013 0.0747 X3 −0.0381 0.0045 X5 −0.0984 0.0001 X14 0.0025 0.0042 X15 0.0473 0.0009 X16 −1.7609 0.0001 X18 −0.0019 0.0001 X22 −1.93E−09 0.0007 (b) Window No.</sentence>
    <sentence>2 X3 −0.0416 0.0234 0.0007 X4 0.0539 0.1198 X5 −0.0767 0.0013 X9 −0.0011 0.0501 X12 0.0001 0.0192 X14 0.0021 0.0004 X15 0.0522 0.0055 X16 −2.1558 0.0001 X18 −0.0017 0.0001 X19 −0.000002 0.187 X20 −0.0000005 0.2578 X22 −9.83E−10 0.1143 (c) Window No.</sentence>
    <sentence>10 X4 0.0545 0.0314 0.0027 X5 −0.1291 0.0001 X6 0.1397 0.1571 X8 0.1324 0.0054 X9 0.0011 0.1278 X10 −0.1152 0.0169 X16 −0.5481 0.2067 X17 −0.0019 0.0372 The above comparison studies altogether indicate the followings: (i) ANN and MLR equally match.</sentence>
    <sentence>(ii) Pure ridge is improved significantly by GA-Ridge.</sentence>
    <sentence>(iii) GA-Ridge excels others easily.</sentence>
    <sentence>Note that (i) recommends the use of GA-Ridge (see Remark 1) while (ii) implies the checked arbitrariness of pure ridge by GA-Ridge (see Remark 2).</sentence>
    <sentence>For forecasting analysis of HRI, almost identical steps are done.</sentence>
    <sentence>Fig 4 shows that neither ANN nor MLR excels its counterpart uniformly throughout 10 windows.</sentence>
    <sentence>Significances of the 22 predictor variables are tested for HRI in Table 7, which suggests that most of predictor variables have weak influence on HRI though some of them show strong significance depending on window.</sentence>
    <sentence>For forecasting performance comparison, the four forecasting methods are considered again.</sentence>
    <sentence>Fig 5 depicts the forecasting result when each method is employed during the testing periods of the moving window scheme.</sentence>
    <sentence>Then Fig 5 is summarized by Table 8 numerically, which confirms that GA-Ridge is superior to the other methods across the three distance metrics.</sentence>
    <sentence>Again, mean difference tests (or paired t-tests) are done for 6 pairs out of the four methods from calculation of MASE, MAE and MAPE.</sentence>
    <sentence>Results of the paired tests in Table 9 verify that performances of the four methods are significantly different from each other except the pair (ANN, MLR).</sentence>
    <sentence>Finally the predictors and the ridge value k selected by GA-Ridge are examined for their significance at each window in Table 10 which shows almost all the selected predictors are changed to significant ones in GA-Ridge method.</sentence>
    <sentence>Performance comparison of ANN and MLR for HRI forecasting during testing period Fig 4.</sentence>
    <sentence>Performance comparison of ANN and MLR for HRI forecasting during testing period.</sentence>
    <sentence>Table 7.</sentence>
    <sentence>Significance test for 22 predictor variables when MLR is used for HRI forecasting.</sentence>
    <sentence>Predictor Coefficient p-Value (a) Window No.</sentence>
    <sentence>1 X1 −0.00005 0.9909 X2 0.00204 0.2479 X3 0.06572 0.1896 X4 −0.06273 0.3963 X5 −0.09897 0.1922 X6 −0.05789 0.7676 X7 −4.231E−05 0.4324 X8 0.07073 0.4263 X9 0.00073 0.5915 X10 −0.02647 0.7689 X11 −0.01088 0.4547 X12 −0.00011 0.1922 X13 0.00041 0.6975 X14 0.00396 0.0185 X15 −0.05529 0.2516 X16 −2.65423 0.0031 X17 −0.00029 0.8365 X18 −0.00357 0.0099 X19 −3.072E−06 0.1759 X20 −2.065E−06 0.0131 X21 −4.27E−07 0.5178 X22 −7.82E−10 0.5339 (b) Window No.</sentence>
    <sentence>10 X1 0.01136 0.2566 X2 −7.792E−06 0.994 X3 −0.07624 0.2827 X4 0.12016 0.0183 X5 −0.07448 0.2648 X6 0.06548 0.7083 X7 −6.922E−05 0.2719 X8 0.11270 0.1465 X9 0.00070 0.4835 X10 −0.08147 0.3294 X11 −0.01323 0.3183 X12 −2.074E−05 0.6609 X13 0 0.0000 X14 0.00097 0.4296 X15 0.08063 0.2826 X16 −0.04988 0.9376 X17 −0.00204 0.0491 X18 −0.00170 0.3789 X19 −3.71E−07 0.7183 X20 −3.33E−07 0.4286 X21 −1.21E−07 0.6512 X22 −8.52E−11 0.8627 Predicted vs actual HRI during testing period Fig 5.</sentence>
    <sentence>Predicted vs actual HRI during testing period.</sentence>
    <sentence>Table 8.</sentence>
    <sentence>Comparison of four forecasting methods for HRI.</sentence>
    <sentence>Distance metric GA-Ridge Multiple regression Ridge regression ANN RMSE 0.0057 0.0111 0.0070 0.0112 MAE 0.0045 0.0079 0.0051 0.0085 MAPE 131.14 208.97 140.73 242.38 Table 9. p-Values of 6 paired t-tests for four forecasting methods on HRI.</sentence>
    <sentence>GA-Ridge Multiple regression Ridge regression ANN (a) MSE GA-Ridge – 0.003⁎ 0.020⁎ 0.000⁎ Multiple regression – 0.002⁎ 0.470 Ridge regression – 0.000⁎ ANN – (b) MAE GA-Ridge – 0.000⁎ 0.022⁎ 0.000⁎ Multiple regression – 0.000⁎ 0.298 Ridge regression – 0.001⁎ ANN – (c) MAPE GA-Ridge – 0.000⁎ 0.018⁎ 0.002⁎ Multiple regression – 0.000⁎ 0.151 Ridge regression – 0.049⁎ ANN – ⁎ Significant at 5%.</sentence>
    <sentence>Table 10.</sentence>
    <sentence>Significance test for the selected variables when GA-Ridge is used for HRI forecasting.</sentence>
    <sentence>Selecting variables Coefficient p-Value Ridge value (a) Window No.</sentence>
    <sentence>1 X2 0.0007 0.4126 0.01396 X3 0.0088 0.0968 X4 −0.1116 0.0107 X5 −0.1686 0.0001 X7 −0.0000 0.2943 X12 −0.0001 0.0036 X14 0.0019 0.0428 X16 −2.3114 0.0001 X18 −0.0019 0.0009 X19 −0.0000 0.3463 X20 −0.0000 0.0005 (b) Window No.</sentence>
    <sentence>2 X1 0.0041 0.0964 0.01531 X3 0.01289 0.0001 X5 −0.1242 0.0003 X7 −0.0001 0.0453 X10 0.0228 0.0068 X14 0.0032 0.0006 X16 −2.4530 0.0001 X18 −0.0017 0.0002 X19 −0.0000 0.1365 X20 −0.0000 0.0001 (c) Window No.</sentence>
    <sentence>9 X4 0.0673 0.0183 0.0005 X5 −0.1591 0.0001 X6 0.2497 0.0339 X7 −0.0001 0.1600 X8 0.0356 0.0173 X9 0.0015 0.0465 X14 0.0012 0.0238 X17 −0.0012 0.1975 X22 −5.75E−10 0.0974</sentence>
  </section>
  <section name="Concluding remarks">
    <sentence>We studied ridge regression as an alternative tool in real estate forecasting where one usually faces difficulty finding proper predictors.</sentence>
    <sentence>GA-Ridge is proposed here and its performance is examined against other forecasting methods.</sentence>
    <sentence>It is shown that GA is not only successful for real estate forecasting but also nicely settles critical issues in ridge regression.</sentence>
    <sentence>Experimental results are given for justification of GA-Ridge.</sentence>
    <sentence>It is noteworthy from the experimental results that GA-Ridge becomes a perfect solution particularly when a desirable predictor is hard to quantify but might be decomposed into various other predictors having less influence on response.</sentence>
  </section>
</article>
