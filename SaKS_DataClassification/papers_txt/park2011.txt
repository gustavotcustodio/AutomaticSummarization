Spline regression based feature extraction for semiconductor process fault detection using support vector machine

Abstract

Quality control is attracting more attention in semiconductor market due to harsh competition. This paper considers Fault Detection (FD), a well-known philosophy in quality control. Conventional methods, such as non-stationary SPC chart, PCA, PLS, and Hotelling’s T2, are widely used to detect faults. However, even for identical processes, the process time differs. Missing data may hinder fault detection. Artificial intelligence (AI) techniques are used to deal with these problems. In this paper, a new fault detection method using spline regression and Support Vector Machine (SVM) is proposed. For a given process signal, spline regression is applied regarding step changing points as knot points. The coefficients multiplied to the basis of the spline function are considered as the features for the signal. SVM uses those extracted features as input variables to construct the classifier for fault detection. Numerical experiments are conducted in the case of artificial data that replicates semiconductor manufacturing signals to evaluate the performance of the proposed method.

Keywords
Fault detection
Feature extraction
Spline regression
Support vector machine
Semiconductor manufacturing

1. Introduction

The competition in semiconductor market is getting keener and harsher. Semiconductor manufacturers are eager to take the lion’s share of this market by mass-manufacturing chips. Strategically, leasing other manufacturer’s production lines can increase output. Simultaneously, the quality control for semiconductor fabrication, which raises the yield, attracts more attention.

Before the Industrial Revolution, quality was mainly dependent on the skillfulness of individual workers. James Watt’s steam engine changed the main quality factor from the worker to the manufacturing equipment (Summers, 1997). Consequently, various methods have been developed to control the status of manufacturing equipment. Statistical Process Control (SPC) chart, proposed by Walter A. Shewhart, is the most widely used and well developed method of these (Montgomery, 2005).

Currently, however, owing to the rapid growth of information and manufacturing technology, it is possible to use real-time data to measure and control the status of equipment. The concept of quality control, using the stationary variables, is changed to dynamic control varying the abundant variables that reflect the signal characteristics under a closed loop. Therefore, the process controller, designed to maintain a satisfactory operational environment immune to the effects of disturbances and variations in the process, is essential to achieve desirable performance (Chiang, Russell, & Braatz, 2001).

However, the process controller is unable to control some variations. Those variations are regarded as faults, uncontrollable variations in parameters/variables that reflect the status of processes (Chiang et al., 2001). The most popular method to find faults is to setup control limit for acceptable variation, acquired by statistical computation. These procedures are called fault detection. Fault detection determines the faults present in a system and the time of detection (Isermann & Balle, 1997).

A sampled point over a given time horizon is regarded as a variable to build a fault detection model for non-stationary signals; that is, for signals that vary over time. The non-stationary SPC chart, which sets up the upper control limit (UCL) and lower control limit (LCL) using the calculated mean and standard deviation of each time-point variable, is a very popular method to detect faults (Park, Kim, & Baek, 2009; Park & Kim, 2008). The EWMA chart (Roberts, 1959) and CUSUM chart (Page, 1954) have been applied to analyze the significant changes in view of time series to enhance the performance of fault detection in the case of small changes and autocorrelation (Chiang et al., 2001; Montgomery, 2005).

Since a sampled point of a signal is regarded as a variable, the number of control charts to be handled increases with the signal sampling frequency. Abundant control charts hinder the effective performance of the fault detection system. Furthermore, such control charts cannot illustrate the interrelated-effects between the variables (Park et al., 2009). Thus, multivariate statistical analysis methods, such as Hotelling’s T2 Chart, principal component analysis (PCA), and partial least squares (PLS) (Shi & Jin, 2000), are considered. A fault detection method using wavelets was researched to enhance the ability to analyze effects caused by time (Lada, Lu, & Wilson, 2002; Park, Kim, Park, & Kim, 2008).

However, the previously mentioned methods have difficulties in dealing with the real data. A cyclic signal can be defined as a combination of steps that have similar aspects. Even if the signals are gathered from identical processes, the physical time lengths differ, as shown in Fig. 1. Moreover, serious differences between the signals exist compared to other points of time. Accordingly, standard deviation is high during these intervals; this frequently causes type 2 errors. In addition, sometimes sampling cannot be conducted, so that the missing data problem exists in real world data. Hence, fault detection methods need a preprocessing algorithm to deal with real world data.
Heterogeneous step lengths

    

Fig. 1. Heterogeneous step lengths.

The preprocessing method has some drawbacks; it magnifies the unexpected noise and loses the trait of the original signal. Fig. 2(a) and (b) shows some factors that affect the gathered signal. In Fig. 2(a), f(x) denotes the designated signal pattern by a process engineer. Contrary to the engineer’s expectation, it is certain that white noises ε are always added to the designed pattern f(x). In an actual manufacturing process system, in accordance with a recipe originated by the process engineer, the signal behaves following the pattern of f(x) combined with ε.
The nature of signals (X-axis represents time, Y-axis represents sampled value)

    

Fig. 2. The nature of signals (X-axis represents time, Y-axis represents sampled value).

However, what happened in the continuous fields cannot be known. Data has the form of discrete numerals sampled at specified time intervals. Furthermore, measurement errors that occur in sensor devices, et, are also added. A signal sampled at time t, st, can be expressed as the summation form of these factors st = f(x)t + εt + et. Thus, the ultimate objective of fault detection is to examine the significant changes in continuous fields, rather than that of gathered signals (Park et al., 2009).

Accordingly, various artificial intelligence (AI) methods are widely used to detect faults (Gertler, 1998). Several barriers prevent the capture of the true signal pattern, f(x), causing these misjudgments. AI methods are more robust to these hindrances than conventional statistical methods. They easily converge to the true division line between what is normal and a fault; this leads to lower misdetection rates.

Fault detection techniques that apply AI algorithms are used in industry. Gana, Zhao, and Chow (2009) use clone selection programming to detect faults of induction machines. In addition, the application of SVM, multi-class SVM is used in fault diagnosis (Sugumaran, Sabareesha, & Ramachandrana, 2008). Samanta (2004) uses a genetic algorithm in fault detection for gear, using ANN and SVM.

In this research, a feature extraction method using spline regression is proposed that prevents losing signal characteristics and magnifying undesirable noise. Spline regression is frequently used for image processing and signal processing (Li & Ji, 2009; Unser, 1999; Viola & Walker, 2005). Following Park and Kim (2008), features extracted from spline regression have problems in scale. In addition, heteroscedasticity, which exists in each step, hinders accurate fault detection (Park et al., 2009). Features are divided by the standard deviation to resolve the problem in scale and estimated using generalized least squares (GLS). This stabilizes features to obtain a more appropriate result. After the feature extraction method is presented, the fault detection algorithm is explained using the support vector machine classifier based on the features extracted by the proposed method (Park & Kim, 2008). A benchmark test is given to analyze its performance compared to conventional methods.
2. Feature extraction via spline regression

The spline function is composed of piecewise polynomials defined on a partitioned domain. In this paper, only the univariate polynomial spline function is considered. A univariate spline function S(·), defined on [a, b], maps them to
, the set of real numbers. S(·) consists of real-valued piecewise continuous functions Pi(·) (i = 0, … , k − 2) defined on a subinterval [ti, ti+1]. That is,
(1)
such that a = t0 < t1 < ⋯ < tk−2 < tk−1 = b and
(2)

The given k points, that divide the domain of spline function, ti are called knots (Spline (Mathematics), 2009). Spline functions can be classified by how they are defined, into cubic spline, natural spline, smoothing spline and so on.

The spline function can be expressed as a new form, a composition of the basis using linear basis expansion techniques. A function hm(x), the so-called transformation function, defined on the p dimensional vector space
, has a function value on for a given p dimensional vector x. With this linear expansion technique, any function f(x) is represented as a linear combination of a transformation function, as shown in (3).
(3)

If every p dimensional vector x can be transformed through the basis function, then the linear expansion method can be easily adopted. So that, the parameters βm (m = 1, … , M), where M denotes the number of basis functions, which shape the function, are estimated. Following the type of spline, basis functions are generated and estimators for parameters βm are computed with linear techniques, such as the least square method. After completing those steps, the spline function S(·) is acquired (Hastie, Tibshirani, and Friedman, 2001). From now on, x is regarded as a univariate variable that denotes the sampling time of a signal; only the cubic spline is considered.
2.1. Linear basis expansion

Basis functions for two knotted cubic spline functions can be expressed as (4), where knot points are ξ1 and ξ2:
(4)

where x+ is a function that maps to x when x is greater than 0; 0 otherwise. This definition implies that basis functions h5 and h6 are significant only when x is greater than ξ1 and ξ2, respectively. These characteristics differ from h1, … , h4. Since basis functions h1, … , h4 are defined on the entire domain region, the parameters β1, … , β4, multiplied to h1, … , h4, are regarded as features that reflect the entire trends of a signal. In contrast, β5 and β6 multiplied to h5 and h6, which have meaningful information only on fixed region, can be considered as features of specified regions. In addition, the cubic spline function is second-order differentiable, β5 and β6 can be interpreted as evaluations of variation between the subintervals.

With these characteristics of spline basis, feature extraction for cyclic signals including step-division information is able to be performed efficiently. Cyclic signals st(t = 1, … , T) are the set of sampled signal at a point of time t. By setting step changing points as knots, this linear basis expansion transforms a scalar input value to a multi-dimensional vector. For a sampled cyclic signal set, the data structure is slightly changed by linear basis expansion.
(5)

The matrix form for a signal can be represented as
(6)
(7)

For a given signal, matrix X can be calculated with linear basis expansion. The coefficients multiplied to each basis, β, should be estimated by the least squares technique.
2.2. Generalized least squared estimator

The spline model is presented with regard to Eq. (7). However, according to Park et al. (2009), when the homoscedasticity condition does not hold, estimators for β obtained by ordinary least squares (OLS) fluctuate over time. That is, since the OLS assumes that the error term ε follows independent identical distribution, for the heteroscedastic case, a more generalized estimation procedure is needed. The Generalized Least Squared estimator (GLS) is a solution to solve this problem (Hayashi, 2000; Verbeek, 2008).

In OLS, the estimator of β,
is represented as
(8)

However, the GLS estimator is expressed as (9), where var(ε) = σ2Ω.
(9)

Though GLS has more advantages than OLS, the covariance matrix Ω for ε is unknown in most cases. Thus, estimation for Ω should be performed to obtain the GLS estimator. Based on the objective function, maximizing the likelihood or minimizing the sum of squared error, methods to estimate both β and Ω are different (Verbeek, 2008).

β, obtained by GLS, becomes the features of a cyclic signal. With this process, spline feature extraction projects cyclic signals, of which lengths are different, into the same dimensional space. Various statistical methods can be easily applied, since the input variables are fixed to the same size. Since β1, … , βm are features that reflect the characteristics of a specified subinterval, it is possible to trace where faults occur. As stated in Park and Kim (2008), however, coefficients β are not in the same scale. Even though the features reflect the characteristic of each subinterval well, since the variability of each feature is quietly different, scale differences cause lower performance. Therefore, these features need to be standardized by dividing the scale by its variability. In this research, a standardization method, dividing by its standard variance, is provided. Contrary to previous research (Park et al., 2009), the mean squared error of the signal is no longer needed as a feature of the signal, since the GLS estimator can reflect its variance. Hence, a 5-stepped signal has eight features.
3. Fault detection using support vector machine

The Support Vector Machine (SVM) is the most popular classification method. It is widely used in various fields of classification, such as audio signal (Dhanalakshmi, Palanivela, and Ramalingama, 2009; Guo and Li, 2003), image (Chapelle, Haffner, and Vapnik, 1999; Tsai, 2007), and financial data (Bellotti and Crook, 2009; Chen, Ma, and Ma, 2009; Huang, Yang, and Chuang, 2008). SVM classifies data into two classes, whilst maximizing geometric margin, the nearest distance from the classifier. Since SVM belongs to supervised learning, each datum must be labeled as a class before it is classified (Vapnik, 1995; Vapnik, 1998). The training data set for SVM can be represented as (10); it consists of class label and input variables.
(10)

Training data set D is composed of n elements, pairs of xi (i = 1, 2, … , n) and ci(i = 1, 2, … , n). xi is a p-dimensional vector called input variables and ci is a class label that corresponds to xi. Supervised learning can be applied due to the class label ci. The SVM classifier can be expressed as a form of hyperplane w · x − b = 0. Geometric margin 2/∥w∥, represented in Fig. 3(a), should be maximized for accurate fault detection. Without loss of generality, the objective function that maximizes 2/∥w∥ can be reformulated to minimize ∥w∥2.
Concept of SVM (Support Vector Machine)

    

Fig. 3. Concept of SVM (Support Vector Machine).

Contrary to the separate case represented in Fig. 3(a), any hyperplane that is able to separate two classes does not exist for some data, as shown in Fig. 3(b); this is termed an inseparable case. Slack variable ςi (i = 1, 2, … , n), which represents the degree of misclassification, is adopted for all the training data set to deal with the inseparable case. The multiplication of penalty cost C and the summation of all ςi are added to the objective function that minimizes the inverse of the squared margin. With this, the optimization problem (11), which finds the classifier, can be formulated:
(11)
with its dual program as (12),
(12)

Consequently, the SVM classifier is obtained as a form of (13), where sgn(x) is a function that returns +1 when x is greater than 0; −1 otherwise.
(13)

The SVM classifier is linear. However, it can be transformed into nonlinear using the kernel trick. The SVM classifier can be altered in numerous different shapes; only the positive definiteness of the kernel function should be satisfied. For any positive definite kernel K(x, y), the dual problem and its classifier can be formulated as (14) and (15):
(14)
(15)

More accurate and asymptotic classification can be performed using these various kernel functions and adjusting its shape parameters.

In this paper, following previous research of Park et al. (2009), features extracted from spline regression become the input variable for SVM fault detection. If data known as faults or norms are given, the training set can be formed; the SVM classifier is constructed.
4. Experiments

Replicas of semiconductor FAB data are used, rather than the real FAB data due to the security policy of the semiconductor manufacturer, to evaluate the performance of the proposed method. Three kinds of signals are considered; two resemble temperature/condense parameters, one is similar to the voltage/reference parameter.
4.1. Data description

The three types of signals that resemble the actual signal in the fabrication process are generated. First, signal case I, shown in Fig. 4(a), is similar to the voltage or reference parameter. The actual model for signal case I is data from the optoelectronic sensor. Those signals behave like a step function, since it can vary immediately over time, unlike temperature. It varies from 50 to 200. Second, signal case II replicates a temperature signal. It varies smoothly; this type of signal resembles the chiller temperature in the photolithography process. The signal takes its value from 100 to 1,000. The case III signal has similarities with the variation of temperature in the diffusion process. The signal changes its value from 150 to 550. It differs from the case II signal, though they both sample temperature. In the process that the case II signal gathered, the temperature is the dependent variable of the process. In the case of signal III, the temperature is the independent variable; that is, it is the input recipe of the process.
f(x) of generated signal

    

Fig. 4. f(x) of generated signal.

For all steps, the signal will follow the defined functions. All these kinds of defined functions have the same physical length to 500 and are composed of five steps, but each step length is different. The physical lengths of the generated signal can vary within ±2%. White noises, which follow normal distributions with mean 0 and standard deviation 3, are added to the entire periods of the process signal.

Table 1 defines three types of faults. A type ‘A’ fault is defined as occurrences of mean shift in the fourth step. If a signal is classified as a type ‘B’ fault, that signal has high variance white noise in step 4. The amounts of mean shift and the magnitude of noises vary for signal types. Finally, a signal is regarded as a type ‘C’ fault when the signal starts five points later than the normal signal.

Table 1. Types of fault.
Fault type	Description
Type ‘A’	Mean shift in a step
Type ‘B’	High variation in a step
Type ‘C’	Time delay (start 5 points later)
4.2. Experimental design

The sets of randomly generated signals are formed to evaluate the performance of previously mentioned methods and the proposed method. Three sets are generated for each signal type. Half of first set includes the normal signal and the residual includes fault signals that follow type ‘A’ faults. Similar to set 1, sets 2 and 3 include type ‘B’ and ‘C’ faults in their second half, respectively. Each set consists of 100 signals.

Performance tests are conducted using generated signal sets. 5-folds cross validation tests are applied. Seven methods are considered to compare the performance of fault detection algorithms, of which the non-stationary chart is one. In general, 3-sigma control limits are set up. However, in this research, the tolerance limit for the faulty point and a wide control limit are considered for the sake of coordinating tradeoff between type 1 and type 2 errors. The 3-sigma control limit with two tolerance points and 4-sigma control limit with one tolerance point SPC charts are considered as fault detection methods. Hotelling’s T2 method and SVM are used to extract features by the cubic spline method. The control probability for Hotelling’s T2, which defines the control limit of the chart, is set as 0.75, 0.9 and 0.95 to modify its ability to detect faults. In the case of the support vector machine, the Gaussian kernel and inverse multi-quadratic kernel are considered as the control part of this research.
4.3. Experimental result

In Tables 2–4 below, α denotes the probability that type 1 error occurs; this regards a normal signal as a fault. β denotes the probability of omitting a type 2 error; this mistakes a fault as a normal signal. As mentioned above, fault detection is crucial for semiconductor yield. When faults are unrecognized by the process engineer, manufacturing devices waste their capacities by fabricating faulty wafers. Consequently, the fault detection algorithm should be sensitive to detect small process changes. That is, even though α is relatively high, lower β is more desirable in semiconductor manufacturing. For this reason, the parameters of the proposed method are selected to diminish β.

Table 2. Fault detection for signal case I.
	Type ‘A’	Type ‘B’	Type ‘C’
	Shift +1% in step 4	1.33 times variance in step 4	5 signal delayed
	α	β	α	β	α	β
SPC chart sigma = 3, tolerance = 2	.330	.645	.330	.615	.330	.060
SPC chart sigma = 4, tolerance = 1	.310	.710	.310	.695	.310	.045
T2 chart probability = 0.75	.185	.765	.185	.815	.185	.125
T2 chart probability = 0.9	.115	.885	.115	.865	.115	.265
T2 chart probability = 0.95	.090	.925	.090	.905	.090	.380
SVM with Gaussian radial basis	.385	.565	.480	.310	.075	.050
SVM with inverse multi quadratic	.400	.565	.480	.310	.045	.040

Table 3. Fault detection for signal case II.
	Type ‘A’	Type ‘B’	Type ‘C’
	Shift +2% in step 4	1.67 times variance in step 4	5 signal delayed
	α	β	α	β	α	β
SPC chart sigma = 3, tolerance = 2	.005	.595	.000	.975	.005	.920
SPC chart sigma = 4, tolerance = 1	.005	.715	.000	1.000	.005	.970
T2 chart probability = 0.75	.365	.005	.385	.615	.350	.240
T2 chart probability = 0.9	.135	.080	.275	.730	.165	.520
T2 chart probability = 0.95	.045	.250	.195	.800	.085	.745
SVM with Gaussian radial basis	.245	.315	.485	.545	.085	.035
SVM with inverse multi quadratic	.240	.335	.470	.550	.080	.020

Table 4. Fault detection for signal case III.
	Type ‘A’	Type ‘B’	Type ‘C’
	Shift +.25% in step 4	1.33 times variance in step 4	5 signal delayed
	α	β	α	β	α	β
SPC chart sigma = 3, tolerance = 2	.045	.980	.030	.980	.030	.775
SPC chart sigma = 4, tolerance = 1	.075	.980	.020	.985	.020	.890
T2 chart probability = 0.75	.230	.770	.330	.675	.330	.090
T2 chart probability = 0.9	.095	.905	.190	.815	.190	.225
T2 chart probability = 0.95	.040	.930	.145	.840	.145	.330
SVM with Gaussian radial basis	.400	.575	.510	.555	.120	.110
SVM with inverse multi quadratic	.425	.535	.585	.520	.020	.000

In the case of signal I, unlike cases II and III signals, the shape of the signal that follows the step function has difficulties in representing the combination form of the cubic spline function. Despite this, the proposed method outperforms the former methods in detecting faults, i.e. the proposed methods acquire the desirable β value compared to all other methods. In the case of fault type ‘C’, where the signal starts five points later than normal, the proposed methods dominate with regard to both types of error, α and β. The features extracted using cubic spline regression are expected to well reflect the physical length of signals; the fault detection method using those features, Hotelling’s T2 method and SVM perform, better than do the others.

Concerning the case II signal, which is composed of sinuous curve, even though conventional SPC chart failed to detect the fault, feature based methods get relatively lower β. For detecting fault type ‘C’, proposed method outperforms all other methods. Non-stationary SPC chart is not suitable to detecting fault. However, proposed method and Hotelling’s T2 method are more sensitive than SPC chart, so that they detect faults well, though generated false alarm occasionally.

Finally, in the case of signal III, like other signal cases, the proposed method performs highly on the set composed of fault type ‘C’ signals. The result of this signal type is worse than that for case II due to the lower fit of the cubic spline than for the case III signal. However, the proposed method is more sensitive to fault detection compared to other methods.

Some tendencies of the proposed method are observed from these significant results. As mentioned above, the fault detection performance owes to the fitness of the spline function. The cubic spline function reflects the shape of the signal for case II, the fault detection performance for the case II signal is relatively higher. In addition, for all types of signals, the proposed method and Hotelling’s T2 method, using extracted features, work well in detecting type ‘C’ faults. Corresponding to these results, the extracted feature via cubic spline function well reflects the signal delay. However, the extracted feature based methods are not good at detecting faults with high variance. Last, the trade-off relationship between sensitivity and robustness is observed. Over the entire result, feature based methods, Hotelling’s T2 and SVM, behave in the opposite direction. When β from the SVM method is low, β from the Hotelling’s T2 is relatively high. Applying those two methods at the same time in a harmonious way could be the key to enhancing fault detection.
5. Conclusions

In this paper, cubic spline regression and support vector machine are used for feature extraction and fault detection for semiconductor manufacturing processes, since the deal with real world data problems, such as missing data and heterogeneous sampling intervals and physical lengths. A given signal, composed of several steps, is regressed on a cubic spline function, whilst the step changing points are regarded as knots of the spline function. Features are extracted from the cubic regression function as a form of coefficients multiplied to the basis. Using these extracted features, the SVM classifier is constructed to detect faults from normal signals. The proposed method outperforms earlier methods, although there is further room for improvements.

The natural spline should be considered as alternative for the cubic spline function, since the fitness of the regression function affects fault detection; it is able to adapt its shape more similarly to the original signal. In addition, those extracted features are unable to reflect the effects of variance, some statistics of the regression result, such as the adjusted squared residual should be considered as a feature.

Fault classification must be considered for more practical use in semiconductor manufacturing. Fault detection is insufficient for the process engineer. A given signal is regarded as a fault; the engineer is unable to fix the fault, because she/he does not know where source of the fault. Therefore, the process control system should classify the kinds of faults and diagnose what causes the fault. Features extracted using the proposed methods can be used to classify and diagnose the faults. Multi-class SVM proposed by Sugumaran et al. (2008) is a prominent method for classification and diagnosis.