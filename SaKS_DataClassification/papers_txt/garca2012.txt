On the use of data filtering techniques for credit risk prediction with instance-based models

Abstract

Many techniques have been proposed for credit risk prediction, from statistical models to artificial intelligence methods. However, very few research efforts have been devoted to deal with the presence of noise and outliers in the training set, which may strongly affect the performance of the prediction model. Accordingly, the aim of the present paper is to systematically investigate whether the application of filtering algorithms leads to an increase in accuracy of instance-based classifiers in the context of credit risk assessment. The experimental results with 20 different algorithms and 8 credit databases show that the filtered sets perform significantly better than the non-preprocessed training sets when using the nearest neighbour decision rule. The experiments also allow to identify which techniques are most robust and accurate when confronted with noisy credit data.

Keywords
Finance
Credit risk
Instance selection
Outlier
Filtering
Editing
Nearest neighbour rule

1. Introduction

Modern-day finance is a broad field of business activity that comprises hard decision-making problems related to risk management. Financial risk refers to the uncertainties associated with any form of financing, including credit risk, business risk, investment risk, market risk, and operational risk (Horcher, 2005). The focus of this paper is on credit risk, which denotes the probability that a loan to a borrower will not be repaid. In this context, the credit risk prediction models intend to estimate whether a new credit applicant will default (bad applicant) on a loan or not (good applicant). The development of reliable prediction models is of major importance because the use of inadequate credit risk assessment tools constitutes one of the main reasons of enterprise bankruptcy.

The development of financial prediction methods is a complex process, which involves data collection and preprocessing, model design, validation and implementation. With regard to model design, numerous strategies have been proposed, including statistical methods, computational intelligence techniques and operations research methodologies (Abdou & Pointon, 2011; Ince & Aktan, 2009; Khandani, Kim, & Lo, 2010; Khashman, 2010; Wozabal & Hochreiter, 2012). In all cases, however, the quality of the data represents a critical point to further obtain accurate predictions. This mainly depends on the adequacy of the data in terms of the number of examples, the relevance of the attributes (independent variables) used in the analysis and the presence of outliers in the data set, among other issues. As a result, data preprocessing becomes a crucial step in real-world classification and prediction problems, as is the case of credit risk assessment.

Whilst some characteristics of data have largely been studied in the credit management literature, others have received relatively little attention so far. For instance, a lot of research effort has been devoted to the problem of attribute relevance by using new and existing feature selection algorithms (Chen & Li, 2010; Liu & Schumann, 2005; Piramuthu, 1999; Shukai, Chaudhari, & Dash, 2010; Wang, Hedar, Wang, & Ma, 2012). In contrast, despite its apparent influence on the performance of the prediction models, very few studies have addressed the problem of credit data with noise and outliers. Kotsiantis, Kanellopoulos, and Tampakas (2006) presented a survey of data preprocessing techniques for financial prediction, including discretization, feature selection and instance selection. Tsai and Chou (2011) used a genetic algorithm to perform feature selection and data filtering for bankruptcy prediction. Tsai and Cheng (2012) explored the performance of artificial neural networks, decision trees, logistic regression and support vector machines after removing different amounts of outliers from credit data sets.

Outliers have traditionally been defined as observations that appear to be inconsistent with the rest of the data. Nowadays, this term is being employed to cover a broad range of circumstances, including noisy and atypical data, new unidentified objects that do not belong to any of the predefined classes, and also mislabelled instances. Since the presence of outliers adversely affect the performance of any classification or prediction model, it is necessary to filter the data in order for eliminating the instances that disturb the generalization process. Identifying outliers becomes especially important with instance-based learning methods, as for example is the case of the k nearest neighbours decision rule.

Although instance selection has proved to be effective in many data mining and knowledge discovery applications, it has not been fully explored in the domain of credit management. This paper faces the problem of outlier removal in the context of credit risk prediction with the k nearest neighbours model. To this end, several instance selection algorithms of different nature will be used to filter out outliers from the training set, thus trying to obtain a cleaner representation of the credit data. It has to be pointed out that the aim of this work is not to search for the most effective instance selection method, but to test whether the application of these preprocessing techniques produces some increase in the performance of credit risk prediction models.
2. Instance-based models

From a practical point of view, the credit risk prediction problem can be deemed as a binary classification problem where a new input sample (the credit applicant) must be categorized into one of the predefined classes based on a number of observed variables or attributes related to that sample. The input of the classifier consists of a variety of information that describes socio-demographic characteristics and economic conditions of the applicant, and then the classifier has to produce the output in terms of the applicant creditworthiness.

Formally, the credit risk prediction problem can be described as follows. Given a set of applicants T = {(x1, y1), (x2, y2), … , (xn, yn)}, where each sample xi is characterized by m attributes, xi1, xi2, … xim, and yi denotes the class (good/bad applicant), then the credit risk classification problem consists of constructing a model δ to predict the value y for a new input applicant x, that is, δ(x) = y.

A large number of prediction models compute a distance between the input applicant x and the stored samples in T (called training set) when generalizing. These classification models are referred to as instance-based learning algorithms, where one of the most straightforward examples is the k nearest neighbours (k-NN) decision rule (Dasarathy, 1991).

The k-NN prediction model is a standard non-parametric technique used for probability density function estimation and classification. In brief, this classifier consists of assigning a new input sample to the class most frequently represented among the k closest instances in the training set, according to a suitable dissimilarity measure (e.g., Euclidean distance, Manhattan distance). A particular case is when k = 1, in which an input sample is decided to belong to the class indicated by its closest neighbour.

The characteristics of the k-NN classifier need the entire training set stored in computer memory, what may cause large time and memory requirements. On the other hand, this technique is extremely sensitive to the presence of noisy, atypical and erroneously labeled examples in the training set. Nevertheless, it has been found to perform better than other more sophisticated methods when dealing with arbitrarily complex problems, and it is a fairly intuitive procedure that could be easily implemented and explained to analysts.
3. Instance selection

In machine learning and data mining, the problem of instance selection is primarily related to instance deletion as irrelevant and harmful instances are removed from the original training set while retaining only critical instances. In general, the aim of the instance selection techniques is to produce a representative and relevant subset of the training set with similar or even higher generalization performance of the prediction model. An important advantage of instance selection against instance generation or abstraction refers to the fact that it chooses relevant examples without generating new artificial data, which do not make sense in many real-world applications. For instance, the incorporation of artificially generated samples into a credit data set may distort some socio-economic conditions of the problem in hand.

Let T be a training set and let R ⊆ T be the reference set, that is, the subset of examples selected by some instance selection algorithm. Usually, the reference set will be much smaller than the original training set (∣R∣ ≪ ∣T∣). The class label y of a new input case x will be then estimated by a given prediction function δ using the reference set R instead of T. The general framework for the application of an instance selection algorithm is as follows: some instance selection procedure chooses an appropriate reference set R from the training set T, then a learning algorithm is applied to build the prediction model δ and finally, this is evaluated using an independent test set S.

Traditionally, instance selection methods have been divided into two groups: editing and condensing. The goal of editing (or filtering) algorithms is to remove noisy, atypical and mislabelled instances and clean the possible overlapping between regions from different classes, thereby producing smoother decision boundaries and improving generalization. On the other hand, condensing (or thinning) algorithms aims at discarding superfluous or redundant instances that will not significantly affect the performance of the prediction model. Apart from these two general families of techniques, we can find hybrid methods that search for a small subset of the training set by simultaneously removing both outliers and superfluous instances.

According to their location in the input space, samples can be categorized into two main types: border and internal samples. Border samples are close to the decision boundaries, while internal samples are within the region of a class. Under this taxonomy, another factor that distinguishes instance selection techniques is whether they pursue to retain border points or internal points (Wilson & Martinez, 2000). The filtering algorithms seek to remove border samples because the internal examples are considered as the most representative of each class. Conversely, the condensing algorithms discard internal points under the idea that they do not affect the decision boundaries as much as the border samples and therefore, they can be removed with relatively little effect on the prediction process.

Yet another distinction among the instance selection algorithms can be made in terms of the direction of search for a subset of instances (Garcı´a, Derrac, Cano, & Herrera, 2012; Wilson & Martinez, 2000). An incremental search begins with an empty subset R and adds the training instances in T that fulfill some criterion; in this case, the order of presentation of instances can result very important. The decremental search starts putting all training instances into the reference set R, and then searches for instances to be removed from R; the order of presentation is also important, but unlike the incremental process, all training examples are available for examination at any time. Another way to apply an instance selection process is in batch mode, which involves deciding if each instance in T meets the removal criterion before discarding any of them; then, all instances that satisfy such a criterion are removed from R at once. Finally, a mixed search begins with an initially preselected subset R, obtained either by random selection or by an incremental or decremental process, and then iteratively allows for additions or removals of instances that fulfill some criterion.

It has to be pointed out that only filtering techniques will be here taken into account because the focus of the present study is mainly on improving the performance of credit risk prediction models rather than on reducing the data set size. Some of the benefits of using filters are: (i) they make a prediction problem easier because of smoothing the decision boundaries, (ii) they can substantially increase the class separability of the data in the input space, (iii) they can improve the generalization performance of the learning algorithm, and (iv) they reduce the computational burden of the prediction model because as a by-product, the training set size is also reduced.

Fig. 1 illustrates the result of applying a filter to an exemplar data set. As can be observed, the overlapping between both classes is eliminated, the class separability is increased and the decision boundaries are smoother, thus the prediction problem using the filtered set (Fig. 1(b)) will result much easier than using the (unfiltered) original set (Fig. 1(a)).
An example of filtering a data set

    

Fig. 1. An example of filtering a data set.

From the vast amount of methods proposed in the literature, a total of 20 filtering algorithms available in the KEEL software tool (Alcalá-Fdez, Fernández, Luengo, Derrac, & Garcı´a, 2011) will be analysed in this paper. Table 1 summarizes the methods here used and indicates the direction of search for each algorithm: D (decremental), B (batch) or M (mixed). Note that none of these algorithms works in an incremental mode since this is basically used by the condensing approach.

Table 1. The filtering techniques used in the present work.
Algorithm	Acronym	Search	Reference	
All-KNN editing	AllKNN	B	(Tomek, 1976)	
Edited nearest neighbour	ENN	D	(Wilson, 1972)	
Estimating class probabilities and threshold	ENNTh	D	(Vázquez et al., 2005)	
Edited normalized radial basis function	ERBF	D	(Jankowski & Grochowski, 2004)	
Modified edited nearest neighbour	MENN	D	(Hattori & Takahashi, 2000)	
Model class selection	MCS	B	(Brodley, 1995)	
Multiedit	MEdit	D	(Devijver, 1986)	
Nearest centroid neighbours editing	NCN	D	(Sánchez et al., 2003)	
Patterns by ordered projections	POP	B	(Riquelme et al., 2003)	
Relative neighbourhood graph editing	RNG	D	(Sánchez et al., 1997)	
CHC evolutionary algorithm	CHC	M	(Cano et al., 2003)	
Generational genetic algorithm	GGA	M	(Cano et al., 2003)	
Population based incremental learning	PBIL	M	(Cano et al., 2003)	
Steady-state genetic algorithm	SGA	M	(Cano et al., 2003)	
Automatic noise reduction	ANR	B	(Zeng & Martinez, 2003)	
Classification filter	ClassF	D	(Gamberger et al., 1999)	
Cross-validated committee filter	CVC	B	(Verbaeten & Assche, 2003)	
Ensemble of filters	EnsF	B	(Brodley & Friedl, 1999)	
Iterative partitioning filter	IPF	B	(Khoshgoftaar & Rebours, 2007)	
Saturation filter	SatF	D	(Gamberger et al., 2000)	
4. Experimental set-up

The aim of the experiments is to evaluate the performance of an instance-based credit risk prediction model when training data are preprocessed to remove outliers and noisy examples. More specifically, we are interested in finding out whether the 1-NN decision rule applied on the filtered sets leads to better performance than that applied on the original training sets. After this, we also try to determine which filtering technique yields the best results.

All the experiments have been carried out on eight real-world credit data sets, whose main characteristics are reported in Table 2. The Australian, German and Japanese data sets are from the UCI Machine Learning Database Repository (http://archive.ics.uci.edu/ml/). The Iranian data set comes from a modification to a corporate client database of a small private bank in Iran (Sabzevari, Soleymani, & Noorbakhsh, 2007). The Polish data set contains bankruptcy information of 120 companies recorded over a two-year period (Pietruszkiewicz, 2008). The Thomas data set, which comes with the book by Thomas, Edelman, and Crook (2002), describes applicants for a credit product. The UCSD data sets correspond to two subsets with samples randomly chosen from the database used in the 2007 Data Mining Contest organized by the University of California San Diego and Fair Isaac Corporation.

Table 2. Some characteristics of the credit data sets.
Data set	#Attributes	#Good	#Bad
Australian	14	307	383
German	24	700	300
Iranian	27	950	50
Japanese	15	296	357
Polish	30	128	112
Thomas	12	902	323
UCSD2500	38	1250	1250
UCSD5000	38	2500	2500

The standard way to assess credit risk prediction systems is to use a holdout sample since large sets of past applicants are usually available. However, there are situations in which data are too limited to build an accurate model and therefore, other strategies have to be used in order to obtain a good estimate of the predictive performance. The most common way around this corresponds to cross-validation (Thomas et al., 2002).

Accordingly, a fivefold cross-validation has been adopted for the present experiments: each original data set has been randomly divided into five stratified parts of equal (or approximately equal) size. For each fold, four blocks have been pooled as the training data, and the remaining part has been employed as an independent test set. Ten repetitions have been run for each trial. The training sets have been preprocessed by the filtering algorithms reported in Table 1 in order to obtain the reference sets. The results from classifying the test samples with the 1-NN decision rule using the reference sets have been averaged across the 50 runs and then evaluated for significant differences between algorithms using some statistical tests.
4.1. Evaluation measures

Standard performance evaluation criteria in the fields of credit scoring include accuracy rate, Gini coefficient, K-S statistic, root mean squared error, area under the ROC curve, cumulative accuracy profile, true rate, and type-I and type-II errors (Hand, 2005; Hong, 2009; Yang, Wang, Bai, & Zhang, 2004). For a two-class problem, most of these metrics can be easily derived from a 2 × 2 confusion matrix as that given in Table 3, where each entry (i, j) contains the number of correct/incorrect predictions.

Table 3. Confusion matrix for a credit scoring problem.
	Predicted as good	Predicted as bad
Good applicant	a	b
Bad applicant	c	d

In credit risk prediction applications, the accuracy rate (also called score of hits) is very often utilized as the criterion for performance evaluation. It represents the proportion of the correctly classified cases (both good and bad applicants) on a particular data set, and can be formally defined as follows:
(1)

However, the accuracy ignores the different cost of both error types (bad applicants being predicted as good, or vice versa). This is the reason why it becomes especially important to take into consideration the error on each individual class:
(2)

Type-I error (or miss) is the rate of bad applicants being categorized as good. When this happens, the misclassified bad applicants will become default. Therefore, if the credit granting policy of a financial institution is too generous, this will be exposed to high credit risk. Type-II error (or false-alarm) defines the rate of good applicants being predicted as bad. When this happens, the misclassified good applicants are refused and therefore, the financial institution has opportunity cost caused by the loss of good customers. As stated by Caouette, Altman, Narayanan, and Nimmo (2008), the misclassification costs associated with type-I errors are typically much higher than those associated with type-II errors and therefore, they are also more critical for determining higher credit risk for banks and financial institutions.
4.2. Statistical significance tests

A common way to compare two or more classifiers over various data sets is the Student’s paired t-test, which checks whether the average difference in their performance over the data sets is significantly different from zero. However, this appears to be conceptually inappropriate and statistically unsafe because parametric tests are based on a variety of assumptions (normality, large number of data sets, homogeneity of variance) that are often violated due to the nature of the problems (Demšar, 2006). In general, the non-parametric tests (e.g., Wilcoxon and Friedman tests) should be preferred over the parametric ones, especially in multi-problem analysis, because they do not assume normal distributions or homogeneity of variance (Demšar, 2006; Garcı´a, Fernández, Luengo, & Herrera, 2010).

The Wilcoxon signed-ranks test ranks the differences in performances of two algorithms for each data set, ignoring the signs, and compares the ranks for the positive and the negative differences. Let di be the difference between the performance scores of the two algorithms on ith out of N data sets. The differences are ranked according to their absolute values; in case of ties, average ranks are assigned (for example, if two differences are tied when assigning ranks 2 and 3, we can assign 2.5 to both differences). Let R+ be the sum of ranks for the data sets on which the first algorithm outperforms the second, and R− the sum of ranks for the opposite. Ranks of di = 0 are split evenly among the sums; if there is an odd number of them, one is ignored:
(3)

Let Z be the smaller of the sums, Z = min (R+,R−). If Z is less than or equal to the value of the distribution of Wilcoxon for N degrees of freedom, the null-hypothesis that both algorithms perform equally well can be rejected. However, in the case of multiple hypothesis testing, there exist other statistical methods more powerful than the Wilcoxon signed-ranks test (Demšar, 2006). Two of the most typical procedures for testing the differences between multiple prediction models are ANOVA and the non-parametric Friedman test.

The Friedman test is based on the average ranked performances of a collection of techniques on each data set separately. Under the null-hypothesis being tested, which states that all the algorithms are equivalent and the observed differences are merely random, the Friedman statistic can be computed as follows:
(4)
where N denotes the number of data sets, K is the total number of algorithms and Rj is the average ranks of algorithms. The is distributed according to the Chi-square distribution with (K − 1) degrees of freedom, when N (number of data sets) and K (number of algorithms) are big enough. However, it has been demonstrated that the Friedman statistic produces an undesirably conservative effect. In order to overcome the conservativeness, Iman and Davenport (Iman & Davenport, 1980) proposed a better statistic distributed according to the F-distribution with (K − 1) and (K − 1)(N − 1) degrees of freedom:
(5)
If the null-hypothesis of the Friedman test is rejected, we can then proceed with a post hoc test in order to find the particular pairwise comparisons that produce significant differences. A post hoc test compares a control algorithm opposite to the remainder techniques, making possible to define a collection of hypothesis around the control method. The Nemenyi post hoc test, which is analogous to the Tukey test for ANOVA, states that the performances of two or more algorithms are significantly different if their average ranks are at least as great as their critical difference (CD) with a given level of significance (Demšar, 2006):
(6)
where qα,∞,K is a critical value based on the studentised range statistic divided by

.
5. Results

In this section, the experimental results are discussed. First, we compare the performance of the 1-NN decision rule using filtered sets with that obtained when no filtering has been applied. Next, we try to determine which method performs the best and evaluate for statistically significant differences among the different filtering algorithms presented in Table 1.
5.1. Do the filtered sets perform better than the original training sets?

Table 4 shows the accuracy rates achieved with the selected reference sets. For each database, the best result is highlighted in boldface. As can be seen, the credit risk prediction with the non-preprocessed sets (the original training sets) have consistently performed worse than the classification models using the filtered sets. In most cases, differences between the original set and the edited sets are high and relevant, what is of great value in financial applications because it may lead to significant cost savings for the credit institution. For example, the increase in accuracy is of 5.80% for the Australian database, 7.30% for the German database, 2.90% for the Iranian database, and 6.90% for the Japanese database.

Table 4. Accuracy rates of the filtering algorithms.
	Australian	German	Iranian	Japanese	Polish	Thomas	UCSD2500	UCSD5000
Original	0.804	0.657	0.921	0.795	0.763	0.665	0.754	0.783
AllKNN	0.855	0.713	0.949	0.858	0.729	0.726	0.760	0.780
ENN	0.849	0.706	0.948	0.862	0.788	0.720	0.757	0.783
ENNTh	0.862	0.720	0.950	0.854	0.700	0.736	0.754	0.784
ERBF	0.859	0.702	0.950	0.859	0.675	0.736	0.710	0.718
MENN	0.861	0.716	0.950	0.854	0.700	0.740	0.753	0.782
MCS	0.841	0.669	0.940	0.825	0.783	0.680	0.764	0.781
MEdit	0.851	0.708	0.947	0.858	0.758	0.728	0.747	0.779
NCN	0.855	0.703	0.944	0.864	0.742	0.722	0.773	0.791
POP	0.807	0.608	0.918	0.787	0.763	0.659	0.754	0.783
RNG	0.855	0.723	0.948	0.864	0.738	0.729	0.776	0.795
CHC	0.854	0.716	0.950	0.858	0.708	0.739	0.774	0.779
GGA	0.846	0.709	0.950	0.851	0.696	0.731	0.748	0.768
PBIL	0.838	0.708	0.950	0.842	0.721	0.714	0.751	0.773
SGA	0.846	0.706	0.950	0.845	0.667	0.724	0.760	0.781
ANR	0.851	0.730	0.950	0.861	0.696	0.711	0.762	0.765
ClassF	0.843	0.699	0.942	0.842	0.763	0.716	0.768	0.786
CVC	0.835	0.680	0.941	0.827	0.763	0.728	0.758	0.789
EnsF	0.855	0.692	0.942	0.845	0.771	0.721	0.775	0.801
IPF	0.830	0.682	0.945	0.827	0.758	0.733	0.757	0.789
SatF	0.812	0.659	0.945	0.813	0.733	0.696	0.756	0.784

Table 5 reports the set size reduction rates for each filtering method on each database. Although this is not the main objective of the present study, it is still interesting to analyse the size reduction produced by each algorithm because it is usual to find real databases with millions of records in banking and financial applications. Thus one can observe that the evolutionary-based filtering methods (CHC, GGA, PBIL and SGA) have achieved more than 90% of set size reduction rate, what means significant saving in time computing and storage requirements when compared to the unfiltered data set. Conversely, the ensemble-based filters, such as CVC, EnsF and IPF, have contributed to low size reduction rates with less than 10% in most cases: also the POP algorithm, which is a filter based on feature projection, has obtained reduction rates similar to those of the ensemble-based techniques.

Table 5. Set size reduction rates of the filtering algorithms.
	Australian	German	Iranian	Japanese	Polish	Thomas	UCSD2500	UCSD5000
Original	–	–	–	–	–	–	–	–
AllKNN	26.27	42.58	9.72	24.94	40.21	43.00	35.37	32.28
ENN	16.12	29.38	6.50	15.90	26.25	29.59	22.92	21.37
ENNTh	34.78	59.28	15.48	34.41	56.88	63.16	48.82	45.20
ERBF	14.02	29.25	5.00	13.49	37.40	26.37	47.70	29.10
MENN	35.00	61.53	15.60	34.87	57.40	63.92	50.13	46.26
MCS	10.87	17.13	3.78	10.65	11.77	17.96	12.29	10.26
MEdit	20.47	40.15	7.53	20.69	37.60	42.35	32.20	30.21
NCN	18.70	29.28	6.22	18.05	22.08	32.00	22.36	19.80
POP	5.65	54.28	29.45	4.90	0.00	3.53	0.12	0.31
RNG	16.34	29.35	5.55	16.13	23.75	28.86	22.56	20.52
CHC	99.13	98.98	99.83	99.16	96.98	99.47	99.62	99.69
GGA	96.63	93.90	99.03	97.47	92.29	95.37	91.66	90.44
PBIL	97.46	94.65	99.63	97.32	92.92	96.14	91.61	88.97
SGA	96.67	94.00	99.65	97.28	91.04	97.41	94.22	93.31
ANR	11.56	24.10	4.90	12.41	30.83	17.08	27.15	32.48
ClassF	19.60	31.35	7.60	19.23	22.81	34.41	20.63	18.14
CVC	2.46	2.97	2.08	3.22	0.83	16.06	1.25	1.30
EnsF	6.92	11.45	3.60	8.16	11.88	16.47	7.74	6.28
IPF	2.97	4.47	2.20	3.56	1.77	19.29	1.48	1.47
SatF	11.74	1.68	4.38	14.37	14.48	10.02	0.75	0.38

To visualize the performance of a data filtering algorithm we can use a scatterplot of the size reduction rate versus the accuracy rate of 1-NN with the selected reference set. Since the accuracy rates are very different for the different data sets, using the average accuracy across the data sets will be inadequate. Instead we calculate ranks for the methods. For each data set, the best method receives rank 1, and the worst receives rank 21. As there are two criteria –accuracy rate and size reduction rate– each method will receive two ranks. Let ra(i, j) and rs(i, j) be the accuracy and the size reduction ranks for method i, respectively evaluated on data set j. Fig. 2 displays all the methods (including the unfiltered original set) in the space spanned by the average rank on size reduction rate
on the x-axis and the average rank on accuracy

on the y-axis. In such a way, points close to the origin (0, 0) of the plot correspond to the best methods in terms of a reasonably balanced trade-off between accuracy and size reduction.
Accuracy versus set size reduction plot for the filtering methods using average…

    

Fig. 2. Accuracy versus set size reduction plot for the filtering methods using average ranks.

One can observe in Fig. 2 that the 1-NN rule applied to the unfiltered (original) set lies the furthest from the origin of the plot, showing that it has achieved both the worst accuracy and the lowest reduction rate as computed in terms of average ranks. All methods below the point that represents the original set have obtained a better average rank on accuracy, whereas the methods left to this point have received a better average rank on size. The CHC algorithm appears to be the model with the most suitable trade-off between accuracy and size reduction. The technique that has yielded the highest accuracy corresponds to the RNG filter, but it is still far from the best methods in terms of reduction percentage. On the other hand, as already seen in Table 5, the evolutionary filters (CHC, GGA, PBIL and SGA) have obtained the highest set size reduction rates.

As a further confirmation of the findings with the accuracy rates, we have run a Wilcoxon signed-ranks test between each pair of techniques. The upper diagonal half of Table 6 summarizes this statistic for a significance level of 0.10 (10% or less chance), whereas the lower diagonal half corresponds to a significance level of 0.05. The symbol “•” indicates that the method in the row significantly improves the method of the column, and the symbol “∘” means that the method in the column performs significantly better than the method of the row. The two bottom rows show how many times the algorithm of the column has been significantly better than the remaining techniques for α = 0.10 and α = 0.05.

Table 6. Summary of the Wilcoxon statistic for the filtering methods.
	(1)	(2)	(3)	(4)	(5)	(6)	(7)	(8)	(9)	(10)	(11)	(12)	(13)	(14)	(15)	(16)	(17)	(18)	(19)	(20)	(21)
(1) Original	–	∘	∘				∘		∘		∘						∘	∘	∘	∘	
(2) AllKNN		–								•	∘		•	•	•						•
(3) ENN	•		–				•			•				•							•
(4) ENNTh				–	•								•		•						
(5) ERBF					–	∘															
(6) MENN						–							•		•						
(7) MCS	•		∘				–			•									∘		
(8) MEdit								–						•							•
(9) NCN	•								–	•				•							•
(10) POP			∘				∘		∘	–	∘						∘	∘	∘	∘	
(11) RNG	•	•								•	–		•	•	•	•					•
(12) CHC												–	•	•	•						•
(13) GGA				∘		∘					∘	∘	–								
(14) PBIL		∘	∘						∘		∘			–							
(15) SGA				∘		∘					∘	∘			–						
(16) ANR																–					
(17) ClassF	•									•							–		∘		•
(18) CVC	•									•								–	∘		•
(19) EnsF	•						•			•								•	–	•	•
(20) IPF	•									•										–	•
(21) SatF			∘						∘		∘						∘	∘	∘	∘	–

α = 0.10	0	6	5	3	0	3	2	2	4	0	8	4	0	0	0	0	3	3	7	3	0

α = 0.05	0	1	5	2	0	2	2	0	4	0	7	2	0	0	0	0	3	3	5	3	0

It is worth pointing out that the original training set has not been significantly better than any other algorithm. At a significance level of 0.10, 9 out of the 20 filters here tested have been significantly superior to the original set, while 8 techniques have performed significantly better than the unfiltered set for a significance level of 0.05. On the other hand, it appears that the best filtering methods correspond to RNG, EnsF, ENN and NCN, which have achieved accuracy rates significantly higher than the many other compared algorithms. The CHC evolutionary filter, which has produced the highest size reduction rates, is also among the best algorithms in terms of accuracy, performing significantly better than four other filtering methods.
5.2. Which filtering method performs the best?

Analysing the results of the Wilcoxon signed-ranks test reported in Table 6, it is possible to draw the following remarks:

•

    The RNG method, which corresponds to the best filter in terms of accuracy rates, is significantly superior to AllKNN, POP, GGA, PBIL, SGA and SatF at a significance level of 0.05.
•

    The CHC method appears to be the best performing evolutionary-based filtering technique, being significantly superior to GGA and SGA at a significance level of 0.05, and better than GGA, PBIL and SGA at a significance level of 0.10.
•

    Among the methods based on Wilson’s editing, it seems that ENN and NCN are the best performing algorithms. At significance level of 0.05, ENN is significantly better than MCS, POP, PBIL and SatF, whereas NCN is significantly better than POP, PBIL and SatF.
•

    The POP algorithm is the worst filter: 12 out of the 20 methods are significantly better than POP at a significance level of 0.05.

Next, an additional statistical test will be carried out in order to support our preliminary conclusions and produce evidence of the pros and cons related to the filters here compared. Thus, as a way to further corroborate that there exist statistically significant differences in the accuracy rates, we have also computed the Iman-Davenport statistic (Eq. 5), yielding the value FF = 2.055. The critical values for the F-distribution with (20 − 1 = 19) and (20 − 1)(8 − 1) = 133 degrees of freedom at confidence levels of 0.05 and 0.10 have been F(19,133)0.05 = 1.665 and F(19,133)0.10 = 1.487, respectively. Thus the null-hypothesis that all filters here explored perform equally well can be rejected and consequently, we can now apply a Nemenyi post hoc test in order to find out the set of techniques that are significantly worse than the control method (the algorithm with the best Friedman’s rank).

The results of the Nemenyi test are depicted by significance diagrams (Lessmann, Baesens, Mues, & Pietsch, 2008), plotting the Friedman average ranks and the critical difference tail (calculated using Eq. 6). The diagram in Fig. 3 plots filtering algorithms versus average rankings, whereby all methods are sorted according to their ranks. The line segment to the right of each algorithm represents the critical difference (CD = 10.483 for α = 0.05 and CD = 9.818 for α = 0.10). The vertical dotted line indicates the end of the best performing method. Therefore, all algorithms right to this line mean to perform significantly worse than the best filtering technique.
Nemenyi’s significance diagrams for the filtering algorithms

    

Fig. 3. Nemenyi’s significance diagrams for the filtering algorithms.

Fig. 3 shows that the only algorithms that have been significantly worse than the best performing method (RNG) correspond to POP for a level of significance of 0.05 and POP and SatF at a significance level of 0.10. This result is in agreement with the conclusions drawn from the Wilcoxon test in the sense that the POP algorithm is the worst filter for the credit databases used in these experiments. On the other hand, the fact that there are no other methods right to the vertical dotted line in Fig. 3 suggests that any of the remaining techniques could be appropriate for accurately filtering the training set. Therefore, when testing a number of filters for a real-world credit risk prediction problem, apart from the accuracy rate, one should also take care of other factors, such as the set size reduction rate or the type-I and type-II errors.

Here we will analyse the type-I and type-II errors because, in many banking and financial applications where the misclassification cost associated to each type of error may be different, it is especially important to evaluate the error on each class separately. As already stated in Section 4.1, the type-I errors are usually more critical for the financial institution since they typically lead to higher credit risks. Taking this feature into account, Tables 7 and 8 report the type-I and type-II errors derived from the filtering algorithms, respectively. For each database, the lowest error rates are highlighted in boldface.

Table 7. Type-I error rates of the filtering algorithms.
	Australian	German	Iranian	Japanese	Polish	Thomas	UCSD2500	UCSD5000
AllKNN	0.162	0.700	0.980	0.179	0.404	0.814	0.319	0.286
ENN	0.144	0.610	1.000	0.157	0.260	0.774	0.293	0.258
ENNTh	0.141	0.693	1.000	0.171	0.404	0.861	0.307	0.275
ERBF	0.120	0.960	1.000	0.157	0.580	1.000	0.445	0.441
MENN	0.146	0.730	1.000	0.174	0.404	0.858	0.315	0.281
MCS	0.154	0.590	0.840	0.174	0.232	0.725	0.258	0.233
MEdit	0.146	0.720	1.000	0.191	0.287	0.929	0.326	0.270
NCN	0.141	0.603	0.920	0.157	0.313	0.753	0.254	0.233
POP	0.180	0.393	0.720	0.193	0.214	0.635	0.249	0.212
RNG	0.154	0.680	1.000	0.157	0.366	0.817	0.275	0.254
CHC	0.144	0.550	1.000	0.149	0.329	0.864	0.220	0.221
GGA	0.146	0.580	1.000	0.168	0.287	0.848	0.267	0.251
PBIL	0.136	0.563	1.000	0.146	0.275	0.857	0.269	0.247
SGA	0.146	0.560	1.000	0.157	0.366	0.817	0.262	0.223
ANR	0.167	0.610	1.000	0.177	0.268	0.809	0.284	0.277
ClassF	0.133	0.617	0.860	0.132	0.241	0.746	0.259	0.238
CVC	0.154	0.573	0.780	0.132	0.214	0.799	0.248	0.213
EnsF	0.133	0.590	0.920	0.154	0.232	0.786	0.255	0.214
IPF	0.159	0.580	0.760	0.134	0.214	0.808	0.250	0.215
SatF	0.154	0.547	0.900	0.146	0.268	0.715	0.246	0.213

Table 8. Type-II error rates of the filtering algorithms.
	Australian	German	Iranian	Japanese	Polish	Thomas	UCSD2500	UCSD5000
AllKNN	0.124	0.110	0.002	0.098	0.157	0.081	0.160	0.153
ENN	0.160	0.159	0.002	0.115	0.173	0.103	0.193	0.176
ENNTh	0.134	0.103	0.000	0.115	0.210	0.051	0.184	0.157
ERBF	0.166	0.014	0.000	0.122	0.101	0.000	0.136	0.123
MENN	0.130	0.093	0.000	0.112	0.210	0.047	0.178	0.154
MCS	0.166	0.220	0.019	0.176	0.203	0.175	0.214	0.205
MEdit	0.153	0.109	0.003	0.084	0.204	0.037	0.180	0.171
NCN	0.150	0.166	0.011	0.111	0.211	0.109	0.201	0.185
POP	0.208	0.391	0.048	0.237	0.257	0.236	0.244	0.222
RNG	0.134	0.104	0.002	0.111	0.172	0.075	0.172	0.156
CHC	0.150	0.170	0.000	0.135	0.257	0.045	0.231	0.220
GGA	0.163	0.167	0.000	0.125	0.320	0.062	0.238	0.213
PBIL	0.196	0.176	0.000	0.172	0.281	0.081	0.229	0.207
SGA	0.163	0.180	0.000	0.152	0.305	0.082	0.218	0.215
ANR	0.127	0.124	0.000	0.095	0.335	0.103	0.191	0.192
ClassF	0.186	0.166	0.016	0.189	0.235	0.119	0.206	0.190
CVC	0.179	0.211	0.021	0.223	0.257	0.083	0.235	0.209
EnsF	0.160	0.187	0.013	0.155	0.226	0.098	0.195	0.183
IPF	0.182	0.206	0.018	0.220	0.265	0.073	0.235	0.208
SatF	0.231	0.253	0.011	0.237	0.265	0.156	0.242	0.220

The POP filtering method, which corresponds to the algorithm with the worst overall accuracy rates, has paradoxically achieved the lowest type-I error in 5 out of the 8 databases, but the type-II error rates associated to this filter are certainly very far from those of the rest of the methods. It is also interesting to note that a lot of techniques have yielded a type-I error rate of 100% and a type-II error rate of 0% on the Iranian database, what means that they have misclassified all the bad applicants and have correctly predicted the class of every good applicant in the test set. These unexpected results on the Iranian database are probably due to the combined effect of noise and high class imbalance (950 good applicants and 50 bad applicants), which refers to an extremely important situation that deserves more attention for further research.

Despite the asymmetric misclassification costs, it is commonly accepted that a reliable prediction model should procure a well balanced trade-off between type-I and type-II error rates. Correspondingly, in a similar way as previously done for the accuracy and size reduction rates, the performance of the filtering algorithms can be visualized by means of a scatterplot of the type-II error versus the type-I error of the 1-NN decision rule using the selected reference set. Here the two ranks, rTIE(i, j) and rTIIE(i, j), calculated for each method i evaluated on each data set j correspond to both types of errors. Fig. 4 displays all the filtering techniques in the space defined by
on the y-axis and

on the x-axis, that is, the average rank on type-I error and the average rank on type-II error respectively.
Type-II error versus type-I error plot for the filtering methods using average…

    

Fig. 4. Type-II error versus type-I error plot for the filtering methods using average ranks.

The plot in Fig. 4 reinforces the comments made from the results in Tables 7 and 8. As can be observed, the POP algorithm is among the best methods in terms of average rank on type-I error, but has achieved the highest average rank on type-II error. The opposite behaviour is shown by AllKNN, ERBF, MENN, MEdit and ENNTh, which have attained the highest average ranks on type-I error and the lowest average ranks on type-II error. The ENN, NCN and CHC methods appear to yield the most balanced trade-off between both ranks, suggesting that these might be the most suitable techniques to filter out noisy data in a credit risk prediction domain.
6. Conclusions

This paper has presented a comprehensive study of noise filtering techniques when used for credit risk assessment with an instance-based prediction model. In particular, the performance of 20 algorithms (plus no filtering) on 8 real-world credit databases has been compared by means of the accuracy, the type-I error, the type-II error and the set size reduction rate. Statistical significance differences among the methods have been evaluated with the Wilcoxon signed-ranks and Friedman statistics, as well as the Nemenyi post hoc test.

From the experiments carried out, some conclusions can be drawn. First, it has empirically been demonstrated that the use of filters leads to significant improvement in performance and impressive saving in storage resources when compared to the nearest neighbour prediction model with no filtering. Second, the RNG algorithm appears to be the best filtering method in terms of overall accuracy, but it is only significantly superior to POP and SatF when using the Nemenyi post hoc test. This suggests that most filters can become useful for cleaning a credit data set and therefore, other factors have also to be taken into account in order for choosing the most suitable filtering method.

When measuring the type-I and type-II errors, we have found that the ENN, NCN and CHC algorithms yield the most balanced trade-off between both rates. It has also been observed that the class imbalance problem plays a key role in the results of the different filtering methods. In this sense, for instance, many algorithms have achieved a type-I error of 100% and a type-II error of 0% on the strongly imbalanced Iranian database, what is certainly of no practical value. Although the type-I error is typically judged as the most critical for banks and financial institutions, a reliable credit risk prediction model should still obtain a relatively low type-II error rate in order not to lose potentially good customers. Therefore, the POP, IPF, CVC and SatF algorithms, despite achieving the lowest type-I error rates in average, seem not to be appropriate filtering methods because of their high type-II error rates.

Future work will extend this study by using filters in conjunction with condensing algorithms in order to check whether they can achieve even better results. Another avenue for further research refers to analyse the combined problem of noise and class imbalance in credit risk prediction. Also, this work could be extended by using other classification models rather than instance-based learners.