Swarm intelligence-based extremum seeking control

Abstract

This paper proposes an extremum seeking control (ESC) scheme based on particle swarm optimization (PSO). In the proposed scheme, the controller steers the system states to the optimal point based on the measurement, and the explicit form of the performance function is not needed. By measuring the performance function value online, a sequence, generated by PSO algorithm, guides the regulator that drives the state of system approaching to the set point that optimizes the performance. We also propose an algorithm that first reshuffles the sequence, and then inserts intermediate states into the sequence, in order to reduce the regulator gain and oscillation induced by population-based stochastic searching algorithms. The convergence of the scheme is guaranteed by the PSO algorithm and state regulation. Simulation examples demonstrate the effectiveness and robustness of the proposed scheme.

Keywords
Particle swarm optimization
Extremum seeking control
State regulation
Adaptive control
Swarm intelligence-based optimization

1. Introduction

Regulation and tracking of system states to optimal setpoints or trajectories are typical tasks in control engineering. However, these optimal setpoints are sometimes difficult to be chosen a priori, or vary with the environmental condition changes. Extremum seeking control (ESC) is a kind of adaptive control schemes that can search for the optimal setpoints online, based on the measurement of the performance output or its gradient. ESC can be regarded as an optimization problem, and many of the schemes used in ESC are transferred from optimization algorithms. However, some optimization algorithms cannot be incorporated into the ESC framework easily, for the reason that, practical issues, such as stability, noise, regulation time, control gain and oscillation limitation, will prevent the use of some optimization algorithms from ESC context. Thus, the study on suitable combination of ESC and optimization algorithms is of great interest both in academics and in engineering.

Unlike the traditional variational calculus-involved optimal control method, the explicit form of the performance function is not needed in ESC. Therefore, ESC is useful in the applications that the performance functions are difficult to model. After Krstic and Wang’s stability studies (Krstic & Wang, 2000), research on ESC has received significant attention in recent years. The recent ESC application examples include active flow control (Beaudoin, Cadot, Aider, & Wesfreid, 2006), bioreactor or chemical process control (Bastin, Nescaroni, Tan, & Mareels, 2009; Hudon, Guay, Perrier, & Dochain, 2008; Hudon, Perrier, Guay, & Dochain, 2005), cascaded Raman optical amplifiers (Dower, Farrell, & Nesic, 2008), antilock braking system design (Zhang & Ordonez, 2007), thermoacoustic cooler (Li, Rotea, Chiu, Mongeau, & Paek, 2005), and fuel cell power plant (Zhong, Huo, Zhu, Cao, & Ren, 2008). There also have been considerable theoretical studies in ESC, such as stability studies on perturbation-based ESC (Krstic, 2000; Krstic & Wang, 2000), ESC for discrete-time systems (Joon-Young, Krstic, Ariyur, & Lee, 2002), PID tuning by ESC (Killingsworth & Krstic, 2006), ESC for nonlinear dynamic systems with parametric uncertainties (Guay & Zhang, 2003), and ESC for state-constrained nonlinear systems (DeHaan & Guay, 2005). The majority of ESC literature focused on two issues, the one is the searching for the optima, and the other is the regulation of the systems. The recent studies of Zhang and Ordonez (2007, 2009) presented a numerical optimization-based ESC (NOESC) framework that takes the advantage of numerical algorithms to find the optima online. However, these algorithms are unable to find the global optima if the assumption that the performance functions are convex and continuous does not hold. Furthermore, the NOESC is sensitive to measurement noise, due to the poor robustness of the numerical algorithms.

Particle swarm optimization (PSO) algorithm is a population-based stochastic optimization method which first devised by Kennedy and Eberhart (1995). PSO algorithm mimics the food-seeking behavior of birds or fishes. Due to its simplicity and effectiveness, PSO algorithm witnesses a considerable interest and is applied in many areas. The convergence of PSO algorithm is studied by deterministic method (Eberhart & Shi, 2001) or stochastic process theory (Jiang, Luo, & Yang, 2007). Clerc and Kennedy (2002) presented a convergence condition of PSO algorithm. Rapaic and Kanovic (2009) studied the time-varied parameter PSO algorithm and the selection of the parameters. Studies have shown that PSO algorithm is able to handle a wide range of problems, such as integer optimization (Laskari, Parsopoulos, & Vrahatis, 2002), multi-objective optimization (Dasheng, Tan, Goh, & Ho, 2007), and global optimization of multimodal functions (Liang, Qin, Suganthan, & Baskar, 2006). The recent application of PSO algorithm includes power systems (del Valle, Venayagamoorthy, Mohagheghi, Hernandez, & Harley, 2008), flights control (Duan, Ma, & Luo, 2008), and nuclear power plants (Meneses, Machado, & Schirru, 2009), to name a few. In control engineering, PSO algorithm is usually employed to identify the models (Panda, Mohanty, Majhi, & Sahoo, 2007), or to optimize the parameters of the controller offline (El-Zonkoly, 2006). PSO algorithm is usually regarded as an effective global optimization method. However, it is often used in offline optimization, and depends on the explicit form and the solvability of the performance functions. However, for some complex models, e.g. active flow control problems which described by Navier–Stokes equations, it is difficult to obtain the optimal parameters of the controllers by time-consuming numerical simulations.

In this paper, we extend the numerical optimization-based ESC (Zhang & Ordonez, 2007) by incorporating PSO algorithm into the extremum seeking framework. We also address the practicability issues of this scheme, and propose a reshuffle-then-insertion algorithm to reduce the control gain and oscillation. In the proposed scheme, a sequence converging to the global optima is generated by PSO with reshuffle-then-insertion algorithm. The sequence used as a guidance to regulate the state of the plant approach to the optimal set point. This paper is organized as follows. Section 2 gives a problem statement, where a PSO-based ESC (PSOESC) framework is introduced. We then review the standard PSO algorithm in Section 3. The details of the PSOESC scheme for linear time invariant (LTI) systems and feedback linearizable systems are presented in Section 4, where the reshuffle-then-insertion approach for improving the practicability of the PSOESC is also proposed. Section 5 presents the results of the numerical experiments. Finally, Section 6 concludes the paper.
2. Problem statement

In control practice, the problem of seeking for an optimal set point is encountered usually. In general, this problem can be represented as model
(1)
(2)
where is the state, is the input, is the performance output to be optimized, is a sufficiently smooth function on D, and is an unknown function. For simplicity, we assume

in this paper. Without loss of generality, we consider the minimization of the performance function (2). Unlike optimal control, extremum seeking control finds the optimal set point by online measurement of the performance function, without the knowledge of its explicit form.

ESC can be considered as a class of constrained optimization problem whose constraint is the differential Eq. (1), instead of the algebraic constrains in traditional optimization problems. Then, ESC control problem can be stated as:
(3)

When (1) is controllable, there always exists a control u such that state x transfers to any position in

in a finite time. We then can apply any optimization algorithm to produce a guidance sequence to determine a trajectory to the optimal set point in the state space (Zhang & Ordonez, 2007).

Similar to numerical optimization-based ESC (Zhang & Ordonez, 2009), we present a PSO-based ESC block diagram as Fig. 1, where the nonlinear plant

is modeled as (1) and the performance function J is (2). Unlike Zhang & Ordonez, 2009, we apply PSO algorithms to substitute the numerical optimization algorithms to produce the extremum seeking sequence {Xk}. The sequence {Xk} is generated as the target state in every seeking iterative step, based on the online measurement of y. The regulator K regulates the state of F follows the sequence as X1, X2, …, Xk, toward the optimal set point.
PSO-based extremum seeking block diagram

   

Fig. 1. PSO-based extremum seeking block diagram.
3. Particle swarm optimization

PSO is a population-based random optimization algorithm that mimics the behavior of bird or fish swarm in searching food. In the swarm, each particle has a variable speed, moving toward the positions of its own best fitness achieved so far and the best fitness achieved so far by any of its neighbors.

Let
is an n-dimensional search space. The size of the particle swarm is denoted as N. The position of the ith particle is represented as an n-dimensional vector , and its velocity is denoted as Vi = (vi1, vi2, …, vin)T  ∈ S, where i = 1, 2, …, N is the identity of each particle. The best position it has achieved so far is . The best position achieved so far in its neighborhood is Pgbst. On the time step k, the update equation of the basic PSO algorithm is (Kennedy & Eberhart, 1995)
(4)
(5)

where, c is an acceleration coefficient, r1, r2 are two independent random numbers with uniform distribution in the range of [0, 1]. Though this version of PSO has been shown to perform well in some optimization cases, it is not suitable in ESC context for its possible explosion property will lead to an unfeasible large control gain.

Clerc and Kennedy (2002) suggested a PSO algorithm which is known as the standard PSO, by introducing constriction coefficient χ. The update equation in the standard PSO is
(6)
(7)
where χ, φ1, φ2 are nonnegative constant real numbers. Obviously, by choosing appropriate parameters, (6) can transform to
(8)

where ω, c1, c2 are nonnegative constant real numbers. The velocity update Eq. (8) has three major components. The first component is referred to as “inertia” for it models the tendency of the particle to continue in the same direction it has been traveling. The second component is referred to as “memory” for it models the moving towards the best position ever found by the given particle. The third component is referred to as “cooperation” for it models the moving towards the best position by particles in neighborhood (del Valle et al., 2008).

From the formulation above, the standard PSO algorithm can be described as

Algorithm 1 Standard PSO algorithm

    Step (1) Initialize the swarm by assigning a random position to each particle.

    Step (2) Evaluate the fitness function for each particle.

    Step (3) Identify the best position 

    of each particle.

    Step (4) Identify the best position Pgbst.

    Step (5) Update the velocities and positions of all the particles using (6)or (8) and (7).

    Step (6) Repeat (2)–(5), until a stop criterion is met.

Because the positions of particles involve randomness, the convergence of PSO-involved algorithms is defined in a possibility sense in this paper. We say Xi(k) converges in mean square to Q∗, when
, for ∀i ∈ {1, 2, …, N}, where N is the population size of the swarm, and E represents the expectation operator.

Theorem 1

Given ω, c1, c2, when
, c1 + c2 > 0, and, where, and

, are all satisfied, then the particle systems driven byAlgorithm 1will converge in mean square to Pgbest.

Proof

Refer to the Theorem 5 in Jiang et al. (2007). □

Remark 1

Theorem 1 shows that the standard PSO algorithm guarantees every particle converges in mean square to the best position of the whole swarm. However, that position may not be the global optima or the local optima. There are modified PSOs that converge in mean square to local optima (van den Bergh & Engelbrecht, 2002) or global optima (Xin & Yangmin, 2007). In order to ensure the convergence to the optimal value, modified PSO algorithms need to make tradeoffs between exploration and exploitation, and numerous papers have presented various modified PSOs for this issue (Rapaic & Kanovic, 2009; van den Bergh & Engelbrecht, 2002; Xin & Yangmin, 2007; Zihui et al., 2008). For simplicity, in the following section we will focus on standard PSO algorithm, assuming that the standard PSO algorithm can converge in mean square to the global optima in the ESC cases. For the same reason, we also drop the “mean square” in the convergence analysis.
4. PSO-based extremum seeking scheme

Similar to Zhang and Ordonez (2007), we discuss the PSO-based extremum seeking control scheme in the order of linear time-invariant systems (LTI), feedback linearizable systems, and input-output feedback linearizable systems.
4.1. PSOESC for LTI systems

Consider a single-input-single-output (SISO) linear time-invariant system
(9)

with the performance function defined as (2). The matrices A and B are given, while J is unknown. We will find the minimum of J online by the measurement of the function value y. To do this, the following assumption is needed to ensure the feasibility of ESC for the LTI system (9).

Assumption 1

The LTI system (9) is controllable.

From the linear system theory, we have the following lemma that states the existence of a perfect regulator.

Lemma 1

For LTI system(9), whenAssumption 1holds, given system statex(tk) on timetk, regulation timeδk, andXk+1, lettk+1 = tk + δk, the control input

(10)
where
(11)

thenx(tk+1) = Xk+1.

Proof

From the assumption, the LTI system (9) is controllable, then the controllability Gramian G(δk) is nonsingular, thus the control input (10) is feasible. From linear system theory, the solution of (9) gives
(12)

Then, substituting (10) into (12) will complete the proof. □

Remark 2

From Lemma 1, there always exists a control u that regulates the state to any given state from any initial state, provided the LTI system is controllable. Thus, we can device a scheme that drives the state of the system to the global optima along a sequence produced by any optimization algorithms including that of swarm intelligence-based. By combining the Algorithm 1 and ESC, the PSO-based ESC scheme can be formed as follows.

Algorithm 2

PSO-based ESC scheme for LTI systems

    Step (1) Initialize the particle swarm, and let i = 1, k = 1.

    Step (2) Let particle i’s position as the ith target state, i.e. let 

    in (10) to get the control u.

    Step (3) The u regulates the plant to the target state, and the performance output y is readily to be measured by sensors.

    Step (4) Let the output y be the fitness 

    of the particle.

    Step (5) Let i = i + 1, repeat 2–4, until i = N, where N is the predefined size of the particle swarm.

    Step (6) Update the velocity and position according to (6) and (7).

    Step (7) Let k = k + 1, repeat 2–5, until a stop criterion is met.

We say (2)–(4) is a PSO loop, and (2)–(7) an ESC loop. The convergence result of Algorithm 2 can be shown by Lemma 1 and Theorem 1, and is presented as the following theorem.

Theorem 2

Let X∗ be the solution of optimization problem(3), i.e.
, if the solution of problem

obtained byAlgorithm 1converges to X∗, thenAlgorithm 2drives system(9)to its minimum performance function value J∗.

Proof

Let
denote the position of the ith particle on the kth ESC loop step. The PSO scheme described as Algorithm 1 produces a sequence
(13)

in k steps. From Theorem 1, the sequence converges to Pgbest, and from the assumption in the statement of Theorem 2 we also have Pgbest = X∗, when k → ∞. □

Let x(t0) denote the state of LTI system (9) at time t = t0. From Algorithm 2, we have the target state
. Then, at time t1 = t0 + δ0, from Lemma 1, we will have . For the same reason, we also have

.

By induction, at the Kth ESC loop, we suppose
, where i = 1, 2, …, N. Then, at the (K + 1)th ESC loop, from the Lemma 1, we will have
(14)
where
(15)
(16)

For the same reason, we will have
, where i = 1, 2, …, N. Thus, the Algorithm 2 guarantees that the state of system (9) follows exactly the sequence

Therefore, the performance output of system (9) converges to its minimum value J∗.

Remark 3

Since the standard PSO algorithm and its variants (del Valle et al., 2008; Xin & Yangmin, 2007) have shown their effectiveness in global optimization problems, it is safe to assume Algorithm 2 converge to the global optima in ESC. Thanks to the global searching ability of PSO algorithm, we need not apply the assumptions in Zhang and Ordonez (2007) to ensure the global convergence condition for the numerical-based optimization algorithms. The performance function J can be nonconvex and discontinuous in ESC context now.

Remark 4

Due to the population-based feature of PSO algorithms, one may think that the use of PSO would require N sensors, or plants, able to provide N simultaneous measurements of J every time a new element in the optimization sequence is needed. However, this is not the case, since Algorithm 2 ensures that we can track each state represented by a particle in the swarm in a serial manner, thus the sensors needed in PSOESC are no more than other ESCs. It should be noted that, compared to NOESC, PSOESC have to spend more time to obtain the best state in an ESC loop. However, this cost can be regarded as the expense paid for the global optima. For example, in some active flow control cases, the plants keep working for a long period, and the drag reduction obtained by ESC would be great beneficial (Beaudoin et al., 2006), so it is reasonable to spend some time to find the global optima.

Remark 5

The Algorithm 2 requires a regulator to drive the state of the system (9) from one to another along the sequence produced by the PSO loop. However, due to the randomness feature of the update equation (6), at some stages, the regulator gain may be very large, and the state of the system would oscillate rashly. This could cause serious problems in real-world applications. This difficulty may hinder PSO or any other swarm intelligence-based algorithms from being applied in the ESC context. We will address this issue in Section 4.4, and provide an algorithm that rearranges the sequence to reduce the control gain and oscillation.

Remark 6

It is not difficult to extend the Algorithm 2 to the multi-input-multi-output (MIMO) systems, like the NOESC schemes presented in Zhang and Ordonez (2007). Moreover, other design of controller (10) can be applied for a more robust regulator as suggested in Zhang and Ordonez (2007, 2009).
4.2. PSOESC for feedback linearizable systems

Consider a SISO nonlinear affine system
(17)

with the performance function defined as (2), where f and g are smooth functions on D.

Assuming the system (17) is feedback linearizable on D, we have that there exists a diffeomorphism
such that Dz = T(D) contains the origin, and transforms the system (13) into the form (Khalil, 2002)
(18)

where z = T(x), and (A, B) is controllable, γ(x) is nonsingular for all x ∈ D. Then, we can extend the results on LTI system in 4.1 to the feedback linearizable system (17).

Let Zk+1 = T(Xk+1), then we can design a regulator
(19)
where
(20)
(21)

such that the state of system (17) transfers from x(tk) to Xk+1 in a finite time δk.

Then, we have the PSOESC scheme for the feedback linearizable system (17), represented as the following algorithm.

Algorithm 3

PSOESC scheme for feedback linearizable systems

    Step (1) Initialize the particle swarm, and let i = 1, k = 1.

    Step (2) Let particle i’s position as the target state, i.e. let 

    , then let Zk+1 = T(Xk+1) to get the control u by (15).

    Step (3) The u regulates the plant to the target state, and the performance output y is readily to be measured by sensors.

    Step (4) Let the output y be the fitness 

    of the particle.

    Step (5) Let i = i+1, repeat 2–4, until i = N, where N is the predefined size of the particle swarm.

    Step (6) Update the velocity and position according to (6) or (8) and 7.

    Step (7) Let k = k + 1, repeat 2–5, until a stop criterion is met.

The convergence analysis mainly follows the proof of Theorem 2. Similar remarks like 3–6 in Section 4.1 also apply here.
4.3. PSOESC for input–output feedback linearizable systems

The results on state feedback linearizable systems in Section 4.2 can be extend to input-output feedback linearizable systems by substituting the regulator with appropriate designs, in a similar way as NOESC. The details refer to Zhang and Ordonez (2007).
4.4. Regeneration of the sequence produced by the PSO algorithm

As indicated by Remark 5, tracing the sequence produced by the standard PSO algorithm would cause serious problems in practice. The sequence has to be regenerated to suit ESC context. Without loss of generality, we discuss the regeneration of the sequence produced by Algorithm 2 presented in Section 4.1.

Consider system (6), and the state from time tk to tk+1. From (10), the output of the regulator
in a state transfer procedure is determined by the regulation time δk = xk+1 − xk, the initial state x(tk), and the terminal state x(tk+1). Given the regulation time δk, if we want to minimize the maximum control, then we need an algorithm to reshuffle the sequence {xk} to minimize the . However, the optimal regeneration is difficult to devise because it is NP-hard as Problem 1 shows.

Problem 1

Let G = {V, E} be a complete weighted graph, where V = {1, 2, …, N} is the vertex set, indicating the states in sequence
, and E is the edge set. We also denote D as the distance matrix, where D = [di,j]N×N, and
(22)
(23)
Now, the searching for the optimal sequence is equivalent to solve the programming problem represented as:
(24)

where, |S| is the number of the vertices contained in G in S.

The Problem 1 is also known as the bottleneck traveling salesman problem (BTSP), and is shown NP-hard (Kabadi, 2002), i.e. there is no polynomial algorithm for this kind of problems so far.

We propose here an algorithm to regenerate the PSO-produced sequence for the practical use of PSOESC, based on the following consideration.

In practice, limiting maximum control gain within a certain bound is sufficient to have a practical controller. From (10), we have that for each predefined Σ > 0, there always exists a regulation time δk such that ||u(t)|| < Σ. Therefore, the control gain can be limited in a bound by choosing a sufficiently long regulation time δk. However, too long regulation time may be unacceptable in practice. Another feasible approach is to insert new intermediate states between two states that may lead to unacceptable large control gain. These new intermediate states would smooth the original state sequence produced by the PSO algorithm. However, this procedure may prolong the regulation time as well. Therefore, we propose a reshuffle procedure below, in order to reduce the requests of the insertion procedure as less as possible.

Consider the sequence {xk} produced by PSO in an ESC loop. First, we simply choose the nearest state to x1 from {xk} as x2, then we choose the nearest state to x2 from the remainder states in {xk} as x3, and so on. Repeating the procedure will lead to a new sequence that each distance of the first k − 1 states are minimal. The distance of the last two state, i.e. xk+1 and xk may be very large. Moreover, there may still be some unacceptable large distances in the reshuffled sequence {x1, x2, …, xk−1}. However, the requests of the insertion procedure are much less than the original sequence.

By combination of the reshuffle procedure and the insertion procedure above, we have the following algorithm.

Algorithm 4

Reshuffle-then-Insertion algorithm for PSOESC

    Step (1) Obtain the distances di,j according to (22).

    Step (2) Let the last state of the last ESC loop be the basis point x1.

    Step (3) Find the state nearest to the basis point x1 in the sequence 

    , and let the point be x2.

    Step (4) Delete the state obtained in step 2) from 

    .

    Step (5) Compute the distance d1,2 between x1 and x2.

    Step (6) If d1,2 > Σ, then insert a new state into x1 and, and let the new state be x2, then, let the original x2 be x3.

    Step (7) Repeat step 5 and 6, until d1,2 >Σ, every time add 1 to the index of every point in the new sequence after x2.

    Step (8) Let the last point, i.e. the x2 obtained in step 3), be the new basis point, and repeat step 4 ∼7, until the sequence 

    is empty.

The result that the Algorithm 4 is applied on a random sequence in the range of [0, 10] is shown in Fig. 2. It can be seen that the algorithm smoothes the original sequence remarkably, and the control gain in each step is limited in a predefined bound.

Remark 7

Algorithm 4 is also applicable to ESC based other swarm-intelligence-based algorithms, such as Artificial Bee Colony, Ant Colony Swarm and Bacteria Foraging.
The regeneration of the target state sequence

   

Fig. 2. The regeneration of the target state sequence. The original sequence is built up with 30 random numbers, indexed by k, distributed uniformly in [0, 10]. The maximal distance between two neighbors in the original sequence is 7.2183, which is reduced to 3 by the regeneration algorithm. The two inserted intermediate states in the regenerated sequence are indicated by arrows.

Then, we have a practical PSOESC scheme as follows.

Algorithm 5

PSOESC scheme with reshuffle-then-insertion algorithm

    Step (1) Initialize the particle swarm, and let i = 1, k = 1.

    Step (2) regenerate the sequence 

    by Algorithm 4.

    Step (3) Let particle i’s position as the ith target state, and get the control u by (10).

    Step (4) The u regulates the plant to the target state, and the performance output y is readily to be measured by sensors.

    Step (4) Let the output y be the fitness 

    of the ith particle.

    Step (5) Let i = i+1, repeat 3–4, until i = N′, where N′ is the size of the regenerated sequence.

    Step (6) Update the velocity and position according to (6) or (8) and 7.

    Step (7) Let k = k + 1, repeat 2–6, until a stop criterion is met.

5. Numerical experiments

The numerical experiments were carried out on a personal computer running on the Matlab/Simulink environment. The parameters of the standard PSO were chosen as w = 0.9, c1 = 0.12, and c2 = 0.012. For the purpose of simulation, performance functions were provided explicitly, while they would be unknown in real-world applications. As mentioned above, the convergence of the PSOESC scheme is described in mean square sense, and the convergence to the global optima is not guaranteed by the standard PSO. Therefore, the given illustrations are typical results obtained by numerous simulations. We define the success rate of the PSOESC scheme as Rc = Nc/Nt, where Nc is the number of times PSOESC converged to the known global optima, and Nt the number of repeated test times. In practical applications, the measurement noise, unmodeled dynamics, input disturbance, and computational error are inevitable, so the convergence to the precise optima is unnecessary. We define the error limit as ±0.05J∗, where J∗ is the known global minimal value.
5.1. First-order single-input–single-output (SISO) LTI system

We first considered a LTI system
(25)
with the performance function defined as
(26)

where u is the control obtained by (10). The plot of J(θ) in the range of [0, 1] is illustrated in Fig. 3. Obviously, there are two local minima in [0, 1]. The two minima 0.225, 0.883 can be easily found by the optimtool toolbox in Matlab by choosing different start point, and the function value according to the two minima are −0.080 and −0.308, respectively. When setting the population size as 10 particles, we observed a 100% success rate in 120 repeated tests. We also tested a swarm with only 2 particles, and have a success rate of 82% with much less steps for convergence than that of 10 particles cases. This is shown in Fig. 4.
The plot of the function J(θ)=−0

   

Fig. 3. The plot of the function J(θ) = −0.4sin(9θ)sin(θ) in [0, 1].
PSOESC for the first-order LTI system, where k indicates the iteration steps

   

Fig. 4. PSOESC for the first-order LTI system, where k indicates the iteration steps.

By comparison, the performance of NOESC is dependent on the initial point. It would convergence rather fast provided a good initial point was chosen. However, if we choose θ0 = 0 as the initial point, for example, the seeking procedure would converge to the local minimum 0.225.
5.2. Second-order LTI system

Consider a system which has been discussed in Zhang and Ordonez (2007):
(27)
(28)

The performance function (28) is also known as Rosenbrock function whose minimum is x∗ = [1 1]T, and J(x∗) = 0. In this case, we use 10 particles, and the result is shown in Fig. 5, where it can be seen that the PSOESC drives the performance function converge to the minimum successfully. To test the robustness of the PSOESC, we disturbed y with a random number uniformly distributed with amplitude 0.2. As Fig. 6 shows, the PSOESC scheme drives the state of the system approaching to the optimal set point successfully, which is consistent with the stability analysis of swarms in noisy environment (Liu & Passino, 2004).
PSOESC for the second-order LTI system

   

Fig. 5. PSOESC for the second-order LTI system. The performance function is the Rosenbrock function. The size of the particle swarm is 15.
PSOESC for the second-order LTI system with measurement noise

   

Fig. 6. PSOESC for the second-order LTI system with measurement noise.
5.3. PSOESC for ABS design

Consider a one wheel ABS design case which is usually used in ESC literatures (Zhang & Ordonez, 2006, 2007). The problem can be modeled as (Zhang & Ordonez, 2007)
(29)
(30)
where v denotes the linear velocity, ω is the angular velocity, N = Mg is the weight of the wheel, R is the radius of the wheel, I the moment of inertia of the wheel, Bω the braking friction torque, u braking torque, μ(λ) the friction force coefficient, and

is the slip, 0 ⩽ λ ⩽ 1 for ωR ⩽ v. There exists a maximum μ∗ for the friction force coefficient μ(λ) at the optimal slip λ∗. The main challenge in designing ABS systems is to devise a robust control scheme that can handle the uncertainty due to environment, for the reason that λ∗ and μ∗ will change as the road condition changes. Now, the design purpose is to device a control u such that maximize the μ(λ).

Since the
can be measured by an acceleration sensor, we can let the control be
(31)
such that transform the system (29) and (30) into the form
(32)
where η is the regulator devised according to (10), and
(33)

where λk+1 is the target state produced by the PSO algorithm applied in ESC loop.

In the simulation, the parameters were chosen as (Zhang & Ordonez, 2007): M = 400 kg, B = 0.01, R = 0.2 m , λ(0) = 0, a = 1, δk = 0.5. For the simulation purpose, we also postulated that the relation of μ(λ) and λ satisfies
(34)

where, μ∗ = 0.25, λ∗ = 0.6. Obviously, the optimal λ of (34) is λ∗ = 0.6. PSOESC scheme with 10 particles worked well, and we observed a 99% convergence rate in 120 repeated tests. We also tested a swarm with only a single particle, and have a convergence rate of 61%. This is shown in Fig. 7. To test the robustness of the PSOESC, we added on μ a random disturbance with uniform distribution in the range of [−0.2, 0.2] to simulate the influence of noises. As Fig. 8 shows, the PSOESC scheme drives the state of the system converging to the optimal slip. By comparison, as Fig. 9 shows, the NOESC scheme, which is based on the simplex search method in our experiment, converges fast to the optimal slip when no measurement noises are added, whereas it converges to the wrong point when measurement is disturbed by noise,.
A single particle-based ESC for the ABS problem

   

Fig. 7. A single particle-based ESC for the ABS problem.
PSOESC for the ABS with measurement noises

   

Fig. 8. PSOESC for the ABS with measurement noises.
NOESC for the ABS without measurement noises (top), and with measurement noises…

   

Fig. 9. NOESC for the ABS without measurement noises (top), and with measurement noises (bottom).
5.4. PSOESC for a system with incontinuous performance function

Finally, we discuss a LTI system with a discontinuous performance function. Consider the system (20) with performance function defined as
(35)

The plot of this function in range [0, 1] is illustrated in Fig. 10. The PSOESC scheme drives the performance output of the system converge to the optima, as Fig. 11 shows. This test illustrates the intrinsic advantage of the PSOESC in solving discontinuous optimization problems. By comparison, the existing NOESC (Zhang & Ordonez, 2005) is unable to deal with such kind of problems.
The plot of function (35)

   

Fig. 10. The plot of function (35).
PSOESC for a system with discontinuous performance function

   

Fig. 11. PSOESC for a system with discontinuous performance function.
6. Conclusion

This paper presents a PSO-based ESC scheme that is able to find the optimal set point online. The PSO algorithm produces a sequence converging to the global optima. The sequence serves as a guidance to regulate the state of the plant approaching to the optimal set point. We also propose a reshuffle-then-insertion algorithm that is able to reduce the control gain and oscillation, thus improving the practicability of PSOESC scheme.

The numerical experiments show the effectiveness of the scheme. The PSOESC scheme found the global optima successfully in various cases. The simulations also demonstrate the ability of PSOESC scheme for handling measurement noise. The disadvantage of PSOESC is that it costs more time in every ESC loop. However, the robustness to measurement noise makes PSOESC preferred to NOESC in some cases.

The scheme proposed in this paper will be easily extend to other swarm intelligence-based algorithms, such as Artificial Bee Colony, Ant Colony Optimization and Bacteria Foraging Algorithm.