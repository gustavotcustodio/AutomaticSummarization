Sales forecasting is crucial in fashion business because of all the uncertainty associated with demand and supply .
Many models for forecasting fashion products are proposed in the literature over the past few decades .
With the emergence of artificial intelligence models , artificial neural networks ( ANN ) are widely used in forecasting .
ANN models have been revealed to be more efficient and effective than many traditional statistical forecasting models .
Despite the reported advantages , it is relatively more time-consuming for ANN to perform forecasting .
In the fashion industry , sales forecasting is challenging because there are so many product varieties ( i.e. , SKUs ) and prompt forecasting result is needed .
As a result , the existing ANN models would become inadequate .
In this paper , a new model which employs both the extreme learning machine ( ELM ) and the traditional statistical methods is proposed .
Experiments with real data sets are conducted .
A comparison with other traditional methods has shown that this ELM fast forecasting ( ELM-FF ) model is quick and effective .
Forecasting refers to predicting the future events , usually based on historical data .
Statistical models have been widely employed in conducting forecasting for a long time ( Abraham & Ledolter , 1983 ; Box , Jenkins , & Reinsel , 2008 ; Diebold & Rudebusch , 1991 ) .
Most of the statistical models are computationally fast in doing forecasting .
For instance , the ARIMA model often finishes making a time-series forecast within a few seconds based on hundreds of historical data points ( Box et al. , 2008 ) .
However , statistical methods may fail to perform well in forecasting problems with highly complex data pattern .
As a result , expert systems based on ANN models have been well proposed in making forecasting and the results are impressive ( Chang , Wang , & Liu , 2007 ; Chang , Wang , & Tsai , 2005 ; Hamzaçebia , Akayb , & Kutayb , 2009 ; Ling , Leung , Lam , & Tam , 2003 ) .
Despite being shown to be versatile , most ANN based sales forecasting methods use gradient-based learning algorithms , such as the back-propagation neural network ( BPNN ) ( Hecht-Nielsen , 1989 ) , and problems such as over-tuning and longer computational time arise .
For example , some ANN-based algorithms , which are devoted to improving the forecasting accuracy , may easily take hours in completing the task even for a simple forecasting task ( Au , Choi , & Yu , 2008 ; Harrald & Kamstra , 1997 ) .
As a result , while the ANN model can give accurate forecast , the required forecasting time can be a big barrier to its real-world applications .
As we know , for many industries , forecasting need be done very quickly .
Classic examples include the forecasting of stock price ( Kim & Chun , 1998 ; Leigh , Purvis , & Ragusa , 2002 ) , and forecasting sales in fashion business ( Chern , Ao , Wu , & Kung , 2010 ; Choi & Chow , 2008 ; Saad , Prokhorov , & Wunsch , 1998 ) .
Undoubtedly , nowadays , how to develop a reasonably accurate and fast forecasting method becomes an important research topic .
A relatively novel learning algorithm for single-hidden-layer feedforward neural networks ( SLFN ) called extreme learning machine ( ELM ) has been proposed recently ( Huang , Zhu , & Siew , 2006a ) .
ELM not only learns much faster with a higher generalization performance than the traditional gradient-based learning algorithms but it also avoids many difficulties faced by gradient-based learning methods such as stopping criteria , learning rate , learning epochs , local minima , and the over-tuned problems .
As a consequence , the ELM model reduces learning time of ANN dramatically and it even makes it possible to apply ELM in real-time applications such as real-time control ( Huang , Zhu , & Siew , 2006b ) .
In this paper , we propose a new method which employs the extreme learning machine ( ELM ) , with the combination of statistical methods , for making fast forecasting for fashion products with a focus on real-world applications .
The forecasting accuracy and time cost of the ELM model are both explored .
Experiments are conducted with real data from the fashion industry .
The proposed system is shown to be effective and efficient for conducting fast forecasting for fashion products .
The rest of this paper is organized as follows .
We review the related literature in Section 2 .
We explore the structural properties as well as features of ELM in Section 3 .
The fast forecasting model and the respective experimental analysis are proposed in Section 4 .
We conclude in Section 5 .
Statistical models have been widely applied for forecasting for decades .
Models like linear regression , exponential smoothing , ARIMA , SARIMA ( Abraham & Ledolter , 1983 ; Box et al. , 2008 ) , etc .
are proved to be effective and efficient under a “ prerequisite ” that the users have the knowledge to choose the right model and select the appropriate parameters for exercising that model .
Statistical models are usually fast in conducting forecasting and a forecasting which makes use of thousands of historical datasets can often be finished in seconds .
However , the accuracy of these models depends highly on the expert ’ s knowledge of the model selection and the underlying pattern ( which is hidden in the data ) and this poses a limitation for many real life applications when the decision makers are not sufficiently knowledgeable .
In light of this , companies start to employ artificial neural network ( ANN ) because it has appealing features such as the capability of learning ( modeling ) data with any underlying features , and requiring no specific expert knowledge on the problem to be studied in order to complete the task .
With the emergence of ANN , forecasting systems that rely on it have been widely developed and studied ( see Hansen & Nelson , 1997 for more discussions ) .
One of the most popularly used ANN models in forecasting is the three-layer feed-forward back-propagation neural network ( 3L-FF-BPNN ) .
This model becomes popular because it is robust and can approximate function mappings of any kind .
Thus , it becomes an ideal tool for learning the mapping between forecasting inputs and outputs , and then it can help generalize the pattern to produce forecasting results .
For some recent publications and more details on the applications of 3L-FF-BPNN in forecasting , we refer the interested readers to ( Acciani , Brunetti , & Fornarelli , 2006 ; Chaoui , Sicard , & Gueaieb , 2009 ; Orlowska-Kowalska & Szabat , 2007 ; Takahashi , Ukishima , Kawamoto , & Hirota , 2007 ) .
Although there are many advantages of ANN in forecasting ( which make ANN popular ) , the learning algorithms for ANN are generally slow .
It is nothing surprising for users to observe that in order to train the ANN and generate the forecasting outputs it could take hours , or even days ( Huang et al. , 2006a ) .
Over the past decades , gradient descent-based methods have mainly been used in various machine learning algorithms of ANN .
However , it is well-known that gradient descent-based learning methods are generally very slow as by nature many iterative learning steps are required by such learning algorithms in order to obtain a good learning performance .
Rather recently , Extreme Learning Machine ( ELM ) for Single-hidden Layer Feedforward Networks ( SLFN ) is proposed in ( Huang et al. , 2006a ) .
The ELM algorithm randomly assigns the input weights and hidden layer biases of SLFN , whereas the output weights ( linking the hidden layer to the output layer ) of SLFN are analytically determined through simple generalized inverse operation of the hidden layer output matrices .
The ELM is reported to be very fast and its learning speed can be thousands of times faster than the traditional feed-forward network learning algorithms like back-propagation ( BP ) algorithm while obtaining better generalization performance ( Huang et al. , 2006a ) .
Since ELM is a learning algorithm of SLFN , it apparently can also be employed in conducting robust forecasting ( Miche , Bas , Jutten , Simula , & Lendasse , 2008 ; Singh & Balasundaram , 2007 ) .
However , ELM is not perfect because it is known that ELM ’ s forecasting outputs are unstable when compared with the traditional ANN and statistical models .
As a result , an averaging approach is proposed in ( Sun , Au , & Choi , 2007 ) with an aim to improve the stability of the ELM forecasting .
Even though this averaging step has been shown to be effective , it also requires repeatedly running the ELM for a lot of times which directly leads to a lengthened forecasting time .
As a result , it is not as quick as the original ELM model .
Undoubtedly , as we proposed in Section 1 , fast forecasting ( Yu , Choi , & Hui , 2011 ) is necessary for many real-life applications .
Even though various ANN schemes have very recently been proposed to offer reasonable forecasting results in terms of accuracy and time ( Chen & Ou , 2009 ; Wu , 2009 , 2010 ) , many of them are not fast enough for real applications for sales forecasting ( see Park , Rilett , & Han , 1999 ; Swanson & White , 2006 for more details ) .
As we all know , in many circumstances , forecasting has to be completed within a certain short time limit ( Park et al. , 1999 ) in order to support the other important functions such as inventory planning .
However , the above reviewed forecasting models , both the ANN-based and statistics-based models , have not sufficiently incorporated the time costs into formulating and developing the respective forecasting algorithms .
In light of this and based on the above reviewed literature , we present a model which extends ( Yu et al. , 2010 ) and employs the ELM algorithm , combined with statistical tools , in conducting forecasting .
In addition , we explicitly take into account of the time limit for forecasting .
With this revised scheme , we believe ( and will show in the following ) that an intelligent fast sales forecasting model ( IFSFM ) can be established .
This IFSFM not only can provide good forecast result but it can also do so within a specified time limit .
It is hence especially suitable for the forecasting sales of products in the fashion industry which are known to be highly volatile and with a large number of SKUs .
It is known that ELM tends to be unstable in a single run forecasting ( Rong , Ong , Tan , & Zhu , 2008 ; Singh & Balasundaram , 2007 ; Sun , Choi , Au , & Yu , 2008 ) .
An extended ELM method ( ELME ) is hence proposed in which we need to compute the average “ predicting time series ” from the given dataset by first repeatedly running the ELM for P times .
It is known that the results obtained by the ELME will become more stable when the parameter P becomes larger .
Unfortunately , the computation time also increases when the parameter P is larger .
Considering both the computation time and the stability of ELM , an appropriate value of parameter P must be found .
Please notice that when P is too small , large variation of forecasting results which is undesirable .
Making reference to ( Sun et al. , 2007 ) , P = 100 is a reasonable parameter for ELME .
Thus , in our model , only when the time limit T is exceeded will we find P less than 100 to produce the forecasting which could meet our requirements .
The fashion product data Two fashion products sales datasets are studied in our work .
The first one is from a fashion retailing company in Hong Kong .
In this dataset , sales data of several fashion SKUs , together with other related properties of the SKUs are included .
Table 1 illustrates the dataset .
In the analysis , we want to find the relationship of sales amount with respect to the attributes such as colour , size , and price of the fashion product .
This data set consists of 120 samples .
The first 60 % of the samples are used as training data to train the ELM , the following 20 % is the validation data , and the remaining 20 % of the samples are used to do the forecasting test .
This scheme has been employed in the literature ( e.g. , Au , Choi , & Yu , 2008 ) .
The second dataset is from an online fashion shop .
The POS data of three months sales are extracted from the shop ’ s sales log .
A Semantic Web tool – piggy bank ( Huynh , Mazzocchi , & Karger , 2005 ) is employed in the extraction of online data .
Three months data of five SKUs are retrieved and employed in the study .
Similar to the first dataset , attributes such as colour , size , and price are given as inputs and the sales amount as output .
Table 2 lists the sample data for one the SKUs .
The dataset consists of 320 samples .
The dataset is segmented into three parts of training data , validation data and forecasting data under the similar scheme as the first dataset .
Table 1 .
Real data of the sales of a fashion SKU vs. the parameters of the SKU ( 10 samples listed ) .
Sample number Sales amount Colour code Size Price 1 1 92 29 196 2 2 99 25 196 3 1 81 25 196 4 1 99 26 196 5 1 92 24 196 6 1 92 25 196 7 1 92 29 196 8 1 92 24 196 9 2 99 30 196 10 3 92 30 196 Table 2 .
Real data of the SKU sales of an online shop vs. the parameters of the SKU ( 10 samples listed ) .
Sample number Sales amount Colour code Size Price 1 1 11 4 139 2 3 11 3 139 3 1 10 4 139 4 1 11 4 139 5 2 11 3 139 6 2 10 3 139 7 1 11 4 139 8 3 11 3 139 9 1 11 4 139 10 5 11 4 139 3.2 .
The relationship between time cost and P & N To study the feature of the ELM forecasting model , the fashion sales dataset is employed in the ELME model .
Number of repeat P is varied within a given range to study its relationship with the time cost .
The time cost is also related to the number of neurons N , and for a given fixed N , the time cost at P = 1 is a constant c. It is thus intuitive to propose that the overall time cost t of the model is linearly related to the repeating time , i.e. , ( 1 ) where c is a constant time cost for a single run of the forecasting step .
With the first set of data as in Table 1 , an experiment is conducted to use the ELM model for making a forecast of fashion sales .
Fig 1 illustrates that the time cost is almost perfectly linearly related to P. Note that there are some fluctuation in Fig 1 , and these “ special cases ” are usually caused by the system processes of Windows OS that consume CPU time during the execution of the testing program .
As the Matlab testing program is running in a time-sharing manner and this causes the timing functions of Matlab unreliable in measuring the efficiency of a program ( Davis & Sigmon , 2004 ) .
Time cost t of the ELM model vs Fig 1 .
Time cost t of the ELM model vs. P. The major and most time-consuming part of ELM training is to solve two multiplications of matrixes of dimensions ( N * S ) • ( 1 * S ) and ( N * S ) • ( N * 1 ) , where N is the hidden neuron number , and S is the number of training simple ( Huang et al. , 2006a ) .
Considering a given S , its time complexity can be given by ( 2 ) which shows that the running time is a quadratic function of the number of hidden neurons .
In addition , the approximate function for t is given by ( 3 ) The numerical experiment ’ s result on the time cost vs. the number of neurons ( N ) is given in Fig 2 , and a simple fitting can yield the function in ( 3 ) as a = 0.0003 , b = 0.016 , c = 4.8 .
The curve in Fig 2 is observed to fit the time curve very well .
Time cost t of the ELM model vs Fig 2 .
Time cost t of the ELM model vs. N. These analyses reveal that the time cost t of the ELME forecasting model is linearly related to the repeating time that is denoted by P , and is quadratically related to the number of neurons N of the ELM structure .
Given these , the time cost of ELME in forecasting can be estimated when we have a few records of the time costs with some starting “ initial ” P and N. 3.3 .
The relationship between forecasting time cost and sales data When the time cost of the forecasting model is found to be related to the repeating times and hidden neuron numbers in the previous analysis , it is also related to the properties of the fashion sales data such as variance of the sales , and/or the number of data points in the sample .
In this section , the second dataset in Table 2 is studied to reveal the relationships between the forecasting time cost and the features of the fashion sales data .
We select this specific dataset because its number of samples is much larger than the datasets in Table 1 ( 320 vs. 120 ) and it can better reflect the relationships that we want to explore .
In the analysis , the forecasting model is run several times with the number of samples varied from 10 to 320 , the respective time costs are depicted in Fig 3 .
As we can observe , the time cost is generally increased when the number of samples is increased .
The relationship of the time cost vs. number of samples actually tends to be linear , because when the IFSFM model runs , the data samples go into the model one by one , and this is very similar to the situation of repeatedly running the model .
Similar to ( 1 ) , we can give the time cost by ( 4 ) where d is the average running time of the IFSFM for one data sample and S the total number of samples .
The forecasting errors ( measured in terms of mean-squared error ( MSE ) ) with respect to the number of samples are also shown in Fig 4 .
A sharp drop is observed before the number of samples reaches some point near 50 , and the fluctuation becomes minor thereafter .
This indicates that forecasting based on fewer data would cost a shorter time provided that the data sample size is not too small ( otherwise the forecasting error will increase ) .
The forecasting time cost vs Fig 3 .
The forecasting time cost vs. number of samples of the fashion sales data .
The forecasting error ( MSE ) vs Fig 4 .
The forecasting error ( MSE ) vs. number of samples of the fashion sales data .
Properties of the sales data are also related to the IFSFM model ’ s time cost .
Specifically , the variation of the sales is studied .
With the original data coming from Table 2 , the sales amount is artificially changed to increase its variance .
Two different approaches are used to increase the variance of sales data .
One of the approaches is to enlarge the fluctuation of the original data in which a given number multiplies the original sales amounts .
This makes the variance increased as presented , ranging from 10.2 to 1.02 × 109 .
We call it the multiplicative case and the details are listed in Table 3 .
The other approach is to add another data series into the original one to introduce extra variation and we call it the additive case .
To be specific , a white noise data series is added into the original series of sales quantities .
The variance of the white noise varies in a given range from 1 to 1.0 × 108 and it causes the combined data variance to range from 20.1 to 1.02 × 109 .
Fig 5 illustrates the results .
With the first approach , the time cost of the IFSFM model is almost unchanged , as is shown in Table 3 .
This is because the ELME model internally performs normalization on the input–output parameters , so the multiplication hardly has any effect on the forecasting performance .
However , when a random white noise is added to the sales dataset , the time cost is observed to increase with the increase of the variance of the changed sales data .
Table 4 shows the details .
Table 3 .
Increased variance vs. time cost under the multiplicative case .
Variance of changed sales Time cost ( s ) 10.2 41.2981 1.02 × 103 42.2016 1.02 × 105 39.9028 1.02 × 107 41.4064 1.02 × 109 41.5208 The forecasting time cost vs Fig 5 .
The forecasting time cost vs. number of samples of the fashion sales data .
Table 4 .
Increased variance vs. time cost under the additive case .
Variance of the white noise ( times of original one ) Variance of the changed sales Time cost ( s ) 1 20.1 47.7666 1.00 × 102 1.02 × 103 49.5613 4.00 × 102 4.07 × 103 48.4347 9.00 × 102 9.15 × 103 49.0080 1.60 × 103 1.63 × 104 49.4823 2.50 × 103 2.54 × 104 50.7420 1.00 × 104 1.02 × 105 51.0042 4.00 × 104 4.06 × 105 50.0875 1.00 × 106 1.02 × 107 52.1802 1.00 × 108 1.02 × 109 52.1623 3.4 .
The relationship between forecasting error and P & N Generally , the forecasting error E drops as P is increased .
In our experiment , as shown in Fig 6 , there are sharp drops of E when P is small , and they become stable and almost unchanged later .
This gives guidance on how to set the appropriate P in our model : P does not necessarily need to be set at a very large number .
The forecasting error E vs Fig 6 .
The forecasting error E vs. repeating time P. The number of neurons of the ELM is certainly a crucial factor that influences the forecasting error .
In Fig 7 , the two fashion sales datasets are tested with forecasting error vs. neuron number .
For all the four tests : P is set to 500 which is found to be large enough to produce stable forecasting .
Fig 7 shows that similar to the traditional ANN the error E of ELM forecasting is generally reduced with the increasing of neuron number N , until some point N when over-fit occurs and then E will stop reducing and may even become increasing .
The forecasting error E vs Fig 7 .
The forecasting error E vs. neuron number N. Moreover , by observing Fig 7 , before the occurrence of overfit , the curve of MSE vs. N appears to be a negative exponential curve ( while this is not obvious in Fig 7a because the number of data points is only 2 before overfit occurs , it is much more obvious in Fig 7b ) .
If the theoretical negative exponential function of such curve is given as ( 5 ) the validation curve in Fig 7b , when N < 13 ( before the overfit ) , can be fitted as in Fig 8 .
This illustrates that the function in ( 5 ) can fit the curve of E vs. N quite well .
Curve fitting of MSE and Neuron number N based on N=1–13 Fig 8 .
Curve fitting of MSE and Neuron number N based on N = 1–13 .
If the fitted function is used to predict the overfit when we have MSE tested with N = 2–10 , the function in the form of ( 5 ) can be retrieved as in Fig 8 .
With the function in Fig 8 , it is at around N = 12 when the changing of MSE is very minor ( less than 5 % of the previous MSE ) , and this prediction of overfit is close to the one we know from Fig 6b ( which is 13 ) .
The prediction of N of is very close to the real one , and the predicted best E is also very near , 0.02082 from the function in Fig 9 having N = 12 , to the real one 0.02080 from real validation MSE in Fig 8 .
Such exponential curve fitting must have at least 3 points , so this method is only useful when the overfit N is larger than 3 .
Curve fitting and prediction of overfit based on N=1–10 Fig 9 .
Curve fitting and prediction of overfit based on N = 1–10 .
Based on the above analysis , we know that the time cost of the ELM model is linearly related to P , and is quadratically related to the neuron number N. The forecasting error of the ELM model is related to the underlying data feature and is almost negatively exponential related to N. Based on these findings , a fast forecasting model which takes care of the forecasting accuracy and time cost at the same time is proposed and developed in Section 4 .
In this section , we propose a new forecasting model that systematically sets the appropriate factors of ( i ) the repeating time P , and ( ii ) the hidden neuron number N. The features of the ELM forecasting model obtained in the previous section are employed here in order to find the right parameters .
In the first place , we set P = 100 , which is reasonable for averaging and trying to find the appropriate N ( Sun et al. , 2007 ) .
When the time predicted exceeds the time limit T , the proper P is found by making use of ( 1 ) .
The time limit T is a governing limit so that if the forecasting time t > T , statistical model is used as a part of our fast forecasting model , The following steps present the model in details , and please refer to Fig 10 for the formal flowchart of the fast forecasting model : 1 .
Initialization the time limit T is set ( 5 min is used in our experiments ) , repeating time of P = 100 , number of hidden neurons N = 2 are set .
Do training and validation with the neuron number N. Validation error En and runtime t are obtained in this step , these data will be used in fitting of En–N curve as in Fig 4 , and t–N curve as in Fig 2 .
The overfitting occurs if the validation error En is greater than the previous error Ep ( Ep is the error of last run ) .
If no overfitting occurs at N , prediction of the best N′ at which the overfitting will occur is conducted by fitting of the En–N curve , and also the prediction of time cost t at N′ .
If time cost t is within the limit , i.e. , t < T , perform the final forecasting of ELME with parameters P and N found in the model .
If time cost t exceeds the time limit , statistical forecasting model is used instead .
The flowchart of the fast forecasting model Fig 10 .
The flowchart of the fast forecasting model .
With this model , the forecasting can be finished within the time constraint with good accuracy .
Experiments for the two sets of fashion products sales data are conducted and the respective results are listed in Tables 5 and 6 .
In Table 5 , the forecasting errors of our fast forecasting model are given by MSE , and the corresponding errors of the statistical model ( polynomial regression is used ) and traditional ANN are also listed .
As we have seen from Table 5 , our fast forecasting model achieves best forecasting among the three models .
Further experiments are conducted with the two datasets and with different time constraints .
The results are given in Table 6 .
The forecasting time limit is deliberately chosen to trigger the event of reducing P , or fallback from the ELME model to the traditional statistical model .
The numbers of ELM runs and statistical runs reveal the real running results following the procedures in Fig 10 .
For dataset 1 , with a time limit of 5 min ( 300 s ) , at the second run of the ELME , validation error En begins to increase , this shows that the overfit has occurred and no prediction of N for overfit is needed , the forecasting result is then based on the ELME model , and the statistical model is not needed ( 0 runs ) .
When the time limit is 0.5 min ( 30 s ) , the first run of ELME ( cost around 0.3 min ) reveals that another run of the ELME with a current P will exceed the time limit , P is then reduced based on ( 1 ) , and the second run of ELME finishes the forecasting within time limit .
With a time limit of 6 s , the first run of ELME , which costs around 18 s , already exceeded the time limit , and the forecasting model would get back to the statistical model which can produce a quick forecasting result .
Results for dataset 2 is similar , but with a difference that the ELME usually runs for 4 times , as it needs three runs to produce overfitting on neural number N , and the last run to produce forecasting result .
The time limit is also exceeded with a short time limit 0.5 min .
Notice that there is another case where the time limit of 2.5 min ( 150 s ) is exceeded , this occurs when the running time of the ELME is very near to the time limit .
As the error of forecasting by ELME is usually not large , the real run time of the IFSFM is usually close to the time limit even if it is exceeded .
These results show that , except for some rare cases when the time limit is too short , the IFSFM can attain the best forecasting result and meets the time constraint at the same time .
Table 5 .
Forecasting errors comparison in MSE .
Forecasting error Dataset 1 Dataset 2 IFSFM Model 0.208 0.0382 Statistical model ( polynomial regression ) 0.246 0.0711 Traditional ANN 0.211 0.0493 Table 6 .
Forecasting Time of Intelligent Fast Forecasting Model ( IFSFM ) .
Dataset Time constraint ( min ) Forecasting time ( min ) ELME runs Statistical run IFSFM ANN Statistical 1 5 0.63 0.38 0.02 2 0 0.5 0.46 2 0 0.1 0.32 1 1 2 5 2.75 0.40 0.02 4 0 2.5 2.75 4 0 2 1.90 4 0 0.5 0.73 1 1
In this paper , ELM is studied for developing a sales forecasting tool for fashion products known as intelligent fast sales forecasting model ( IFSFM ) .
IFSFM can achieve both good forecasting accuracy and fast in time cost at the same time .
The fashion industries often face the challenge of forecasting problems because they must perform forecasting on many SKUs at one time and timely forecasting results are needed .
To alleviate such problem with fast forecasting , we concentrate on modeling the time cost and forecasting error of the model in relation to the crucial model parameters of repeating time and hidden neuron number of the ELM .
We found that the time cost and forecasting error can be predicted with the model we have proposed , and thus the whole forecasting time cost and error are controllable at the early stage of the forecasting , so that we can optimally choose the appropriate values of P and N , or even the forecasting model to be used , to achieve good forecasting accuracy and satisfy the time limit given at the same time .
Experiments with real fashion POS data sets show that this model is efficient in providing good accuracy within the given time limit .
Many statistical models can be used in the fast forecasting model , and this choice certainly influences the final forecasting accuracy , ( the selection of the “ optimal ” statistical model would be a future research direction of this fast forecasting model ) .
With the help of such fast forecasting model , forecasting can be completed within the given time constraint and at the same time the forecasting accuracy is also optimized .
This fast forecasting model is especially useful for the fashion industry where , very often , forecasting tasks conducted over many SKUs are required and prompt and accurate forecasting results are requested .
Future research will be conducted to explore further on how an intelligent fast forecasting model can be developed for time-series forecasting with different targets , such as consumer products and financial indices .