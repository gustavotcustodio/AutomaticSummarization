Providing guidelines for practical implementation of neural networks in near-field sound source localization. Obtained optimal sensors setups. Obtaining optimal network configuration. Obtaining optimal training parameters. Proving effectiveness of feedforward neural network in solving hyperbolic positioning problem under the uncertainties.

0.057509 - Using time difference of arrival (TDOA) is one of the two approaches that utilize time delay for acoustic source localization.
0.076923 - Combining the obtained TDOAs together with geometrical relationships within acoustic components results in a system of hyperbolic equations.
0.054321 - Solving these hyperbolic equations is not a trivial procedure especially in the case of a large number of microphones.
0.051852 - The solution is additionally compounded by uncertainties of different backgrounds.
0.153651 - The paper investigates the performance of neural networks in modelling a hyperbolic positioning problem using a feedforward neural network as a representative.
0.034409 - For experimental purposes, more than 2000 sound files were recorded by 8 spatially disposed microphones, for as many arbitrarily chosen acoustic source positions.
0.014035 - The samples were corrupted by high level correlated noise and reverberation.
0.033333 - Using cross-correlation, with previous signal pre-processing, TDOAs were evaluated for every pair of microphones.
0.152381 - On the basis of the obtained TDOAs and accurate sound source positions, the neural network was trained to perform sound source localization.
0.116508 - The performance was examined using a large number of samples in terms of different acoustic sensors setups, network configurations and training parameters.
0.240988 - The experiment provided useful guidelines for the practical implementation of feedforward neural networks in the near-field acoustic localization.
0.030108 - The procedure does not require substantial knowledge of signal processing and that is why it is suitable for a broad range of users.
0.069926 - Acoustic source localization is the determination of sound source position, relative to some reference frame, by using sound signals.
0.016216 - It is used in diverse military, industrial, scientific, office, and home applications for speech signal processing (Clifford, Rathborn, & Bull, 1981), intelligent living environments (Principi, Droghini, Squartini, Olivetti, & Piazza, 2016), maintenance and structural monitoring systems (Costiner et al., 2014), sonar (Bokhari & Khan, 2012), surveillance systems (Vozáriková, Pleva, Juhár, & Cižmár, 2011) or to locate sources of artillery fire (Calhoun, Showen, Beldock, Manderville, & Dunham, 2012).
0.027778 - The process is performed passively or actively and can take place in liquids, gases and solids.
0.026667 - Passive systems use sound signals coming from acoustic sources to locate them.
0.037333 - On the other hand, active localization is used for locating targets that are not necessarily acoustic sources.
0.026667 - Sound pulses are sent and their echoes, reflected from objects, are used for localization (Bokhari & Khan, 2012).
0.028571 - Many implementations of acoustic localization systems try to imitate flexible and integrated sensory functionality of animals (Nikolić, Kim, & Allen, 2012).
0.013333 - People and animals are able to point at the horizontal direction that sound is coming from using slightly different signals that arrive at each ear (Lin, Xiao-Yan, Xu, & Zhen-Yang, 2015).
0.041667 - For the vertical direction, features of the sound spectrum, produced by a sound reflector (pinna) (Macpherson & Sabin, 2013), are used as the auditory cue.
0.038889 - The localization ability can be established by the process of learning through the repetition of movement.
0.016667 - Biologically inspired, audio localization systems can be realized by only one, two or by array of microphones (Argentieri, Danes, & Soueres, 2015; Belloch, Gonzalez, Vidal, & Cobos, 2015; Seewald, Gonzaga, Veronez, Minotto, & Jung, 2014).
0.027778 - Audio localization can be applied on different scales, which mostly depends on the sound power level.
0.029167 - Localization of lighting phenomena, volcano explosion (Rowell et al., 2014) or aircrafts (Martín, Genescà, Romeu, & Clot, 2016), for instance, is performed from dozens of kilometers, while in the case of small precision mechanisms or material structure investigation localization is performed on the millimeter scale (Grabowski et al., 2016; Tan, Zhu, Su, Wang, Wu & Gu, 2016).
0.035088 - The error is determined by the geometry of the microphone array, accuracy of the microphone setup, uncertainties in microphones locations, lack of synchronization within microphones, inexact propagation delays, bandwidth of emitted pulses, presence of noise sources, numerical round off errors, anisotropy, obstacles in the propagation path and other terms.
0.033333 - Representative methods of acoustic localization are intensity difference, beam forming and using time delay estimation (TDE).
0.096575 - These methods primarily differ in physical variables utilized for the sound source localization.
0.047608 - The first method uses the phenomenon of decreasing the sound source energy as it propagates through the medium (Wu, Wang, Dai, & Tong, 2014).
0.043509 - Beam-forming uses a collection of signals, from the array of microphones, for computing a correlation matrix, which is thereafter used for the determination of sound source direction (Radcliffe, Naguib, & Humphreys, 2014).
0.014035 - The procedure, known as spatial filtering, is based on subspace-theory.
0.060750 - The last approach uses the time delay of arrival (TOA), for the case of far-field localization, or the time difference of arrival (TDOA), in the case of near-field localization, collected at different spatial positions.
0.064000 - Obtained delays are used, together with the geometrical positions between acoustic components, for estimating the emitter position.
0.060049 - Near-field localization is also known as hyperbolic localization because it requires solving hyperbolic equations (Park, Jeon, & Kim, 2014).
0.034667 - The complexity of calculations needed for achieving accurate localization increases dramatically with the size of the sensor array (Belloch et al., 2015; Seewald et al., 2014), yet the problem can be mitigated by doing some of the processing on the sensor platform.
0.042778 - The experiment presented in this paper dealt with near field 3D acoustic localization, motivated by the possibility of locating flying objects, such as insects or drones, in the near field of the microphone array on the basis of acoustic signals they emit.
0.031746 - It was performed under extremely bad conditions reflected in high disturbances and reverberation.
0.076974 - Neural networks were employed for evaluating acoustic source location because of their high speed during exploitation and possibility to model some of the uncertainties in the experimental setup (microphone positions, locations of parabolic reflectors, lack of synchronization within microphones).
0.138725 - The performance of neural networks was investigated using a basic, feedforward, neural network, as a representative, in terms of sensor parameters, network configuration and training parameters.
0.085333 - The results and the optimal solution of the localization problem are discussed and presented at the end.
0.026190 - Sound signals received on two spatially separated audio receivers can be expressed by equations (1) (2) where s0(t) is the signal of emitter, ni(t) and nj(t) are the uncorrelated zero-mean Gaussian noise processes, α is the scaled difference in amplitude between the two received signals.
0.007843 - After discretization, the previous equations take the form (3) (4) where k is the time sample index and l is the correlation lag between the samples.
0.014815 - Time difference of arrival, Δtij, between signals is commonly determined using the cross-correlation function (5) as the argument l that maximizes its value within the range of possible lags (6) where Fs is the sampling frequency and T is the size of the observation window.
0.028718 - A good approximation of the cross-correlation function can be obtained using the inverse discrete Fourier transformation (7) where Rij(f) is the cross-power spectral density (XPSD) (8) Some other TDOA estimation algorithms developed for the purpose of TDOA estimation are phase transform, maximum likelihood estimator, average square difference method, adaptive last mean square filter, etc.
0.062745 - All of them differ in accuracy and computational complexity.
0.036364 - TDOA estimation in real circumstances is always negatively affected by disturbances of various backgrounds.
0.029630 - The quality of signal is measured by the ratio between the original signal amplitude and the amplitude of noise.
0.020513 - It is usually expressed in decibels (9) If SNR falls below a certain threshold all methods become unreliable.
0.030476 - For the case of the cross-correlation function it is about 13 dB and for the phase transform algorithm about −13.5 dB (Dhull, Arya, & Sahu, 2010).
0.034783 - Since computational efforts have limited effect, SNR ratio is improved using different types of filters.
0.034783 - The role of a filter is to suppress the noise while leaving the signal unchanged.
0.023392 - The general equation of infinitive impulse response (IIR) filters, in the time domain, has the following form (10) Unlike finite impulse response (FIR) filters, the future state of IIR filters depends not only on the finite number of previous inputs, but also on the finite number of previous outputs.
0.041481 - The transfer function of IIR filters can be obtained after the z-transformation of Eq (10) (11) A filter design procedure means searching for suitable transfer function coefficients that will provide a filter to meet the specification.
0.053333 - For instance, a typical specification of a low-pass filter consists of the following parameters: • [0, ωp]
0.022222 - Filter design requires considerable knowledge about signal processing and frequency response of both signal and noise.
0.044444 - Fortunately, some filters possess the possibility to adapt the transfer function according to an optimization algorithm.
0.038095 - Implementing adaptive filters does not require a priori knowledge of signal and noise.
0.000000 - Fig 1.
0.047059 - Graphic representation of typical low-pass filter specification parameters.
0.025641 - A sound signal propagates through a non-dispersive isotropic homogeneous medium, with a constant speed, in all directions.
0.033333 - That is why an acoustic source can be considered as the core of a spherical wave.
0.044444 - However, in most practical implementations, the wave front is assumed to be planar.
0.022857 - Although the assumption significantly simplifies mathematical modeling and processing, it stands only if the radius of the wave front is much greater than the distance between microphones.
0.075130 - In other words, the sound source has to be in the far-field of the microphone array.
0.063768 - The result of far-field localization is the direction from which the sound is coming.
0.081712 - An alternative model, developed for the case of near-field implementation, is the spherical model.
0.073333 - It is much more computationally complex but gives the estimation of sound source position.
0.064993 - Moreover, the common case in practical applications is that the signal collected by the microphone array is a mixture of far-field and near-field acoustic sources.
0.074074 - The position of an acoustic source is obtained using geometrical relations between the mobile acoustic source and fixed microphones.
0.040317 - The distances are estimated indirectly by measuring the time it takes for the signal to reach the microphones from the sound source.
0.088889 - Multiplying the obtained times by the velocity of the sound yields unknown distances.
0.046154 - The speed of sound is supposed to be known or its dependency with respect to relevant physical parameters.
0.038889 - As shown before (6), TDOA has to be calculated for the whole range of possible delays.
0.012121 - Harmonic signals behavior repeats periodically so the maximal phase difference is restricted to 2π.
0.034188 - To avoid exceeding the range, the maximal allowed difference of distances dij between each pair of microphones should be (12) where λmin is the minimal wave length present in the signal.
0.047619 - Fortunately, the majority of real signals are quasi-periodic, which means they never repeat periodically in the exactly same manner.
0.048485 - In such cases, the range of possible lags is limited only by computational complexity.
0.026016 - Suppose that Mi(xi, yi, zi), i ∈ [1, N], are the spatial coordinates of N arbitrary disposed microphones and S0(x0, y0, z0) are the spatial coordinates of an acoustic source (Fig 2).
0.021429 - If the source emits a sound pulse at a time t0, then the time of the pulse arrival at the microphone is given by (13) where ti is the time of arrival at the microphone Mi, ri is the distance between the sound emitter and the microphone Mi.
0.044444 - The speed of sound c depends on different physical conditions (temperature, height, humidity…).
0.050000 - A simple formula for the case of dry air, commonly used for the approximation of sound velocity in practical applications, is (14) where θ is the temperature of air in degrees Celsius (°C).
0.000000 - Fig 2.
0.151049 - Near field sound source localization.
0.029043 - Using Eq (13) the time delay of arrival between two microphones can be evaluated by (15) The expression (15), indeed, describes a three-dimensional hyperbola because, in terms of Cartesian coordinates, vectors ri can be computed by (16) where xi, yi, zi and x0, y0, z0 are the Cartesian coordinates of the microphone Mi and the sound source respectively (Kundu, 2014).
0.000000 - Combining Eqs.
0.036782 - (15) and (16) for all pairs of microphones gives a system of equations (17) where n is the number of microphones.
0.020202 - Since there are 3 unknown variables, x0, y0 and z0, three simultaneous equations, such as Eq (17), are necessary to determine the sound emitter position.
0.064368 - Actually, even three equations are not enough for obtaining a unique solution because of the square root terms (Bucher & Misra, 2002).
0.054545 - Adding another microphone provides one more equation and a unique solution of the problem.
0.049123 - Four equations also additionally improve the accuracy of the estimated position.
0.038095 - More microphones provide a better elimination of, necessarily present, disturbances of different backgrounds.
0.033333 - That is why even more than 4 microphones are used in practice.
0.048485 - Because there are more equations than unknown variables (n > 3), system of Eq (17) is overdetermined and finding its solution becomes a nonlinear optimization problem.
0.040000 - It is obtained approximately by involving a certain quality criterion (objective function).
0.049123 - The most often used in practice is the least squares criterion.
0.089557 - Several algorithms of different complexity have been proposed for solving hyperbolic localization problems.
0.022222 - The Newton–Kantorovich method (Polyak, 2006) provides accurate and robust results but has high computational complexity.
0.036364 - It can be partially overcome by using some of the gradient methods (Nemirovski, 1999).
0.032000 - Unlike the Newton–Kantorovich method, the gradient methods do not require the computation of the inverse matrix.
0.032323 - However, they also require a good initial guess, while convergence is not guaranteed in the presence of high disturbances and is still quite intensive computationally.
0.040404 - The Levenberg–Marquardt algorithm (Chen, 2016), which is used for the comparison in this paper, combines advantages of the gradient and the Gauss–Newton algorithm.
0.035556 - For the case when the number of arbitrarily placed microphones is equal to the number of equations, the Fang's method (Fang, 1990) provides an exact solution and is computationally less intensive in comparison with iterative methods.
0.052726 - Chan's (Dehkordi, 2011) solution is valid for both far-field and near-field localization.
0.047619 - It is better than Fang's in the sense that it provides a possibility of improving accuracy using redundant measurement.
0.031746 - However, it is not suitable when the level of disturbances is relatively high.
0.073939 - Feewforward neural networks (FFN) are a nonlinear model easy to understand and operate with.
0.046377 - These networks have the ability to detect complex nonlinear relationships between dependent and independent variables.
0.059259 - Since network nodes contain primitive functions, their behavior is determined by composition rules, implicitly placed in the interconnection pattern.
0.025806 - The adjustment of the pattern is done not by an explicit programming but through an adaptive method called the learning algorithm (Gonzalez, 1996).
0.014035 - The learning algorithm may be slow but the convergence is certain.
0.026667 - Because of their massive parallelism, FFN are computationally efficient especially during exploitation.
0.079167 - The worst property of neural networks is the “black box” learning approach which does not provide any interpretation of the relations between input and output data.
0.071746 - The operating region is limited to the measured region because neural networks are incapable of extrapolating the results outside the measured region.
0.079509 - There are plenty of applications of neural networks for sound source localizations (Chlada, Prevorovsky, Blahacek, 2010; Kalafat & Sause, 2015; Murray, Erwin, & Wermter, 2009; Sidibe, Druaux, Lefebvre, Maze, & Léon, 2016; Zhang, Liu, Zhang, Zhang, & Gu, 2016).
0.102725 - In this experiment, feedforward neural networks were chosen because of their speed of signal processing during exploitation.
0.042105 - It was essential because of the intention to locate fast objects.
0.048485 - The learning process intensity can be avoided easily in the case of mass production.
0.088061 - In addition, neural networks provide simple physical realizations made by adjusting electronic components without processors.
0.028070 - The physical approach provides better parallelism and consequently faster signal processing.
0.099897 - Artificial neural networks consist of simple processing elements called artificial neurons.
0.012698 - A neuron has two functional parts (Fig 3), the integrating and activation function.
0.014035 - The integrating function reduces n input arguments to a single value.
0.010256 - The most used integrating function is the sum function (18) although it can be some other, like multiplication.
0.028571 - The second part, the activation function, produces the output of the node taking the result of integrating as its argument.
0.056116 - Neural networks with purely linear activation functions are able to approximate linear problems (Kalafat & Sause, 2015) and the most common activation function for approximating nonlinear problems is the sigmoid (hyperbolic tangent) function (19) Fig 3.
0.000000 - Processing element (artificial neuron).
0.012698 - It squashes the output domain to a narrow range, f(x) ∈ [0, 1].
0.029630 - The derivative of the sigmoid function can be determined by a simple formula (20) which significantly reduces computational complexity.
0.112626 - In the case of neural networks, generally, any symmetrical activation function has some advantages (Gonzalez, 1996).
0.012121 - The sigmoid function (19) tends to the step function as constant c approaches infinity.
0.019753 - The step function has only two possible values, one and zero, so it is used for performing logical operations.
0.021053 - The alternative activation functions are symmetric sigmoid, hyperbolic, tangent, ramp, etc.
0.073249 - The topology of an artificial neural network is determined by the number of processing elements (artificial neurons) and the way they are connected (Fig 4).
0.038095 - Artificial neurons are organized in layers.
0.044444 - The minimal configuration consists of one input and one output layer of neurons.
0.029630 - Between these may be an arbitrary number of hidden layers.
0.090741 - A neural network is recurrent if it contains any cycle.
0.077289 - If all outputs flow forward to the next layer, without any cycling, the network is called a feedforward neural network.
0.073333 - The functionality of an unweighted network is determined only by its topology.
0.058667 - On the other hand, each connection of a weighted network is multiplied by an appropriate weighting factor.
0.055152 - The factor determines the effect of each input on the final decision (Fig 3).
0.083951 - The global function of a weighted network is achieved by the topology and by the optimal combination of weights.
0.000000 - Fig 4.
0.185714 - Topology of a feedforward neural network.
0.105495 - Searching for suitable weighting factors that provide the desired function of an artificial neural network is called training or learning.
0.016667 - The learning process can be supervised or unsupervised.
0.029630 - The goal of supervised learning is to approximate an unknown mapping function between the known input and output data.
0.031746 - Unsupervised learning is performed on the basis of input and no output data.
0.028571 - The goal is to model the underlying structure or distribution in the data in order to draw conclusions about them.
0.020000 - Acoustic localization requires a direct relation of input dataset to a corresponding set of output data and that is why only supervised learning methods are suitable for this purpose (Kalafat & Sause, 2015).
0.025000 - The function is given by a set of input–output pairs, , where Xi are the n-dimensional and Tj are the m-dimensional vectors.
0.027451 - The combination of weights that minimizes the error between the realized and the given data set is considered to be a solution of the learning problem.
0.049123 - The most dominant learning algorithm, in practice, is the backpropagation algorithm.
0.057971 - It searches for the minimal error, in weighting space, using the method of gradient descent.
0.025806 - Since the method requires computation of the gradient, at each iteration step, continuity and differentiability of the error function have to be guaranteed.
0.057143 - The procedure is performed in two phases starting with arbitrarily chosen network weights.
0.077193 - In the first, feedforward, phase inputs are fed into the network.
0.011594 - The primitive functions and their derivatives are calculated at each node and derivatives are stored.
0.043411 - For the common case of sum integration and sigmoid activation function the output of a single neuron is given by (21) The network produces the output vector Oi generally different from the target vector Ti.
0.038889 - The error is defined by (22) where Oi is the output vector and Ti is the target vector and the gradient of the error by the equation (23) where l is the total number of weights in the whole network.
0.057778 - In the second, back propagation, phase of the algorithm, constant 1 is fed into the outputs and the network is run backwards.
0.032000 - The cumulative result of the backward computation up to the current node is called the backpropagated error.
0.018182 - If the backpropagated error at the jth node is denoted by δj, a partial derivative of the error can be expressed by the equation (24) where wij is the weight between the ith and jth nodes.
0.034783 - After all of the partial derivatives are computed, the gradient decent is performed by correcting each weight wij by the increment (25) The correction of network weights, by the factor Δwij, is the final step of the iteration.
0.048485 - A single iteration of the training algorithm is otherwise known as a training epoch.
0.033611 - The procedure is repeated until the error function (22) falls under the tasked error or the number of epochs reaches the maximal allowed one.
0.028070 - The learning constant γ is chosen experimentally to accelerate training the algorithm.
0.039216 - Another possibility of acceleration is introducing a momentum term in the correction formula (25) (26) where is the previous correction direction and α is the momentum factor.
0.035556 - Such an approach, called the backpropagation with momentum, provides a kind of inertia that helps avoiding excessive oscillation of the error function.
0.077193 - A sketch of the experimental setup is presented in Fig 5.
0.023810 - A source of mechanical vibrations is assembled from a table tennis ball and a cell phone vibration motor inside it.
0.019512 - The cell phone motor beats the walls of the ball emitting a quasi-periodic audio signal that resembles the fluttering of the wings of a beetle, a large fly or some other insect.
0.022222 - The source moved within a 1.6 m edge long cubic space hanged on three strings.
0.030303 - The opposite end of each string was wrapped around a step motor driven pulley.
0.036036 - Three of these pulleys were geometrically placed in vertices of a horizontal, approximately 4.35 m edge long equilateral triangle, above the cubic space, building a simple routing mechanism.
0.056211 - A meaningful winding and unwinding of the pulleys was used for achieving any specified location of the sound source within the cubic space.
0.008081 - Step motors were driven by an Arduino CNC driver and the driver was governed using a PC through the USB connection and the ATmega328P microcontroller.
0.062778 - Geometrical relations of the experimental setup and the realized spatial positions of the sound source were precisely measured using the Total Station Sokkia SET630R.
0.021622 - The instrument allows laser measurement of distances with the accuracy of ±3 mm, at the used range of lengths, memorization and automatic data transfer through an RS-232 port.
0.016667 - After the measured positions were compared with the given positions, the resulting mean absolute error, achieved by the routing mechanism, was approximately 10 mm.
0.037500 - Since the given positions were randomly chosen, the realized positions were adopted as chosen, and used for the computation of theoretical delays between channels.
0.000000 - Fig 5.
0.082051 - Sketch of the experimental setup.
0.019512 - The sound was recorded by 8, low cost, mini spy microphones, designed for CCTV (Closed Circuit Television) security camera and connected to a PC through an 8 channel TerraTec EWS88 MT sound card.
0.039846 - These microphones were spatially disposed at vertices of a 2 m edge length cube, symmetrically around the sound source moving zone, with the purposely chosen tolerance of ±20 mm.
0.000000 - Parabolic reflector dishes were applied on each microphone as audio signal mechanical amplifiers.
0.028070 - Inside of the emitter moving space, 2000 positions were randomly chosen and as many as 8 channel audio files were recorded at the sampling frequency of 44.1 kHz and each for a period of 3 s. At the same manner 126 points were taken along the training path.
0.064320 - The sound source was stopped at each position during the period of acquiring sound samples.
0.061966 - The experiment was performed in a highly reverberant room full of interfering sound sources (fans and step motors).
0.011594 - Therefore, the recorded audio samples were corrupted by the high level correlated noise and reverberation.
0.029847 - SNR ratio was evaluated to −20 dB To cancel the negative effect of corruption to TDOA estimation accuracy, signals were filtered by a suitable second order IIR filter.
0.031746 - At the beginning the filter coefficients were chosen randomly and then tuned up through iterative steps of a genetic algorithm in order to minimize the mean absolute TDOA estimation error over the recorded samples.
0.030000 - The error was calculated as a difference between TDOA estimated using the filtered signal and the theoretical TDOA calculated on the basis of speed of sound and geometrical positions of acoustic components.
0.058824 - After optimization the filter took a form of a band-pass filter with the transfer function (27) and the frequency response as shown in Fig 6.
0.056061 - The filter fitted only the specific sound source and the environmental conditions used in the experiment.
0.038889 - Except optimal filter coefficients, the genetic algorithm was employed to determine some other TDOA estimation parameters, such as the maximal allowed lag, and to decide on implementing the absolute operator that also showed a positive effect on TDOA estimation accuracy.
0.000000 - Fig 6.
0.090909 - Frequency response of the optimal, second order, digital filter obtained by the genetic algorithm.
0.051659 - TDOA estimation was performed using the cross-correlation function with the maximal allowed lag for every of 28 possible combinations, with 8 channels, and for every sound source position realized in the experiment (2126 recorded files).
0.062745 - It resulted in the matrix of 2126 × 28 elements.
0.027922 - Comparing the matrix with the theoretical delays, computed on the basis of real sound source positions, gave a mean absolute error of approximately 0.13 ms. For the sound velocity of 334.33 m/s, related to the temperature of 5°C that rules during the experiment (14), the mean absolute error corresponded to the length of 45 mm.
0.066667 - This is quite satisfactory since the size of the acoustic source in the experimental setup was 40 mm.
0.107359 - Acoustic localization was performed on the basis of the obtained TDOAs using a feedforward neural network with the sigmoid activation function and the backpropagation algorithm with momentum.
0.045977 - The network was realized and the backpropagation algorithm was performed entirely on the basis of equations from the two previous chapters.
0.055556 - The number of neurons in the input layer was determined by the number of employed microphones.
0.035556 - In order to achieve the best accuracy, all redundant pairs that correspond to the certain number of microphones were employed as inputs.
0.050794 - 8 microphones, for example, gave 28 different TDOAs and as many network inputs.
0.037333 - The number of outputs was always 3 because the spatial position was determined by 3 independent coordinates.
0.030769 - Although orthogonal coordinates were used in the experiment, other referent systems can be applied in the same manner.
0.086081 - The rest of the network configuration (number of hidden layers and artificial neurons in them) was a subject of examination.
0.135036 - The performance of the feedforward neural network in acoustic localization was examined, at first, with respect to the disposition of microphones.
0.048000 - The influence of disposition on the localization error was evident particularly for the case of 4 microphones.
0.029630 - Since 8 microphones were fixed in the vertices of a cubic space, a different disposition of 4 microphones was achieved by turning half of them on or off.
0.078019 - The results were obtained by a neural network with 10 neurons in a single hidden layer, on the basis of 1866 training points, using the learning rate of 0.7, the momentum factor of 0.9 and 1600 training epochs.
0.026087 - The training error was defined as a mean absolute difference between realized (actual) and estimated positions of training points, and the testing error was defined as a mean absolute difference between actual and estimated positions of testing points.
0.034409 - After measurements were made for plenty of different dispositions, three different groups were segregated on the basis of the error values (Table 1).
0.056396 - Since the network performance was previously carefully examined with respect to the training parameters, a high ratio between testing and training errors must be a consequence of uncertainties and nonlinearities that were not well approximated by neural network and the high deviation of TDOAs caused by unfavorable experimental conditions.
0.041026 - The maximal error was for the microphones placed in the slantwise plane that obliquely intersected the cubic space.
0.044444 - Slightly lower values of training and testing errors were corresponding to the disposition of microphones in the horizontal and vertical plane, along the side of the cubic space.
0.064000 - Several times lower errors were obtained for any disposition where microphones were placed out of a plane.
0.041379 - It is logical to assume that the standard linear disposition of microphones would have extremely unfavorable effect on the localization error.
0.070175 - What follows from the preliminary results is that microphones have to be deployed equally in all three spatial directions in order to achieve optimal accuracy of the near field 3D acoustic localization.
0.000000 - Table 1.
0.044444 - Mean absolute localization errors with respect to the microphone disposition for the case of 4 microphones.
0.033333 - Microphone disposition Training error (mm) Testing error (mm) Horizontal or vertical plane 53 305 Slantwise plane 64 329 Out of plane 3 72 The dependency of localization errors with respect to the number of microphones is presented in Fig 7.
0.077419 - The results were obtained with the same parameters as in the previous case using the most favorable disposition for each number of microphones.
0.032000 - They show a natural tendency of decreasing both errors with respect to the increasing number of microphones.
0.017778 - On the other hand, the number of TDOAs and thus processing time dramatically increases with a higher number of microphones according to the square equation (28) where n is the number of possible TDOAs using M microphones.
0.074074 - The optimal number of microphones for acoustic localization has to be a compromise between accuracy and the allowed complexity.
0.000000 - Fig 7.
0.042105 - Mean absolute localization errors with respect to the number of microphones.
0.110850 - The following subject of investigation was the network configuration.
0.041026 - According to the general rule, an optimal numbers of neurons and layers are achieved when the model quality does not change significantly by adding additional neurons and layers (Kalafat & Sause, 2015).
0.046667 - The quality is measured by the value of training and testing errors.
0.076467 - At the beginning, the neural network was supposed to be with a single hidden layer.
0.038596 - The graphs in Fig 8 represent the resulting mean absolute training error and the mean absolute testing error with respect to the number of neurons in a single hidden layer.
0.076190 - The results were obtained with 8 microphones, 5000 training epochs and the same remaining parameters as in the previous case.
0.066667 - It is obvious that the optimal number of neurons for the case of a single hidden layer is approximately 10.
0.035088 - The next step was to involve another hidden layer of neurons.
0.037500 - The dependency of the mean absolute testing error with respect to the number of neurons in two hidden layers is presented in Fig 9.
0.040000 - The corresponding graph for the training error had a very similar shape.
0.045455 - The two layer configuration did not show any advantage in comparison with the single layer configuration that had the same number of hidden neurons, so the single layer configuration with 10 neurons was adopted as optimal.
0.000000 - Fig 8.
0.053333 - Mean absolute localization errors with respect to the number of artificial neurons in a single hidden layer.
0.000000 - Fig 9.
0.052174 - Mean absolute testing error with respect to the number of neurons in two hidden layers.
0.053968 - The graphs in Fig 10, obtained with the same parameters as in the previous case, represent the mean absolute training and the mean absolute testing error with respect to the number of training points.
0.066667 - The optimal number of training points is generally dependent on the number of neurons and the complexity of the system.
0.049383 - The higher density of points provides better accuracy of the approximation but increases computational complexity of the training algorithm.
0.074074 - It has been proven, in practice, that at least twice as many training points are required as the number of weights present in the network (Kalafat & Sause, 2015).
0.058667 - For the current configuration, with the total number of 310 weights, it is approximately 600 training points.
0.088889 - The testing error in Fig 10 confirms that 600 training points provide the optimal accuracy of the training algorithm.
0.046377 - Further increasing the density of training points gave no significant improvement of the training error.
0.077778 - On the basis of the graph, 500 was adopted as the optimal number of training points.
0.059770 - The training error slightly increases as the number of training points increases since the network cannot approximate all measurement errors well.
0.000000 - Fig 10.
0.053333 - Mean absolute localization errors with respect to the number of training points.
0.071795 - At the end, the performance of the network was examined with respect to the parameters of backpropagation algorithm.
0.037500 - When the tasked error was not known a priori, the only criterion for stopping the algorithm was the maximal allowed number of training epochs.
0.040000 - It can be limited, generally, by the duration of the training process, but for the purpose of acoustic localization this criterion can be neglected in compilation with other steps that have to be taken (assembly of the equipment, programming, measuring TDOAs, etc.).
0.023704 - In the absence of other criteria, the maximal allowed number of epochs can be determined, similar as the number of neurons, as the number at which the model quality does not change significantly by adding new epochs.
0.072727 - In practice, the training error starts to oscillate after a certain number of epochs.
0.027778 - The maximal allowed number of epochs can be picked as slightly lower to prevent such behavior.
0.053333 - The dependency of localization errors with respect to the total number of training epochs is given, in exponential scale, in Fig 11.
0.080000 - It is obtained with 500 training points and the same remaining parameters.
0.035897 - The graphs show that both errors decrease with respect to the number of training epochs until approximately 4000.
0.053333 - Further continuation of training may lead to the oscillation of the error.
0.000000 - Fig 11.
0.053333 - Mean absolute localization errors with respect to the number of training epochs.
0.014815 - The learning rate and the momentum factor are determined experimentally.
0.048485 - Higher values of learning rate speed up the training algorithm but decrease the accuracy.
0.021053 - The problem can be mitigated by introducing an appropriate momentum factor.
0.072381 - Fig 12. represents the result of searching for the optimal learning rate and momentum factor in two dimensional space using the training error as the searching criterion.
0.082051 - The results were obtained after 4000 training epochs and the same remaining parameters as in the previous case.
0.045977 - The graph suggests that the optimal values are 0.7 for the learning rate and 0.9 for the momentum factor.
0.000000 - Fig 12.
0.000000 - Mean absolute testing error with respect to learning rate and momentum factor.
0.074074 - The final results of localization are presented in Fig 13.
0.044444 - The smooth (red) line is the actual path of the acoustic source along 126 successive points.
0.161905 - The blue one is the result of the estimation using 8 microphones and the feedforward neural network with, experimentally obtained, optimal parameters.
0.031373 - The mean absolute testing error was approximately 35.7 mm which is more than satisfactory because the diameter of the utilized acoustic source was 40 mm.
0.046377 - The result of localization by the Levenberg–Marquardt algorithm is presented with the green line.
0.069686 - It is obtained using the same data set and the termination tolerance of 10−4 m. The resulting mean absolute testing error was approximately 36.3 mm which is almost the same as in the case of the neural network.
0.012698 - On the other hand, the processing time was more than 104 times longer.
0.000000 - Fig 13.
0.050000 - Actual path and estimated paths of acoustic source.
0.055172 - (For interpretation of the references to color in the text, the reader is referred to the web version of this article.)
0.236944 - The experiment proved the effectiveness of the feedforward neural network in solving hyperbolic positioning problems under the uncertainties of microphone positions and obtained TDOAs.
0.112718 - The performance was examined and analyzed using a large number of sound samples, for different sensors setups, different network configurations and training parameters.
0.071429 - The analysis showed that the network compensated successfully for some of the experimental uncertainties (microphone positions, locations of parabolic reflectors).
0.068966 - The resulted positions had even better accuracy than the obtained TDOAs in spite of using a relatively small number of microphones.
0.277350 - The paper provides useful guidelines for practical implementation of neural networks in the near-field sound source localization.
0.023810 - The procedure can be performed using low-cost equipment, available software and does not require substantial knowledge of signal processing.

[Frase 11] The experiment provided useful guidelines for the practical implementation of feedforward neural networks in the near-field acoustic localization.
[Frase 5] The paper investigates the performance of neural networks in modelling a hyperbolic positioning problem using a feedforward neural network as a representative.
[Frase 10] The performance was examined using a large number of samples in terms of different acoustic sensors setups, network configurations and training parameters.
[Frase 9] On the basis of the obtained TDOAs and accurate sound source positions, the neural network was trained to perform sound source localization.
