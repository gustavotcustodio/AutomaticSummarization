RST-PSO- BPNN algorithm based on computational intelligence techniques is presented. RST-PSO- BPNN can solve the multifactor highway passenger volume prediction problem. PSO can optimize the structure and parameters of the back propagation neural network. RST is used to act as the pre-treatment cell of ANN and mine knowledge.

0.253005 - A novel hybrid optimization algorithm combining computational intelligence techniques is presented to solve the multifactor highway passenger volume prediction problem.
0.078026 - In this paper, we can get and discretize a reduced decision table, which implies that the number of evaluation criteria such as travel quantity, fixed-asset investment, railway mileage, and waterway passenger volume are reduced with no information loss through rough set theory (RST) method.
0.163273 - Particle swarm optimization (PSO) algorithm based on the random global optimization is inducted into the network training.
0.165108 - The PSO algorithm is used for glancing study in order to confirm the initial values, and then the back propagation neural network (BPNN) is used for given accuracy to found the PSO-BPNN model.
0.120193 - And this reduced information is used to form a classification rule set, which is regarded as an appropriate input parameter to training PSO-BPNN model.
0.252710 - The RST-PSO-BPNN model is obtained to forecast highway passenger volume.
0.107201 - The rules developed by RST analysis show the best prediction accuracy if a case matches any one of the rules.
0.131215 - The keystone of this hybrid optimization algorithm is using rules developed by RST for an object that matches any one of the rules and the PSO-BPNN model for one that does not match any of them.
0.086957 - The effectiveness of our optimization algorithm was verified by experiments comparing the traditional gray model method.
0.125199 - For the experiment, highway passenger volumes of China during the period 1995–2009 were selected, and for the validation, the novel hybrid optimization algorithm is reliable.
0.063489 - Econometric demand models have been used for many years to provide important behavioral insights into the highway passenger businesses in many the total increase in journeys (Yin, Wang, Xu, et al., 2002).
0.080904 - Correct prediction is the basis for a scientific decision; highway passenger volume prediction (HPVP) is a scientific analysis to make scientific judgments for HPVP according to the existing highway passenger volume data in the future (Tsai, Lee, & Wei, 2009).
0.053493 - With the sustained, rapid and healthy growth of the economy and the gradual improvement of the denizen income and living standards, changing of consumption fashion and conversion of consumption concept, passengers have an ever-increasing demand for the highway transportation, which is also facing more and more severe competition in the transportation market (Khashei & Bijari, 2010; Pai & Hong, 2005).
0.052493 - Therefore, correct HPVP will be of great significance for investment structure, optimization allocation of funds and management decision, etc.
0.110852 - In the artificial intelligence domain, computational intelligence (CI) is a fairly new category of algorithms, including evolutionary computing, fuzzy computing, rough set, artificial neural network and granular computing.
0.055914 - Wide applications of these algorithms have proven that they are very useful in practice to solve real world problems when deterministic solutions are hard to obtain.
0.034143 - So many different prediction model prototypes have been applied, such as exponential smoothing method, non-parametric regression, artificial neural network, linear recursive method, expert experience predicting, gray prediction model and rough set theory, and combination prediction method (Inuiguchi, 2006; Khosravi, Nahavandi, & Creighton, 2010; Li & Wang, 2004).
0.091429 - Although comparative studies and selection models have discussed the issue of models selection, it is still controversial to say which model prototype can globally obtain the best predictive performance among alternatives.
0.082273 - The common advantage of rough set theory (RST) and artificial neural network (ANN) is that they do not need any additional information about data like probability in statistics or grade of membership in fuzzy-set theory (Bazan, Skowron, & Synak, 1994).
0.056140 - RST has proved to be very effective in many practical applications.
0.089385 - However, in RST, the deterministic mechanism for the description of an error is very simple (Craven & Shavlik, 1997).Therefore, the rules generated by RST are often unstable and have low classification accuracies.
0.050633 - So RST cannot forecast for high accuracy.
0.093458 - ANN is considered the most powerful classifier for low classification-error rates and robustness.
0.033670 - But ANN has two obvious shortcomings when applied to large data problems.
0.125415 - The knowledge of ANN is buried in their structures and weights (Lu, Setiono, & Liu, 1996; Swiniarski & Hargis, 2001).
0.131586 - Particle swarm optimization (PSO) algorithm is a global optimization evolutionary algorithm; it can optimize all kinds of complex problems.
0.169101 - So the combination of RST, PSO and ANN is very natural for their complementary features.
0.178176 - One typical approach is to use the PSO approach to optimize the topology structure and parameters of ANN for constructing the PSO-ANN model.
0.126150 - RST approach is used as a pre-processing tool for the PSO-ANN model (Ahn, Cho, & Kim, 2000; Pawlak, 1982; Wang & Ziarko, 1985).
0.099738 - RST provides useful techniques to reduce irrelevant and redundant attributes from a large database with a lot of attributes.
0.074766 - ANN has the ability to approach any complex function and possesses a good robustness.
0.129734 - Therefore, this study takes the advantages of the inductive conclusion and adopts the combining RST, PSO and ANN (RST-PSO-ANN) to develop the HPVP model.
0.075055 - The effectiveness of proposed novel hybrid prediction method was verified with experiments that compared the traditional gray model method and proposed the hybrid prediction method.
0.077670 - The experiment result shows that prediction value and actual value are very proximate.
0.000000 - This method has instructional significance for HPVP.
0.057395 - Rough set theory introduced by Pawlak in 1982 is a mathematical tool to deal with vagueness, incompletion and uncertainty of information (Zhang, Wu, & Liang, 2001).
0.119141 - The philosophy of the method is based on the assumption that with every object some information (data, knowledge) can be associated.
0.081500 - Objects characterized by the same information are indiscernible in view of the available information.
0.069565 - The indiscernibility relation generated in this way is the mathematical basis for the rough set theory.
0.044310 - Information system An information system can be seen as a four-tuple S = {U, Q, V, f}, where U is a finite set of objects, called the universe, Q is a finite set of attributes, V = UaϵQ, Va is a domain of attribute a, and f: U × Q → V is a total function such that f(x, a) ϵ Va, for every a ϵ Q, x ϵ U, called an information function.
0.051530 - In classification problems, an information system is also seen as a decision table assuming that and , where C is a set of condition attributes and D is a set of decision attributes (Xie, Li, & Zhou, 2002; Zhang & Qiu, 2006).
0.042872 - Indiscernibilify relation Let S = {U, Q, V, f} be an information system: every and generates a indiscernibility relation IND(P) on U, which is defined as follows (Sun, Yuan, Yu, et al., 2007): Obviously, IND(P) is an equivalence relation for any P. Equivalence classes of IND(P) are called P-elementary sets in S. If P = Q, the Q-elementary set is called atoms.
0.069930 - The family of all equivalence classes of relation IND(P) on U is denoted by U|IND(P), or in short, U|P.
0.065728 - Approximation of sets Let and .
0.032668 - The P-lower approximation of X (denoted by P_X) and the P-upper approximation of X (denoted by P−X) are defined in the following expressions (Kodogiannis & Anagnostakis, 2002; Wang, 2003; Wang, Wei, Zhang et al., 2007; Zhao, 2005): (1) (2) P_(X) is the set of all objects from U which can be certainly classified as elements of X employing the set of attributes P. P−(X) is the set of all objects of U which can possibly be elements of X using the set of attribute P. The P-boundary (doubtful region) of set X is defined as: (3) The set Bnp(X) is the set of objects which cannot be certainly classified to X using the set of attributes P only.
0.036036 - Decision rules derived from a decision table can be used for recommendations concerning new objects.
0.097938 - Specifically, matching its description to one of the decision rules can support the classification of a new object.
0.053170 - With every set , we can associate an accuracy of approximation of set X and P in S, or in short, accuracy of X, defined as: (4) 2.4.
0.036304 - Reduction of attributes Let S = {U, Q, V, f}, the reduction of condition attribute C means a nonempty subset satisfied the following condition (Zhang, Xiao, & Wang, 2005): (1) IND(P) = IND(C) (2) There is no subset of , which satisfied IND(P’) = IND(C) The process of finding a smaller set of attributes than the original one with the same classification capability as the original set is called attribute reduction.
0.119944 - Attribute reduction is one of the most important concepts in RST.
0.083857 - A reduction is the essential part of an information system (related to a subset of attributes) which can discern all objects discernible by the original information system.
0.075472 - Core is the intersection of all reductions, denoted as: CORE (C) =∩ RED(C), where RED(C) is the reduction of S (Rady, Kozae, & Abd El-Monsef, 2004).
0.044150 - Given S, condition attributes C and decision attributes , for a given set of condition attributes ; we can define a positive region (Dimitras, Slowinski, & Susmaga, 1999).
0.057143 - The positive region POSP contains all objects in U, which can be classified without an error into distinct classes defined by IND(D) based only on information in the IND(P).
0.043614 - Degree of dependency Another important issue in data analysis is discovering dependencies between attributes.
0.042584 - Let an information system S = {U, Q, V, f}, Q = CUD and the dependability between D and C is defined (Ahn, Cho, & Kim, 2000; Zhu, Chen, Geng, & Liu, 2008): (5) If k = 1 we say that D depends totally on C, and if k < 1, we say that D depends partially (in a degree k) on C. The coefficient k expresses the ratio of all elements of the universe, which can be properly classified to blocks of the partition U/D, employing attributes C and will be called the degree of the dependency.
0.113821 - The particle swarm optimization (PSO) algorithm is an evolutionary computational algorithm suggested by Kennedy and Eberhart in 1995.
0.123130 - It is a population-based search algorithm based on the simulation of the social behavior of birds within a flock.
0.054963 - The initial intent of the particle swarm concept was to graphically simulate the graceful and unpredictable choreography of a bird flock, with the aim of discovering patterns that govern the ability of birds to fly synchronously, and to suddenly change direction with a regrouping in an optimal formation.
0.080997 - From this initial objective, the concept evolved into a simple and efficient optimization algorithm.
0.074766 - In PSO algorithm, individuals, referred to as particles, are “flown” through hyperdimensional search space.
0.081955 - Changes to the position of particles within the search space are based on the social-psychological tendency of individuals to emulate the success of other individuals.
0.073491 - The changes to a particle within the swarm are therefore influenced by the experience, or knowledge, of its neighbors.
0.084034 - The search behavior of a particle is thus affected by that of other particles within the swarm.
0.064516 - The consequence of modeling this social behavior is that the search process is such that particles stochastically return toward previously successful regions in the search space.
0.072562 - Namely, the velocity (v) and position (x) of each particle will be changed by the particle best value (pB) and global best value (gB).
0.036655 - Kennedy and Spears (1998) suggested the inertia weighing method, and introduced inertia weight (w) into the basic update rule to reduce the particle speed when particle swarms are searching a large area using the inertia weight w. As a result, the particles are forced into a better search area to obtain a better feasible solution using a more efficient method.
0.043185 - The particle’s new velocity and position are updated by the following equation: (6) (7) vij(t + 1), velocities of particle i at iterations j, xij(t + 1), positions of particle i at iterations j.w denotes the inertia weight coefficient of velocity, c1 and c2 denote acceleration coefficient.
0.022409 - r1 and r2 are random numbers uniformly distributed in [0, 1] which denote remembrance ability for study.
0.068754 - Artificial neural network (ANN) is a new artificial intelligence technology with general application and great potential, which is composed of a large number of nerve cells (Hou, Lian, Yao, & Yuan, 2006).
0.047138 - It offers significant support in terms of organizing, classifying, and summarizing data.
0.045802 - It also helps to discern patterns among input data, requires few assumptions, and achieves a high degree of forecasting accuracy.
0.062570 - These characteristics make neural network technology a potentially promising alternative tool for recognition, classification, and forecasting in the area of traffic, in terms of accuracy, adaptability, robustness, effectiveness, and efficiency.
0.051780 - ANN usually consists of an input layer, hidden layer and an output layer.
0.043956 - The neurons are represented by rectangular shape in Fig 1.
0.034783 - An ANN contains very simple and highly interconnected processors called neurons (Xiao, Ye, Zhong, & Sun, 2009).
0.044444 - The neurons represented by rectangular in Fig 1 are connected to each other by weighted links over which signals can pass.
0.094276 - The input layer is represented by circles and behaves as a buffer.
0.043716 - Each neuron receives multiple inputs from other neurons, except the neurons in the input layer, in proportion to their connection weights and then generates a single output in accordance with an activation function.
0.062305 - An activation function can be of a linear or nonlinear form depending on applications.
0.090557 - Training a network consists of adjusting weights of the network using a different learning algorithm.
0.096096 - ANN in this work is trained with the three supervised and one reinforcement learning algorithm.
0.037559 - An ANN structure Fig 1.
0.042328 - An ANN structure.
0.109508 - In this work, the back propagation with momentum (BP) was used to train the samples.
0.090090 - The BP with momentum is the most commonly adopted ANN training algorithm (Liao & Tsao, 2004).
0.119298 - The configuration of BPNN is described as follows in Fig 2.
0.063316 - Here we apply a BP with a hidden layer in which the neural neurons take tan-sigmoid function for transform and (8) A linear function is used in the output layer to transform to get values with a broad range.
0.076190 - And there is the S function of transform function values from 0 to 1, that is to say, (9) where input the interrelated variables of HPVP (Teoh, Cheng, Chu, & Chen, 2008).
0.195039 - BP learning algorithm based on PSO algorithm is used to train the learning algorithm of neural network in this paper.
0.093778 - Structure of BP neural network Fig 2.
0.104479 - Structure of BP neural network.
0.079887 - Fig 2 shows that, firstly, some reductions can be derived from the crude data based on RS and relevant factors can automatically be obtained from historical data.
0.103560 - Then BPNN is trained to learn in order to forecast the traffic demand.
0.067340 - In the past decades, various prediction techniques have been proposed for traffic.
0.014035 - However each method possesses stronger prediction ability for small-scale prediction.
0.051780 - For the large-scale complicated prediction, each method still cannot effectively implement prediction.
0.050891 - In particular, while the related evidences are incomplete or the relevant conclusions are indeterminate, the problems would become more complicated.
0.129496 - RST can effectively illustrate the significance of different attributes in the knowledge expression system and in the reduction of knowledge expression space.
0.063312 - However, it is often helpless when it is used to deal with incomplete data.
0.237250 - In order to simplify ANN structure and improve its anti-interference ability, RST is used to act as the pre-treatment cell of ANN and mine knowledge from the original knowledge base.
0.297983 - PSO algorithm can optimize the structure and parameters of ANN.
0.102102 - Clearly, the synthesized method has better characteristics than single ANN or PSO algorithm or RST.
0.115514 - So we propose a novel computational intelligence hybrid algorithm for prediction of highway passenger volume.
0.205384 - RST is used to mine the rules.
0.088578 - The rules are used as an original knowledge base to directly offer prediction service, when their “confidence” and “support” satisfy a preset criteria.
0.146436 - Neural network, integrated with PSO algorithm weighted fusion algorithm, which is used to forecast the case that cannot be forecasted by RST based mined rules.
0.149673 - This study used the RST-PSO-BPNN, which is characterized by good training performance, a high convergence rate, few repetitive computational steps, improved prediction accuracy, and the ability to handle nonlinear problems.
0.107483 - The parameters of the BP network are optimized by PSO algorithm by steps as follows (Che, 2010): (1) Number of the hidden layer The general problems adopt only one hidden layer and complicated problems adopt two layers.
0.023310 - (2) Number of units in hidden layers To prevent improper setting that would cause a failure to reduce error values or over-learning.
0.068817 - (3) Learning rate (η) To prevent over-learning rate, which would lead to error vibration, the g was between 0.1 and 1 to test the network.
0.067146 - (4) Momentum coefficient (α) To prevent over-learning rate, this would lead to error vibration, α + η = 1.0 is adopted to test the network.
0.064725 - (5) Transfer function This study sets the transfer function to the sigmoid function.
0.084034 - (6) Initialization of particle swarm The m particles are produced according to the study scope and complexity.
0.026936 - Value and velocities in dimension d are set randomly for each particle.
0.068522 - (7) Randomly produce the initial weight and bias of the network The initial network connection weights and initial biases of the hidden layers, as well as output layer are determined by random variables with uniform distribution.
0.079323 - (8) Input training examples and target values Input values and target output values of the training examples input the BP network.
0.053095 - (9) Calculate the output value of the network (10) Repeat from (8) to (9) until the network is converged The above network learning process is generally conducted with a training example at a time until all the training examples are completed, which is called a learning cycle.
0.077193 - Several learning cycles must be repeated until the network is converged.
0.067142 - (11) Calculate the fitness values The fitness values are compared with the proper optimal functional values of the particles, and the particles modify the next particle search speed according to the memory of the individual optimal variable.
0.068185 - (12) Compare the optimal value of the particles (pB) with the global optimal value (gB) If the current fitness value is better than the previous fitness value, the current state is set as pB and the optimal solution of particles is compared with the global solution.
0.070461 - If the current fitness value is better than the global solution, the current state is set as gB.
0.104348 - (13) Update the velocity and position of each particle The update method is shown in Eqs.
0.042328 - (6) and (7).
0.075881 - (14) Meet the termination conditions If the setting number of iteration is not reached, (8) shall be repeated.
0.065268 - For developing a prediction model for HPVP in China, we followed the above model conception, as it is described in the next subsection.
0.087260 - As a universal mapping, multilayer feed forward BPNN, PSO and RST perfectly suit to this concept, and the corresponding comparisons from among the methods are discussed in the sequence (Al-Kandari, Soliman, & El-Hawary, 2004).
0.158281 - Steps of HPVP modeling based on RST-PSO-BPNN has shown in Fig 3.
0.186681 - Modeling of HPVP based on RST-PSO-BPNN Fig 3.
0.205087 - Modeling of HPVP based on RST-PSO-BPNN.
0.043360 - Original knowledge base and conforming system objective In relational database, a knowledge base was formed by original knowledge.
0.056951 - According to the existing research achievements and their disadvantages in the course of research, we will synthetically think over the development environment for the reasonable HPVP in this paper (Carpinteiro Otavio, Reis Agnaldo, & da Silva Alexandre, 2004).
0.081535 - Establishing decision table According to the research objective, it is essential to select reasonable attributes for the establishment of a decision table.
0.000000 - There have many factors for interfering with HPVP (Jin & Chen, 2007).
0.022409 - Thus selected attributes must meet generalization and systematization, applicability and feasibility, comparability and reliability, validity and practicality.
0.079402 - HPVP is influenced by factors such as population total, GDP, fixed-asset investment, highway mileage, railway passenger volume, travel quantity, fixed-asset investment, railway mileage, waterway passenger volume and so on, the modeling of which is difficult through computational methods alone (Wang & Xu, 2007).
0.055056 - There are twenty factors affecting highway passenger volume, shown in Fig 4.
0.035556 - Influence factors of HPVP Fig 4.
0.039801 - Influence factors of HPVP.
0.077473 - We use the method of the regression of principal components analysis to simplify the numerous correlation factors affecting highway passenger volume to the minority non-correlated factor, in order to eliminate multi-colinearity because of excessive variables.
0.044591 - In this paper, Twelve (B, C, E, I, L, M, N, O, Q, R, S, T) of the twenty factors that may affect the HPVP have been selected because they are considered the more important ones.
0.116809 - For the experiment, the highway passenger volume data of China from 1995 to 2009 were selected.
0.056099 - Original data of highway passenger volume in the decision table are shown in Table 1, where condition attributes are B, C, E, I, W = (L + M)/2, N, O, Q, R, S, T which denote the growth ratio (%).
0.034783 - So the final set R = {B, C, E, I, W, N, O, Q, R, S, T}.
0.000000 - Table 1.
0.096650 - Original data of highway passenger volume from 1995 to 2009.
0.000000 - Index Year B(%) C(%) E(%) I(%) W(%) N(%) O(%) Q(%) R(%) S(%) T(%) 1 1995 1.055 6.2 10.2 5.1 3.5 1.1 11.0 12.0 −2.2 18.1 10.4 2 1996 1.042 10.2 9.7 6.9 2.5 3.8 12.7 9.9 −2.3 10.4 5.8 3 1997 1.006 10.1 8.8 4.0 3.4 1.6 8.3 5.7 −5.0 3.4 10.8 4 1998 0.914 12.4 7.8 5.1 4.2 0.0 14.1 7.4 −11.4 3.5 8.2 5 1999 0.818 14.7 7.1 6.6 5.7 17.0 5.2 3.4 −2.5 7.1 10.1 6 2000 0.758 14.6 8.0 4.3 3.8 1.9 9.3 6.5 −3.1 16.2 10.7 7 2001 0.695 5.2 7.3 6.4 21.1 2.0 12.1 5.8 −5.6 12.5 12.1 8 2002 0.645 12.0 8.0 9.1 4.0 2.6 16.1 6.0 −5.6 16.2 13.9 9 2003 0.601 −6.4 9.1 6.7 2.5 1.5 26.7 6.6 −21.1 −0.4 16.0 10 2004 0.587 36.9 9.5 7.3 4.4 1.9 25.8 13.9 3.3 41.1 13.1 11 2005 0.589 12.4 9.9 7.9 78.5 1.3 25.7 4.2 3.6 14.1 17.3 12 2006 0.528 17.9 10.7 8.9 3.3 2.3 24.0 8.7 6.3 15.4 17.0 13 2007 0.517 24.8 11.4 1.9 3.8 1.2 24.8 10.6 9.6 16.3 17.9 14 2008 0.508 12.6 9.0 8.2 4.1 2.2 25.5 7.6 6.0 3.6 17.0 15 2009 0.505 16.4 8.7 9.2 4.5 2.4 30.1 3.6 2.9 19.7 17.4 Note: The data in Table 1 comes from annual China Statistical Yearbook, http://www.stats.gov.cn/tjgb/.
0.076739 - Decision table discretization When the decision table is processed by RST, the attribute values in decision table are described by discretized data.
0.056872 - Continuous attribute discretization is to set some divide-points (breakpoints) in a specific continuous attribute value range and divide the attribute range into some discreted intervals, and use the different symbols or integers to represent each interval’s attribute values.
0.093793 - (Wang & Xu, 2007) A hybrid clustering algorithm – the hierarchical k-means clustering algorithm is used to discretize the continuous data in this paper.
0.084192 - This algorithm combines the advantages of the hierarchical clustering method and k-means clustering method and overcomes the respective disadvantages (Wang & Xu, 2007).
0.061931 - The basic thought is to get some initial information by carrying on the hierarchical clustering method, then refine by using k-means clustering method in order to obtain the high quality clustering results.
0.051780 - The discretization results are obtained by this algorithm calculation, shown in Table 2.
0.000000 - Table 2.
0.096650 - Discretization data of highway passenger volume from 1995 to 2009.
0.000000 - Year B C E I W N O Q R S T 1995 1 1 1 2 2 1 1 3 1 3 2 1996 1 2 2 2 2 1 2 2 1 2 1 1997 1 2 2 1 1 1 1 2 1 3 2 1998 1 2 2 2 2 2 2 2 1 2 1 1999 2 2 2 1 2 2 3 1 1 2 2 2000 2 2 2 1 1 1 1 2 1 3 2 2001 2 2 2 3 2 2 1 2 1 1 2 2002 2 2 2 2 3 1 2 2 1 3 2 2003 2 2 2 3 2 2 1 2 1 1 3 2004 3 3 2 3 2 2 1 3 2 3 3 2005 3 3 2 3 2 2 1 1 2 3 3 2006 3 3 1 3 3 1 2 2 3 3 3 2007 3 3 1 3 1 1 2 2 3 3 3 2008 3 2 2 3 3 1 2 2 3 1 3 2009 3 3 2 3 3 1 2 1 2 3 3 6.4.
0.054054 - Attribute reduction and rule acquisition Attribute reduction removes the unnecessary condition attributes from decision table.
0.078545 - Thus we analyze the obtained reduction condition attribute for decision rules of the decision attribute.
0.097632 - And people always hope to get fewer condition attributes of the reduction result.
0.082121 - Attribute reduction algorithm based on RST mainly includes attribute reduction on discernibility matrix and logic operation, and induction attribute reduction algorithm, etc.
0.097561 - These algorithms do not adequately think over the speciality of knowledge in the data domain and the flexibility.
0.057743 - Hence we may use some heuristic knowledge which will simplify the reduction process by calculating estimation of attribute importance.
0.170086 - An attribute importance reduction algorithm based on RST is used to reduce the attributes (Malcolm & Michael, 2001).
0.067340 - According to attribute reduction algorithm, the attributes in Table 2 are reduced.
0.068027 - Then the core attributes are optimized and the noise and redundant objects are eliminated to get the reduction set, {B, W, N, Q, T}.
0.065440 - That is to say, decision attributes {B, W, N, Q, T} keep the classification ability of attributes {B, C, E, I, W, N, O, Q, R, S, T}.
0.044818 - At the same time, {B, W, N, Q, T} are the necessary attributes for the decision system.
0.051613 - Then we may delete the redundant attributes {C, E, I, O, R, S} in Table 2 and get the reduction attributes {B, W, N, Q, T}.
0.151474 - Compute the output value and Parameters of PSO-BPNN The number of nodes in input layer and output layer are ascertained according to a specific problem.
0.090090 - According to analyzing, we get the number of nodes in the input layer as five.
0.080808 - That is to say, we input the {B, W, N, Q, T}.
0.062016 - The output of each processing unit was computed by (10) Outi is the output of unit I; wij is weight on connection from j to i; xj is input to unit i from unit j; θi is bias on unit i.
0.056912 - We apply a sigmoid transformation to the summation Outi for each unit in a hidden layer, (11) Let k be the number of the output layer, then (12) Tk, target value of the output unit k and , error of the kth output unit.
0.097087 - The final number of nodes in the hidden layer is six by calculation.
0.087912 - The expectant error value is confirmed by repeating the calculation.
0.042042 - We choose expectant error value of sampling which is 0.00001 by repeating comparing training.
0.078431 - The initial weighted values of w1, w2 and matrix & bias values of b1, b2 are random numbers.
0.087912 - And the appropriate value is found out by repeating training.
0.068817 - The other parameter values are set in BP algorithm: ρ = 0.8, ξ = 0.04, η = 1.05, momentum coefficient γ = 0.9 and learning speed value is 0.10.
0.152701 - After training, the condition attribute values of {B, W, N, Q, T} are regarded as the input of RST-PSO-BPNN and the predictions of each goal are obtained.
0.107397 - In allusion to learning samples, we use RST-PSO-BPNN to process repeating network learning and simulation experiment under MATLABR 2009a environment for obtaining learning error curve, shown as Fig 5.
0.180134 - Error curve of RST-PSO-BPNN Fig 5.
0.199801 - Error curve of RST-PSO-BPNN.
0.086331 - Next, we use the statistic function of MATLABR 2009a to get the comparability result between the actual value and the prediction value.
0.068241 - Comparison of prediction errors between prediction result and other methods is shown in Table 3 from 1995 to 2009.
0.057743 - In Table 3, it is very obvious that this prediction result is more precise than the gray model method.
0.182046 - It has been validated that the input variable method of RST-PSO-BPNN is effective.
0.139393 - Therefore, according to the plan of all indexes of ministry of transport of the people’s republic of China, we may use the proposed RST-PSO-BPNN model to forecast highway passenger volume and make informed transportation planning decisions.
0.000000 - Table 3.
0.087542 - Compare between the actual value and prediction value from 1995 to 2009.
0.014188 - Index Year RS-PSO-BPNN Gray model Actual value (Million) Forecasting value (Million) Difference value (Million) Prediction error (%) Prediction error (%) 1 1995 10408.10 10238.62 169.48 1.6553 3.0481 2 1996 11221.10 10502.26 618.84 5.8924 9.2543 3 1997 12045.83 11488.49 557.34 4.8513 6.7523 4 1998 12573.32 13109.05 −435.73 −3.3239 −4.5287 5 1999 12690.04 12110.61 579.43 4.7845 7.5198 6 2000 13473.92 13026.65 445.27 3.4181 6.5281 7 2001 14027.98 13432.17 595.81 4.4357 8.9435 8 2002 14752.57 14434.18 318.39 2.2058 5.8927 9 2003 14643.35 14185.97 457.38 2.2242 6.4538 10 2004 16245.26 16734.72 −489.46 −2.9248 6.5243 11 2005 16973.81 16756.32 217.49 1.2980 7.8762 12 2006 18604.87 18111.33 493.54 2.7250 5.9647 13 2007 20506.80 21094.16 −587.36 −2.7845 −1.8546 14 2008 26821.14 27804.43 −983.29 −3.5365 −6.5683 15 2009 27786.70 26868.39 918.31 3.4179 7.5849
0.136632 - Highway passenger volume prediction is of great significance in investment structure, optimization allocation of funds and management decisions.
0.137895 - In this research, a novel hybrid optimization algorithm (RST-PSO-BPNN) was proposed integrating RST, PSO and BPNN methods.
0.207622 - The RST is used in knowledge reduction to select the suitable input factors; PSO is used to optimize the parameters and structure of BP neural network.
0.226670 - To construct the proper RST-PSO-BPNN model for highway passenger volume prediction, the proposed algorithm based on computational intelligence techniques was first used to train and test the historical highway passenger volume data.
0.125522 - By comparing RST-PSO-BPNN and gray model method, this study confirms that the proposed RST-PSO-BPNN has better stability and solution capacity than the other gray model method.
0.142021 - The results also show that the accuracy of passenger volume prediction for highway can be improved by introducing the PSO into the BP neural network.
0.209274 - The PSO can optimize the structure of the BP neural network and reduce the parameter setting of the BP neural network.
0.187297 - Finally, the new highway passenger volume data was inputted into the RST-PSO-BPNN approach to forecast.
0.282162 - The RST-PSO-BPNN approach is more effective in solving the highway passenger volume prediction problem.

[Frase 101] In order to simplify ANN structure and improve its anti-interference ability, RST is used to act as the pre-treatment cell of ANN and mine knowledge from the original knowledge base.
[Frase 1] A novel hybrid optimization algorithm combining computational intelligence techniques is presented to solve the multifactor highway passenger volume prediction problem.
[Frase 4] The PSO algorithm is used for glancing study in order to confirm the initial values, and then the back propagation neural network (BPNN) is used for given accuracy to found the PSO-BPNN model.
[Frase 195] To construct the proper RST-PSO-BPNN model for highway passenger volume prediction, the proposed algorithm based on computational intelligence techniques was first used to train and test the historical highway passenger volume data.
