Neural network modeling has best correlation to evaluating the effects of coal chemical properties on FSI. Results from ANN showed that nitrogen (dry), carbon (dry), hydrogen (dry), Btu (dry), volatile matter (dry) and fixed carbon (dry) had positive effects on free swelling index, respectively. The negative effects of input parameters were related to moisture, oxygen (dry), ash (dry) and total sulfur (dry), respectively. The fixed carbon was found to have the lowest effect (0.0425) on FSI.

0.190917 - In this research, the effect of chemical properties of coals on coal free swelling index has been studied by artificial neural network methods.
0.085773 - Artificial neural networks (ANNs) method for more than 300 datasets was used for evaluating free swelling index value.
0.066896 - In this investigation, some of input parameters (nearly 10) were used.
0.072351 - For selecting the best model for this study, outputs of models were compared.
0.101809 - A three-layer ANN was found to be optimum with architecture of 12 and 5 neurons in the first and second hidden layer, respectively, and 1 neuron in output layer.
0.052288 - In this work, training and test data’s square correlation coefficients (R2) achieved 0.99 and 0.92, respectively.
0.326240 - Sensitivity analysis shows that, nitrogen (dry), carbon (dry), hydrogen (dry), Btu (dry), volatile matter (dry) and fixed carbon (dry) have positive effects and moisture, oxygen (dry), ash (dry) and total sulfur (dry) have negative effects on FSI.
0.325695 - Finally, the fixed carbon was found to have the lowest effect (0.0425) on FSI.
0.049383 - Coals are organic sedimentary rocks that have their origin from a variety of plant materials and tissues deposited in more or less aquatic locations (Loison, Foch, & Boyer, 1989).
0.063260 - A coal is characterized by a number of chemical, physical, physico-chemical and petrographic properties.
0.129459 - In proximate analysis, moisture, ash, volatile matter and fixed carbon are determined.
0.041648 - Cokeability of coal is an important technological parameter of coals during reducing process in electric furnace method.
0.049899 - This is usually determined by free swelling index.
0.119580 - The simplest test to evaluate whether a coal is suitable for production of coke is the free swelling index test.
0.053945 - This involves heating a small sample of coal in a standardized crucible to around 800 °C (1500 °F).
0.069721 - The free swelling index in British Standards Index (BSI) nomenclature (the crucible swelling index number (CSN) in ISO nomenclature) is a measure of increase in the volume of coal when heated, with the exclusion of air.
0.055096 - This parameter is useful in evaluating coals for coking and combustion.
0.058998 - Coals with a low free swelling index (0–2) are not suitable for coke manufacture.
0.068858 - Coals with high swelling numbers (+8) cannot be used by themselves to produce coke, as the resultant coke is usually weak and will not support the loads imposed within the blast furnace (Thomas, 2002).
0.077455 - When bituminous coals are heated, they develop plastic properties at about 350 °C and as a result exhibit fluidity, swelling, expansion and contraction in volume and after carbonization produce a coherent residue whose strength depends on the rank of the coal.
0.073778 - This plastic property of coals is commonly indicated in free swelling index, Gieseler plastometry, Ruhr dilatometers, Audibert-Amu dilatometer and Gray-king assay tests.
0.061466 - Gieseler plastometer and Ruhr dilatometer are commonly used to study coals’ plastic properties for coke making.
0.087173 - In Gieseler plastometry, the softening temperature, re-solidification temperature and maximum fluidity of coals are determined to predict their coke ability.
0.077258 - In Ruhr dilatometry, the coking capacity, G, is defined by Simonis as (Price & Grandsen, 1987) (1) is used to predict the cokeability of coals.
0.067941 - When the coal particle is heated, its surface becomes plastic while devolatilization occurs from both inside and outside the particle.
0.119057 - Various parameters such as coal type, heating conditions and coal properties affect on free swelling index.
0.153227 - For example, Kidena studied effect of hydrogen/carbon, oxygen/carbon, volatile matter and heating conditions on CSI (Kidena, 2007).
0.193606 - In this work effect of coal chemical properties on swelling index were studied.
0.033126 - After heating for a specified time, or until all volatiles are driven off, a small coke button remains in the crucible.
0.101422 - The cross sectional profile of this coke button compared to a set of standardized profiles determines the free swelling index (Thomas, 2002).
0.092672 - Artificial neural network (ANN) is an empirical modeling tool, which is analogous to the behavior of biological neural structures (Yao, Vuthaluru, Tade, & Djukanovic, 2005).
0.089301 - Neural networks are powerful tools that have the abilities to identify underlying highly complex relationships from input–output data only (Haykin, 1999).
0.078861 - Over the last 10 years, artificial neural networks (ANNs), and, in particular, feed forward artificial neural networks (FANNs), have been extensively studied to present process models, and their use in industry has been rapidly growing (Ungar, Hartman, Keeler, & Martin, 1996).
0.190469 - In this investigation, 10 input parameters such as moisture, volatile matter (dry), fixed carbon (dry), ash (dry), total sulfur (organic and pyretic) (dry), Btu/lb (dry), carbon (dry), hydrogen (dry) nitrogen (dry) as well as oxygen (dry) were used.
0.061885 - In the procedure of ANN modelings the following contents are usually used: 1. choosing the parameters of ANN 2. collecting of data 3. pre-processing of database 4. training of ANN 5. simulation and prediction by using the trained ANN.
0.072351 - In this paper, these stages were used in the developing of the model.
0.069717 - Data set The collected data was divided into training and testing datasets using sorting method to maintain statistical consistency.
0.063694 - Datasets for testing were extracted at regular intervals from the sorted database and the remaining datasets were used for training.
0.085106 - The same datasets were used for all networks to make a comparable analysis of different architecture.
0.036782 - In the present study, more than 300 datasets were collected among which 10% were chosen for testing.
0.037665 - These data were collected from Illinois state coal mines and geological database (http://www.isgs.illinois.edu/maps-data-pub/coal-maps/nonconf_masterfile.xls).
0.240577 - Input parameters In the current study, input parameters include moisture, ash (dry), volatile matter (dry), fixed carbon (dry), total sulfur (dry), Btu (dry), carbon (dry), hydrogen (dry), nitrogen (dry) and oxygen (dry) for predicting the FSI.
0.115211 - The ranges of input variables to FSI prediction for the 300 samples are shown in Table 1.
0.000000 - Table 1.
0.070796 - The ranges of variables in coal samples (as determined).
0.049899 - Coal chemical properties Max Min Mean St. dev.
0.077893 - Moisture (%) 15.94 6.03 10.32 2.21224 Volatile matter, dry (%) 45.10 25.49 36.87 2.458445 Fixed carbon, dry (%) 60.39 30.70 50.58 4.152964 Ash, dry (%) 43.81 4.41 12.56 4.861197 Total sulfur, dry (%) 9.07 0.62 3.00 2.018264 Btu/Ib, dry 14076.00 8025.00 12631.08 841.5436 Carbon, dry (%) 79.32 44.03 70.43 5.026348 Hydrogen, dry (%) 5.36 3.39 4.78 0.310245 Nitrogen, dry (%) 3.03 0.35 1.40 0.290988 Oxygen, dry (%) 12.57 2.16 7.53 1.660288 Free swelling index 8.50 1.00 4.39 1.268707 2.3.
0.061200 - Artificial neural network design and development Artificial neural network models have been studied for two decades, with an objective of achieving human like performance in many fields of knowledge engineering.
0.089301 - Neural networks are powerful tools that have the ability to identify underlying highly complex relationships from input–output data only (Plippman, 1987).
0.112224 - The study of neural network is an attempt to understand the functionality of a brain.
0.055172 - Essentially, ANN is an approach to artificial intelligence, in which a network of processing elements is designed.
0.033126 - Further, mathematics carry out information processing for problems whose solutions require knowledge that is difficult to describe (Stephen, 1990; Zeidenberg, 1990).
0.063857 - ANNs derived from their biological counterparts, ANNs are based on the concept that a highly interconnected system of simple processing elements (also called “nodes” or “neurons”) can learn complex nonlinear interrelationships existing between input and output variables of a data set (Vuthaluru, Brooke, Zhang, & Yan, 2003).
0.037825 - For developing ANN model of a system, feed-forward architecture namely MLP1 is most commonly used.
0.052033 - This network usually consists of a hierarchical structure of three layers described as input, hidden, and output layers, comprising I, J, and L number of processing nodes, respectively (Vuthaluru et al., 2003).
0.010667 - General MLP architecture with two hidden layers is shown in Fig 1.
0.074593 - When an input pattern is introduced to the neural network, the synaptic weights between the neurons are stimulated and these signals propagate through layers and an output pattern is formed.
0.065633 - Depending on how close the formed output pattern is to the expected output pattern, the weights between the layers and the neurons are modified in such a way that next time the same input pattern is introduced, the neural network will provide an output pattern that will be closer to the expected response (Patel et al., 2007).
0.000000 - MLP architecture with two hidden layers (Patel et al Fig 1.
0.000000 - MLP architecture with two hidden layers (Patel et al., 2007).
0.047647 - Various algorithms are available for training of neural networks.
0.061844 - Feedforward back-propagation algorithm is the most versatile and robust technique, which provides the most efficient learning procedure for multilayer perception (MLP) neural networks.
0.061002 - Also, the fact that the back-propagation algorithm is especially capable of solving predictive problems makes it so popular.
0.070610 - The network2 model presented in this article is a supervised back-propagation neural network, making use of the Levenberg–Marquardt approximation.
0.058537 - This algorithm is more powerful than the common used gradient descent methods, because the Levenberg–Marquardt approximation makes training more accurate and faster near minima on the error surface (Lines & Treitel, 1984).
0.024938 - The method is as follows: (2) In Eq (3) the adjusted weight matrix ΔW is calculated using a Jacobian matrix J, a transposed Jacobian matrix JT, a constant multiplier m, a unity matrix I and an error vector e. The Jacobian matrix contains the weights derivatives of the errors: (3) If the scalar μ is very large, the Levenberg–Marquardt algorithm approximates the normal gradient descent method, while if it is small, the expression transforms into the Gauss–Newton method (Haykin, 1999).
0.077519 - For more detailed information the readers are referred to Lines and Treitel (1984).
0.050450 - After each successful step (lower errors) the constant m is decreased, forcing the adjusted weight matrix to transform as quickly as possible to the Gauss–Newton solution.
0.041344 - When after a step the errors increase the constant m is increased subsequently.
0.058394 - The weights of the adjusted weight matrix (Eq (3)) are used in the forward pass.
0.077859 - The mathematics of both the forward and backward pass is briefly explained in the following.
0.062615 - The net input (netpj) of neuron j in a layer L and the output (opj) of the same neuron of the pth training pair (i.e.
0.070519 - the inputs and the corresponding swelling index value of sample) are calculated by: (4) where the number of neurons in the previous layer (L − 1) are defined by n = 1 to last neuron and the weights between the neurons of layer L and L − 1 by wjn.
0.050955 - The output (opj) is calculated using the logarithmic sigmoid transfer function: (5) where θj is the bias of neuron j.
0.044199 - In general the output vector, containing all opj of the neurons of the output layer, is not the same as the true output vector (i.e.
0.071685 - the measured FSI value).
0.055096 - This true output vector is composed of the summation of tpj.
0.047993 - The error between these vectors is the error made while processing the input–output vector pair and is calculated as follows: (6) When a network is trained with a database containing a substantial amount of input and output vector pairs the total error E (sum of the training errors Ep) can be calculated (Haykin, 1999) (7) To reduce the training error, the connection weights are changed during a completion of a forward and backward pass by adjustments (Δw) of all the connections weights w. Eqs.
0.026403 - (3) and (4) calculate those adjustments.
0.030075 - This process will continue until the training error reaches a predefined target threshold error.
0.028369 - Designing network architecture requires more than selecting a certain number of neurons, followed by training only.
0.049689 - Especially phenomena such as over fitting and under fitting should be recognized and avoided in order to create a reliable network.
0.077071 - Those two aspects – over fitting and under fitting – determine to a large extent the final configuration and training constraints of the network (Haykin, 1999).
0.074534 - Training and testing of the model As the above-mentioned, the input layer has six neurons Xi, i = 1, 2, … , 6.
0.047337 - The number of neurons in the hidden layer is supposed Y, the output of which is categorized as Pj, j = 1, 2, … , Y.
0.058667 - The output layer has one neuron which denotes amount of gold extraction.
0.051037 - It is assumed that the connection weight matrix between input and hidden layers is Wij, and the connection weight matrix between hidden and output layers is WHj, K denotes the learning sample numbers.
0.053333 - A schematic presentation of the whole process is shown in Fig 2.
0.013746 - ANN process flowchart Fig 2.
0.014981 - ANN process flowchart.
0.025063 - Nonlinear (LOGSIG, TANSIG) and linear (PURELIN) functions can be used as transfer functions (Figs.
0.029963 - 3 and 4).
0.040057 - The logarithmic sigmoid function (LOGSIG) is defined as (Demuth & Beale, 1994) (8) whereas, the tangent sigmoid function (TANSIG) is defined as follows (Demuth & Beale, 1994): (9) where ex is the weighted sum of the inputs for a processing unit.
0.000000 - Sigmoid transfer functions Fig 3.
0.000000 - Sigmoid transfer functions.
0.000000 - Liner transfer function Fig 4.
0.000000 - Liner transfer function.
0.085265 - The number of input and output neurons is the same as the number of input and output variables.
0.011019 - For this research, different multilayer network architectures are examined (Table 2).
0.000000 - Table 2.
0.064897 - Results of a comparison between some of the models.
0.000000 - No.
0.030480 - Transfer function Model 3SE 1 TANSIG–LOGSIG 10–5–1 1.34 2 LOGSIG–LOGSIG 10–7–1 0.7 3 LOGSIG–LOGSIG–LOGSIG 10–4–3–1 1.21 4 TANSIG–TANSIG–LOGSIG 10–5–3–1 1.02 5 LOGSIG–LOGSIG–LOGSIG 10–6–4–1 0.46 6 LOGSIG–LOGSIG–LOGSIG 10–7–4–1 0.3 7 LOGSIG–LOGSIG–LOGSIG 10–8–4–1 0.1S 8 LOGSIG–LOGSIG–LOGIG 10–8–6–1 0.03 9 LOGSIG–LOGSIG–LOGSIG 10–10–4–1 0.014 During the design and development of the neural network for this study, it was determined that a four-layer network with 14 neurons in the hidden layers (two layers) would be most appropriate.
0.111150 - Artificial neural network (ANN) architecture for predicting of the free swelling index is shown in Fig 5.
0.092004 - ANN architecture for predicting the free swelling index Fig 5.
0.098828 - ANN architecture for predicting the free swelling index.
0.088800 - To determine the optimum network, SSE was calculated for various models by the following formula: (10) where Ti, Oi and N represent the measured output, the predicted output and the number of input–output data pairs, respectively (Haykin, 1999).
0.080201 - The learning rate of the network was adjusted so that training time was minimized.
0.091168 - During the training, several parameters had to be closely watched.
0.082789 - It was important to train the network long enough so it would learn all the examples that were provided.
0.080679 - It was also equally important to avoid over training, which would cause memorization of the input data by the network.
0.089959 - During the course of training, the network is continuously trying to correct itself and achieve the lowest possible error (global minimum) for every example to which it is exposed.
0.063116 - The network performance during the training process is shown in Fig 6, as shown, the optimum epochs of train achieved about 400 epochs.
0.048930 - Network performance during the training process Fig 6.
0.052805 - Network performance during the training process.
0.073563 - For evaluation of a model, a comparison between predicted and measured values of FSI can be fulfilled.
0.025840 - For this purpose, MAE (Ea) and mean relative error (Er) can be used.
0.041408 - Ea and Er are computed as follows (Haykin, 1999) (11) (12) where Ti and Oi represent the measured and predicted output.
0.118203 - For the optimum model Ea and Er were equal to 0.02627 and 0.006633 respectively.
0.060307 - Comparison between measured and predicted free swelling index for training and testing data are shown in Figs.
0.057348 - 7 and 8 respectively.
0.081505 - Correlations achieved from these figures, between measured and predicted free swelling index from training and testing data, indicate that the network has high ability for predicting free swelling index (Figs.
0.029963 - 9 and 10).
0.083371 - Comparison of measured and predicted free swelling index for different samples… Fig 7.
0.080842 - Comparison of measured and predicted free swelling index for different samples of training data.
0.083371 - Comparison of measured and predicted free swelling index for different samples… Fig 8.
0.080842 - Comparison of measured and predicted free swelling index for different samples for test data.
0.073035 - Correlation between measured and predicted free swelling index for training data Fig 9.
0.077915 - Correlation between measured and predicted free swelling index for training data.
0.073035 - Correlation between measured and predicted free swelling index for testing data Fig 10.
0.077915 - Correlation between measured and predicted free swelling index for testing data.
0.107900 - Sensitivity analysis A useful concept has been proposed to identify the significance of each factor (input) on the factors (outputs) using a trained network.
0.130647 - This enables us to hierarchically recognize the most sensitive factors effecting coal swelling index.
0.045977 - This is performed by incorporating values of ‘relative strength of effect’ (RSEs) (Kim, Bae, et al., 2001).
0.085391 - After a BPNN has been trained successfully, the neural network is no longer allowed to adapt.
0.067113 - The output for a one-hidden-layer network can be written as: (13) where (14) (15) where w is a connected weight, Θ is a threshold and oi is the value of input unit.
0.014981 - Thus, we have.
0.032000 - (16) Since the activation function is sigmoid Eq (13), it is differentiable.
0.041537 - The variance of Ok with the change of Oj for a network with n hidden layers can be calculated by the differentiation of the following equation: (17) where G(ek) = e−ek/(1 + e−ek)2, Ojn, Ojn−1, Ojn−2, … , Oj1 denote the hidden units in the n, n − 1, n − 2, … , 1 hidden layers, respectively (Kim et al., 2001).
0.097893 - Obviously, no matter what the neural network approximates, all items on the right hand side of Eq (17) always exist.
0.077348 - According to Eq (17), a new parameter RSEki can be defined as the RSE for input unit i on output unit k (Kim et al., 2001).
0.037174 - Definition of RSE: For a given sample set S = {s1, s2, s3, … , sj, … , sr} where Sj = {X, Y}, X = {x1, x2,x3, … , xp}, Y = {y1, y2, y3, … , yp}, if there is a neural network trained by back-propagation algorithm with this set of samples, the RSEki exists as: (18) where C is a normalized constant which controls the maximum absolute value of RSEki as unit and the function G denotes differentiation of the activation function.
0.048000 - G, w and e are all the same as in Eq (17).
0.084425 - It should be noted that the control of RSE is done with respect to the corresponding output unit, which means all RSE values for every input unit on corresponding output unit are scaled with the same scale coefficient.
0.043796 - Hence, it is clear that RSE ranges from −1 to 1 (Kim et al., 2001).
0.053528 - Compared with Eq (17) RSE is similar to the derivative except for its scaling value.
0.070175 - But it is a different concept from the differentiation of the original mapping function.
0.084530 - RSE is a kind of parameter which could be to measure the relative importance of input factors to output units, and it shows only the relative dominance rather than the differentiation of one to one input and output.
0.119124 - The larger the absolute value of RSE, the greater the effect the corresponding input unit has on the output unit.
0.084602 - Also, the sign of RSE indicates the direction of influence, which means a positive action applies to the output when RSE > 0, and a negative action applies when RSE < 0.
0.080925 - Here, a positive action denotes that the output increases with the increment of the corresponding input, and decreases with reduction of the corresponding input.
0.084848 - On the contrary, negative action indicates that the output decreases when the corresponding input increase and increases when the corresponding input decreases.
0.082645 - The output has no relation with the input if RSE = 0.
0.072693 - RSE is a dynamic parameter which changes with the variance of input factors.
0.109971 - In a further section, the RSE will be used for a sensitivity analysis of the influence of factors on free swelling index predicted by a trained neural network.
0.063116 - Fig 11 shows the average RSE values of the factors calculated for all of 250 field data that used in the previous sections.
0.078161 - It can be seen in Fig 11 that ‘moisture’ and ‘nitrogen’ are usually the most sensitive parameters.
0.209644 - Remaining factors include Btu (dry), carbon (dry), fixed carbon (dry), hydrogen (dry), oxygen (dry), total sulfur (dry) and volatile matter (dry) which were studied in neural network method, also.
0.091031 - In addition, a positive value of RSE indicates that for example, if ‘carbon’ has a positive RSE (see Fig 8) increases the value of RSE, the FSI will increase, and inverse effects will take place in the case of negative RSE (i.e.
0.015686 - ‘ash’, etc.).
0.118427 - Sensitivity analysis between the free swelling index and coal chemical… Fig 11.
0.139301 - Sensitivity analysis between the free swelling index and coal chemical properties.
0.236594 - In this investigation the effect of coal chemical properties on free swelling index were studied.
0.382613 - Results from neural network showed that nitrogen (dry), carbon (dry), hydrogen (dry), Btu (dry), volatile matter (dry) and fixed carbon (dry) had positive effects on FSI, respectively.
0.357749 - The negative effects of input parameters were related to, moisture, oxygen (dry), ash (dry) and total sulfur (dry), respectively.
0.000000 - Figs.
0.112827 - 4 and 5 show that the measured and predicted free swelling indexes value are similar.
0.123688 - The results of artificial neural network shows that training and test data’s square correlation coefficients (R2) achieved 0.9967 and 0.9181 respectively.
0.227333 - In this research, to evaluate the effects of chemical properties of coal on FSI, artificial neural network approach was employed.
0.236224 - Input parameters were moisture, volatile matter (dry), fixed carbon (dry), ash (dry), total sulfur (organic and pyretic) (dry), Btu/lb (dry), carbon (dry), hydrogen (dry), nitrogen (dry) and oxygen (dry).
0.107644 - According to the results obtained from this research, the optimum ANN architecture has been found to be 10 and 4 neurons in the first and second hidden layer, respectively, and one neuron in output layer.
0.161048 - Higher nitrogen (dry), carbon (dry) and Btu (dry) contents in coal can result in higher free swelling index and higher moisture, oxygen (dry) and ash (dry) contents in coal results in lower free swelling index.
0.101227 - In ANN’s method, results of artificial neural network shows that training and test data’s square correlation coefficients (R2) achieved 0.9967 and 0.921, respectively.
0.292937 - Results from sensitivity analysis show that nitrogen (dry), moisture, oxygen (dry), carbon (dry), ash (dry), total sulfur (dry), Btu (dry) and volatile matter (dry) were effective parameter on free swelling index (Fig 11).
0.046243 - For network training performance, when number of epochs is 400, error of training network was minimized and after this point suitable performance was achieved.
0.380694 - Results from neural network showed that nitrogen (dry), carbon (dry), hydrogen (dry), Btu (dry), volatile matter (dry) and fixed carbon (dry) had positive effects on free swelling index, respectively.
0.357749 - The negative effects of input parameters were related to moisture, oxygen (dry), ash (dry) and total sulfur (dry), respectively.
0.335639 - The fixed carbon was found to have the lowest effect (0.0425) on FSI.
0.000000 - 1 Multiple layer perception.
0.048964 - 2 The network is developed in Matlab 7.1, using also a neural network toolbox.

[Frase 7] Sensitivity analysis shows that, nitrogen (dry), carbon (dry), hydrogen (dry), Btu (dry), volatile matter (dry) and fixed carbon (dry) have positive effects and moisture, oxygen (dry), ash (dry) and total sulfur (dry) have negative effects on FSI.
[Frase 34] In this investigation, 10 input parameters such as moisture, volatile matter (dry), fixed carbon (dry), ash (dry), total sulfur (organic and pyretic) (dry), Btu/lb (dry), carbon (dry), hydrogen (dry) nitrogen (dry) as well as oxygen (dry) were used.
[Frase 178] The fixed carbon was found to have the lowest effect (0.0425) on FSI.
[Frase 2] Artificial neural networks (ANNs) method for more than 300 datasets was used for evaluating free swelling index value.

