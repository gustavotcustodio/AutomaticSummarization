The greedy acceptance criterion for the glowworms updating positions is proposed. The new formulas for the glowworms movement are proposed. Uniform design experiments were investigated the effect of parameters. The proposed improvement algorithms were effective than the classical algorithm.

0.137072 - Glowworm swarm optimization (GSO) algorithm is the one of the newest nature inspired heuristics for optimization problems.
0.141298 - In order to enhances accuracy and convergence rate of the GSO, two strategies about the movement phase of GSO are proposed.
0.266214 - One is the greedy acceptance criteria for the glowworms update their position one-dimension by one-dimension.
0.130458 - The other is the new movement formulas which are inspired by artificial bee colony algorithm (ABC) and particle swarm optimization (PSO).
0.092962 - To compare and analyze the performance of our proposed improvement GSO, a number of experiments are carried out on a set of well-known benchmark global optimization problems.
0.238650 - The effects of the parameters about the improvement algorithms are discussed by uniform design experiment.
0.100851 - Numerical results reveal that the proposed algorithms can find better solutions when compared to classical GSO and other heuristic algorithms and are powerful search algorithms for various global optimization problems.
0.073620 - The computational drawbacks of existing derivative-based numerical methods have forced the researchers all over the world to rely on meta-heuristic algorithms founded on simulations to solve engineering optimization problems.
0.056022 - A common factor shared by the meta-heuristics is that they combine rules and randomness to imitate some natural phenomena.
0.083951 - During the last decade, nature inspired intelligence has become increasingly popular through the development and utilization of intelligent paradigms in advanced information systems design.
0.000000 - Cross-disciplinary team-based thinking attempts to cross-fertilize engineering and life science understanding into advanced inter-operable systems.
0.064516 - The term swarm is used for an aggregation of animals such as fish schools, birds, flocks and insect colonies such as ant, termites and bee colonies performing collective behavior.
0.076739 - The individual agents of a swarm behave without supervision and each of these agents has a stochastic behavior due to her perception in the neighborhood.
0.076739 - Local rules, without any relation to the global pattern, and interactions between self-organized agents lead to the emergence of collective intelligence called swarm intelligence.
0.033755 - Swarms use their environment and resources effectively by collective intelligence.
0.019185 - Self-organization is a key feature of a swarm system which results global level (macroscopic level) response by means of low level interactions (microscopic level).
0.027397 - Recently researchers have been inspired by those models and they have provided novel problem-solving techniques based on swarm intelligence for solving difficult real world problems such as traffic routing, networking, games, industry, robotics, economics and generally designing artificial self organized distributed problem-solving devices.
0.038986 - In 1990s, especially two approaches based on ant colony described by Dorigo (1992) and on fish schooling and bird flocking introduced by Kennedy and Eberhart (1995) have highly attracted the interest of researchers.
0.049383 - Both approaches have been studied by many researchers and their new versions have been introduced and applied for solving several problems in different areas.
0.097902 - Following this tradition, in 2005, Krishnanand and Ghose (2005) proposed glowworm swarm optimization algorithm, a derivative-free, meta-heuristic algorithm, mimicking the glow behavior of glowworms.
0.052493 - The algorithm shares some common features with ant colony optimization (ACO) and with particle swarm optimization (PSO), but with several significant differences.
0.086957 - The agents in GSO are thought of as glowworms that carry a luminescence quantity called luciferin along with them.
0.099388 - The glowworms encode the fitness of their current locations, evaluated using the objective function, into a luciferin value that they broadcast to their neighbors.
0.095194 - The glowworm identifies its neighbors and computes its movements by exploiting an adaptive neighborhood, which is bounded above by its sensor range.
0.031496 - Each glowworm selects, using a probabilistic mechanism, a neighbor that has a luciferin value higher than its own and moves toward it.
0.061350 - These movements based only on local information and selective neighbor interactions enable the swarm of glowworms to partition into disjoint subgroups that converge on multiple optima of a given multimodal function.
0.099738 - Since its inception, GSO has been used in various applications and several papers have appeared in the literature using the GSO algorithm.
0.047962 - Krishnanand and Ghose (2006a, 2006b, 2009) implemented a large class of benchmark multimodal functions to tested against the capability of GSO in capturing multiple optima.
0.067227 - Numerical simulation results showed the algorithm’s efficacy in capturing multiple peaks of a wide range of multi-modal functions.
0.024922 - Krishnanand and Ghose (2006c, 2008) researched theoretical foundations involving local convergence results for a simplified GSO model.
0.000000 - Krishnanand, Amruth, Guruprasad, Bidargaddi, and Ghose (2006), Kaipa et al.
0.071197 - (2006) applied the GSO algorithm to multiple source localization tasks that demonstrated through real-robot experiments.
0.060060 - Where four wheeled mobile robots implemented the GSO algorithm to collaborate and achieve a sound source localization task.
0.112150 - Krishnanand and Ghose (2007) described the application of the GSO algorithm to hazard sensing in ubiquitous environments.
0.060215 - Bharat (2008) used GSO to estimate the eigen values obtained from a corresponding transcendental equation, which was used to research analytical solutions for flow of chemical contaminants through soils.
0.119618 - The proposed solver quickly estimates the design parameters with a great precision on a real world inverse problem in environmental engineering.
0.098371 - He and Zhu (2010) presented a multi-population glowworm swarm optimization algorithm, the simulation results showed the improved algorithm could enhance the accuracy of the solution and reduce the computing time.
0.069565 - Since the performance of classical GSO over numerical benchmark functions with high dimensions suffers from stagnation or false convergence.
0.232870 - In the paper, we proposed new strategies for changing the position of the glowworms.
0.079840 - In the movement phase of classical GSO, each glowworm selects probabilistically a neighbor that glow brighter and moves a step that a fix size step multiplied by the distance between the neighbors.
0.077193 - This procedure is quite similar to the prey process of honey bees or birds.
0.093961 - So, the new strategies are inspired by artificial bee colony algorithm (ABC) and particle swarm optimization (PSO).
0.110622 - Moreover, as we know, the performance of population based meta-heurist greatly depends on the control parameters, but the various parameters of the classical GSO algorithm are fixed.
0.162239 - The parameters of GSO through uniform design experiment are discussed.
0.106667 - The remaining of this paper is organized as follows.
0.037559 - Review of GSO is summarized in Section 2.
0.144411 - Section 3 describes the proposed methods, improvement GSO, shortly IGSO.
0.113307 - Section 4 describes the benchmark problems and uniform design (UD) experiments.
0.104390 - The parameters of the algorithm are discussed by UD experiments and the testing of the proposed methods through benchmark problems are carried out and the simulation results are compared with those obtained via other algorithms that have been reported to have good performance.
0.097087 - Finally, the conclusion is drawn based on the comparison analysis reported and presented in Section 5.
0.030030 - GSO algorithm is developed by Krishnanand and Ghose (2005), which is improved from ACO approach to continuous optimization.
0.108832 - It based on the glowworm metaphor and applied to manipulating collective robotics.
0.020356 - In GSO, each artificial glowworm, agent, carries a light on two dimensional works space and has its own vision, called local-decision range.
0.133333 - The luciferin level is associated with the objective value of the agent’s position.
0.064725 - The brighter agent means that it flies to a better position (has a better objective value).
0.077098 - The agent is only attracted by a neighbor whose luciferin intensity is higher than its own within the local decision range and then flies towards the neighbor.
0.135021 - The local-decision range depends on the number of neighbors.
0.086721 - While the neighbor-density is low, the range is enlarged in order to find more neighbors, otherwise the range is reduced.
0.073260 - The agent always changes its moving direction according to which neighbor is selected.
0.107280 - The higher luciferin level the neighbor has, the more attraction which gains.
0.067511 - Finally, most agents will get together at the multiple locations.
0.074766 - Briefly, the GSO involves in three main phases: luciferin update phase, movement phase, and decision range update.
0.177797 - The luciferin update depends on the function value at the glowworm position.
0.086331 - Although all glowworms start with the same luciferin value at the initial iteration, these values change according to the function values at their current positions.
0.098551 - The luciferin value is proportional to the measured value of the sensed profile (temperature, radiation level) at that location.
0.039801 - Each glowworm adds its previous luciferin level.
0.119241 - At the same time, the luciferin level of glowworm is subtracted the previous luminescence value to simulate the decay in luminescence.
0.050534 - The luciferin update rule is given by: (1) where li(t), represents the luciferin level associated with glowworm i at time t, ρ is the luciferin decay constant 0 < ρ < 1, γ is the luciferin enhancement constant, and Ji represents the value of objective function at agent i’s location at time t. During the movement phase, each glowworm uses a probabilistic mechanism to decide a movement of a neighbor that has a luciferin value more than its own.
0.056338 - Glowworms are attracted by neighbors that glow brighter.
0.047472 - For each glowworm i, the probability of moving toward a neighbor j is given by: (2) where is the set of neighborhood of glowworm i at time t. di,j(t) represents the Euclidean distance between glowworms i and j at time t, and represents the variable neighborhood range associated with glowworms i at time t. Let glowworm i select a glowworm j ∈ Ni(t) with pij(t) given by (2).
0.121212 - Then, movements of glowworms can be stated as: (3) where s is the step-size.
0.090395 - ∥∥ represents the Euclidean norm operator.
0.081633 - Neighborhood range update rule: We associate with each agent i a neighborhood whose radial range is dynamic in nature represents the radial range of the luciferin sensor.
0.073260 - The fact that a fixed neighborhood range is not used needs some justification.
0.113652 - When the glowworms depend only on local information to decide their movements, it is expected that the number of peaks captured would be a function of the radial sensor range.
0.079470 - In fact, if the sensor range of each agent covers the entire search space, the entire agents move to the global optimum and the local optima are ignored.
0.062397 - Since we assume that a priori information about the objective function (e.g., number of peaks and inter-peak distances) is not available, it is difficult to fix the neighborhood range at a value that works well for different function landscapes.
0.060952 - For instance, a chosen neighborhood range rd would work relatively better on objective functions where the minimum inter-peak distance is more than rd rather than on those where it is less than rd.
0.054201 - Therefore, GSO uses an adaptive neighborhood range in order to detect the presence of multiple peaks in a multimodal function landscape.
0.073620 - A substantial enhancement in performance is noticed by using the rule given below: (4) where β is a constant parameter and nt is a parameter used to control the number of neighbors.
0.134680 - The computational procedure of the basic GSO algorithm can be summarized as follows Fig 1.
0.204433 - The Flowchart of the glowworm swarm optimization algorithm Fig 1.
0.227737 - The Flowchart of the glowworm swarm optimization algorithm.
0.058608 - In this section, we describe two new strategies for movement phase for GSO.
0.154942 - Our modifications to GSO are based on (1) changes to the acceptance criteria for the movement and (2) changes to the formula for the movement.
0.033755 - We will now motivate why these changes have been made.
0.102840 - Experiments with the classical GSO meta-heuristics over the standard numerical benchmarks suggest that the algorithm does suffer from the problem of premature or false convergence.
0.136467 - Also the performance of the classical GSO algorithm deteriorates with the growth of search space dimensionality.
0.112621 - In the traditional GSO, the glowworm is attracted by neighbors that glow brighter and move a step with all dimensions in the space.
0.064171 - Therefore, although the local convergence speed of a standard GSO is quite good and the ability of exploitation the solution is very well, it might result in the premature convergence in optimizing multimodal and high dimensions problems.
0.066158 - To circumvent this problem, we replace the movement phase in classical GSO with new neighboring solution production mechanism borrowed from ABC and PSO.
0.089636 - The Artificial Bee Colony (ABC) algorithm is a new swarm intelligence technique inspired by intelligent foraging behavior of honey bees.
0.068571 - The first framework of ABC algorithm mimicking the foraging behavior of honey bee swarm in finding good solutions to optimize multi-variable and multi-modal continuous functions was presented by Karaboga and Akay (2009).
0.110312 - Numerical comparisons demonstrated that the performance of the ABC algorithm is competitive to other population-based algorithm with an advantage of employing fewer control parameters.
0.075472 - In ABC, there is no explicit neighbor domain and the honey bees are classified three kinds: employed bees, onlookers and scouts according to the quality (fitness) of the associated solution.
0.074488 - Employed bees are responsible for exploiting the nectar sources explored before and giving information to the waiting bees (onlooker bees) in the hive about the quality of the food source sites which they are exploiting.
0.071247 - Onlooker bees wait in the hive and decide on a food source to exploit based on the information shared by the employed bees.
0.047962 - Scouts either randomly search the environment in order to find a new food source depending on an internal motivation or based on possible external clues.
0.069530 - The position of a food source represents a possible solution to the optimization problem and the nectar amount of a food source corresponds to the quality (fitness) of the associated solution.
0.091954 - The employed bees and onlooker bees update their position using Eq (5).
0.065934 - Where a new position vi is determined by changing one parameter of xi.
0.014652 - k ∈ {1, 2, … SN} and j ∈ {1, 2, … D} are randomly chosen indexes.
0.033613 - Although k is determined randomly, it has to be different from i. φij is a random number between [−1, 1].
0.094488 - It controls the production of neighbor food sources around xij and represents the comparison of two food positions visible to a bee.
0.093240 - As can be seen from (5), The neighboring solution production mechanism used in ABC is similar to the self-adapting mutation process of differential evolutionary (DE).
0.089636 - The solutions in the population directly affect the mutation operation since the operation is based on the difference between them.
0.050891 - However, in DE, the difference is weighted by a constant scaling factor while in ABC; it is weighted by a random step size.
0.030534 - Unlike DE, in ABC, there is no explicit crossover and there is only one dimensional position adjusted for each individual in every cycle.
0.079012 - In the ABC algorithm, while onlookers and employed bees carry out the exploitation process in the search space, the scouts control the exploration process.
0.075055 - The performance of ABC is very good in terms of the local and the global optimization due to the selection schemes employed and the neighboring production mechanism used.
0.160166 - (5) In the paper, the glowworms update their position using Eq (6) and the new position is accepted when it is better than the old one.
0.135283 - Similar to ABC, the glowworm randomly select a neighbor j, and changed the parameter one by one until the fitness of the new position is better than the old one.
0.146169 - If the fitness of new position is no change when all parameters are changed, the the glowworm give up the new position and keep the old one.
0.149081 - In the improvement algorithm (AGSO), the position of the glowworm is adjusted one-dimensional by one-dimension, the the exploitation process is more precise.
0.146520 - Likely the ABC, the PSO also can be incorporate into the algorithm (PGSO).
0.000000 - In a PSO system, multiple candidate solutions coexist and collaborate simultaneously.
0.133065 - Each solution called a “particle”, flies in the problem search space looking for the optimal position to land.
0.064198 - A particle, as time passes through its quest, adjusts its position according to its own “experience” as well as the experience of neighboring particles.
0.092827 - Tracking and memorizing the best position encountered build particle experience.
0.035556 - For that reason, PSO possesses a memory (i.e.
0.120482 - every particle remembers the best position it reached during the past).
0.010499 - PSO system combines local search method (through self experience) with global search methods (through neighboring experience), attempting to balance exploration and exploitation.
0.162123 - The glowworms can also update their position using (7).
0.124542 - Where xbest(t) represents the best position among all glowworms in the population.
0.132132 - ω is called the inertia weight that controls the impact of previous position of glowworms on its current one.
0.107226 - c1, c2 are positive constant parameters called social weight, which control the impact of the position of best glowworms and neighbor glowworms on the current one.
0.162869 - The equation shows that the new position of the glowwoms is effected by itself, its neighbors and the best glowworms.
0.181444 - The Fig 2 shows the example of the 2-dimensions for the glowworms by using three different updating formulas.
0.205384 - represents the new position is calculated by the classic GSO.
0.152619 - represents the new position is calculated by AGSO.
0.152619 - represents the new position is calculated by PGSO.
0.176707 - The improvement of the movement procedure is described in Fig 3.
0.097087 - (6) (7) Detailed pseudo-code of the improvement GSO movement procedure is given in Fig 2.
0.119403 - The movement of schematic diagram Fig 2.
0.135593 - The movement of schematic diagram.
0.160338 - The pseudo-code of the improvement movement procedure Fig 3.
0.178404 - The pseudo-code of the improvement movement procedure.
0.140122 - In this section, the experiments that have been done to evaluate the performance of the proposed improvement GSO algorithm (IGSO) and its variants for a number of analytical benchmark functions are described.
0.057743 - The IGSO are coded in Visual C++ 6.0 and experiments are executed on a Pentium E2200 CPU PC with 2G RAM.
0.053640 - Each benchmark is independently run with every algorithm 30 times for comparisons.
0.104326 - The mean value (Mean), minimum value (Min), and the standard deviation (Dev) in 30 runs are calculated as the statistics for the performance measures.
0.101781 - The mean value and minimum value represent the global convergence of the algorithm, and the standard deviation represents the stability of the algorithms.
0.071942 - Well-defined benchmark functions which are based on mathematical functions can be used as objective functions to measure and test the performance of optimization methods.
0.080997 - The nature, complexity and other properties of these benchmark functions can be easily obtained from their definitions.
0.102564 - The difficulty levels of most benchmark functions are adjustable by setting their parameters.
0.120120 - To test the performances of the modified algorithms, 12 well known benchmark functions are presented in Table 1.
0.018779 - Initial range, formulation, properties are listed in table.
0.022989 - A function is called unimodal, if it has only one optimum position.
0.071111 - The multimodal functions have two or more local optima.
0.037559 - Global optimum values of 12 functions are 0.
0.153257 - In the following experiment, the dimensions of the functions are set 10.
0.000000 - Table 1.
0.000000 - Numerical benchmark functions.
0.000000 - Name Formulation Property Range Sphere Unimodal [−5.12, 5.12] Rosenbrock Unimodal [−30, 30] Griewank Multimodal [−600, 600] Rastrigin Multimodal [−5.12, 5.12] Schwefel2.26 Multimodal [−500, 500] Ackley Multimodal [−32, 32] Step Unimodal [−100, 100] Schwefel 2.22 Unimodal [−10, 10] Schwefel 1.2 Unimodal [−100, 100] Quartic Unimodal [−1.28, 1.28] Dixon-Price Unimodal [−10, 10] Penalized Multimodal [−50, 50] 4.1.
0.108108 - Parameters discussion As we know, the performance of population based meta-heuristic greatly depends on the control parameters.
0.146430 - So, in order to find the right values of the parameters, we investigate the effect of parameters on the performance of HHSABC by uniform design (UD) experiments.
0.023952 - There are many statistical experimental designs which have found wide applications in parameter estimation, such as factorial design (FD), fractional factorial design (FFD), orthogonal array design (OAD) and central composite design (CCD).
0.100719 - However, a drawback of all the above-mentioned experimental designs is that the number of experiments increases dramatically with the increase in number of levels.
0.043360 - For example, for a design with s factors, with each factor having q levels, a full factorial design needs qs experiments.
0.080997 - Although OAD can reduce this number to q2, the number of levels still has to be controlled.
0.022599 - UD only need q experiments.
0.038835 - UD was proposed by Fang (1980) based on quasi-Monte Carlo method or number-theoretic method.
0.117216 - The development of UD was initially motivated by the need in system engineering.
0.049158 - Later, uniform design has been gradually popularized in China, particularly in agriculture, textile industry, watch industry, science researches and so on.
0.107280 - The UD method has at least the following merit: (1) Space filling.
0.070175 - It is capable of producing samples with high representative in the studied experimental domain.
0.000000 - (2) Robustness.
0.072072 - It imposes no strong assumption on the model, and is against changes of model in a certain sense.
0.000000 - (3) Multiple levels.
0.101010 - It accommodates the largest possible number of levels for each factor among all experimental designs.
0.121088 - The theory of uniform design and the use of uniform design tables were introduced by Fang (1980), Liang, Fang, and Xu (2001).
0.074666 - Like orthogonal designs, uniform designs offer lots of experimental tables for users to conveniently utilize.
0.094281 - Uniform design tables of form Un(ns) is purposely chosen to mimic the tables of orthogonal designs, Ln(qs), except that the number of levels equals n the number of experiments.
0.088395 - In the improvement algorithms, there are six key parameters influencing the performance of the algorithms: ρ, γ, β, the range of these parameters are [0, 1]; l0, the range is [0, 5]; nt, the range is (0, 1) × NP; and rs, the range is (0, 1) × ∥ub − lb∥.
0.014652 - In order to get more information from experiment, each factor takes 20 levels.
0.072072 - It is known that there are some interactions among the parameters and we can ignore high-order interactions.
0.065728 - Therefore, Table 2 is chosen for experimental design.
0.086101 - According the use of uniform design table (Table 3), the choice of columns 1, 2, 4, 5, 6 and 7 results in a set of experimental points with the minimum discrepancy.
0.000000 - Table 2. .
0.000000 - No.
0.000000 - 1 2 3 4 5 6 7 No.
0.000000 - 1 2 3 4 5 6 7 1 1 4 5 10 13 16 19 11 11 2 13 5 17 8 20 2 2 8 10 20 5 11 17 12 12 6 18 15 9 3 18 3 3 12 15 9 18 6 15 13 13 10 2 4 1 19 16 4 4 16 20 19 10 1 13 14 14 14 7 14 14 14 14 5 5 20 4 8 2 17 11 15 15 18 12 3 6 9 12 6 6 3 9 18 15 12 9 16 16 1 17 13 19 4 10 7 7 7 14 7 7 7 7 17 17 5 1 2 11 20 8 8 8 11 19 17 20 2 5 18 18 9 6 12 3 15 6 9 9 15 3 6 12 18 3 19 19 13 11 1 16 10 4 10 10 19 8 16 4 13 1 20 20 17 16 11 8 5 2 Table 3.
0.062669 - Using uniform design table.
0.000000 - s Row no.
0.066065 - D 2 1 5 0.0744 3 1 2 3 0.1363 4 1 4 5 6 0.1915 5 1 2 4 5 6 0.2012 6 1 2 4 5 6 7 0.201 In the experiment, the population size of the glowworms is set NP = 100, the max iteration is set MCN = 5000.
0.144345 - Sphere function is tested in the uniform design experiments.
0.162208 - Table 4 shows the result of the uniform design of the parameters about the algorithm.
0.112281 - It shows that the 16th row parameters have the better performance in Mean value.
0.093897 - Fig 4 shows the result of Talbe4 visualization.
0.144578 - So, the parameters are set at bellow in the following experiments.
0.000000 - ρ = 0.8, γ = 0.05, β = 0.65, L0 = 0.5, nt = 0.95, and rs = 0.5.
0.000000 - Table 4.
0.110802 - Uniform design of IGSO for Sphere function.
0.000000 - No.
0.000000 - Mean Dev Min No.
0.008274 - Mean Dev Min 1 1.59E−09 6.90E−09 3.98E−17 11 4.88E−16 8.28E−16 7.04E−17 2 4.89E−05 2.08E−04 9.07E−17 12 2.12E−16 1.17E−16 3.51E−17 3 9.30E−11 2.69E−10 4.97E−17 13 1.54E−07 6.73E−07 7.95E−17 4 1.00E−10 2.50E−10 8.02E−17 14 2.90E−16 2.61E−16 5.19E−17 5 4.65E−06 1.98E−05 2.21E−16 15 1.18E−09 4.15E−09 3.64E−17 6 9.10E−12 3.97E−11 6.54E−17 16 1.88E−16 1.03E−16 4.41E−17 7 1.76E−12 7.65E−12 1.27E−16 17 2.01E−16 9.52E−17 5.79E−17 8 4.55E−16 5.23E−16 7.05E−17 18 1.32E−04 5.68E−04 5.42E−17 9 3.06E+01 7.90E+00 5.80E+00 19 6.96E−07 2.92E−06 1.49E−16 10 3.23E+01 7.78E+00 1.80E+01 20 3.45E+01 6.74E+00 1.95E+01 Uniform design of IGSO for Sphere function Fig 4.
0.110802 - Uniform design of IGSO for Sphere function.
0.112045 - For PGSO, it has the other three parameters: ω and c1, c2, the range is [0, 1] and [0, 2] respectively.
0.105919 - According to the Table 3, the columns 1, 2, and 3 is selected in the UD experiment.
0.151111 - Table 5 shows the result of the UD experiment.
0.126126 - From the table, it can be seen that the 6th row parameters have the better performance than others.
0.150235 - Fig 5 visualize the result of the table.
0.066066 - So, parameters are set at bellow in the following experiments ω = 0.45, c1 = 0.3 and c2 = 15.
0.000000 - Table 5.
0.110802 - Uniform design of PGSO for Sphere function.
0.000000 - No.
0.000000 - Mean Dev Min No.
0.008274 - Mean Dev Min 1 6.32E−20 7.40E−20 7.20E−22 11 9.75E−20 1.29E−19 1.80E−21 2 1.27E−19 2.39E−19 9.16E−22 12 4.62E−18 3.05E−18 1.51E−19 3 6.93E−19 4.15E−19 1.61E−19 13 1.10E−19 9.63E−20 2.37E−21 4 4.02E−16 5.18E−16 2.01E−17 14 1.17E−18 9.45E−19 1.17E−19 5 7.87E−19 9.43E−19 2.68E−20 15 7.39E−18 4.44E−18 1.94E−18 6 5.92E−20 1.03E−19 4.45E−23 16 1.13E−18 1.38E−18 8.42E−20 7 2.75E−19 2.34E−19 1.15E−20 17 5.64E−20 8.66E−20 6.10E−22 8 3.98E−18 3.20E−18 3.17E−19 18 5.80E−19 7.38E−19 6.51E−21 9 2.25E−19 2.06E−19 4.39E−21 19 4.23E−18 2.85E−18 5.76E−20 10 2.06E−18 1.80E−18 8.78E−20 20 7.94E−18 4.82E−18 2.18E−18 Uniform design of PGSO for Sphere function Fig 5.
0.110802 - Uniform design of PGSO for Sphere function.
0.135272 - Comparison between IGSO and GSO In the section, we compare the proposed IGSO with GSO.
0.099502 - The results are listed in Table 6.
0.134680 - The best results obtained by the three algorithms for every function are denoted by bold.
0.087227 - The results show that AGSO and PGSO can obtain much better results than GSO for all benchmarks.
0.134680 - There is no dispute that more precise exploitation can improve the performance of the algorithms.
0.097561 - Comparison between AGSO and PGSO, the PGSO obtain the better results for almost all the test functions except the Rosenbrock function.
0.130082 - It can be explained that the elite glowworm play the key roles in the GSO algorithm, it can guide the glowworm to the better position.
0.000000 - Figs.
0.106454 - 6 and 7 present the typical solution history graph along iterations of the two algorithms for the six functions (Sphere, Griewank, Quartic, Schwefel1.2, Dixon-Price and Penalized).
0.126337 - It can be observed that the evolution curves of the PGSO algorithm descend much faster and reach lower level than the AGSO.
0.000000 - Table 6.
0.000000 - Comparison between IGSO and GSO.
0.000000 - GSO AGSO PGSO Mean Dev Min Mean Dev Min Mean Dev Min Sphere 3.37E+01 7.20E+00 2.16E+01 2.55E−16 9.25E−17 9.17E−17 4.84E−20 7.24E−20 8.19E−23 Rosenbrock 2.55E+07 1.14E+07 5.67E+06 3.80E+00 2.70E+00 1.99E−05 5.35E+00 1.77E+00 2.02E−04 Griewank 1.14E+02 2.80E+01 4.65E+01 3.89E−16 3.21E−16 1.11E−16 0.00E+00 0.00E+00 0.00E+00 Rastrigin 1.05E+02 1.14E+01 8.35E+01 1.36E+00 3.25E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 Schwefel2.26 2.66E+03 2.75E+02 2.00E+03 3.43E+02 3.26E+02 1.27E−04 1.79E+02 1.50E+02 1.27E−04 Ackley 1.94E+01 5.07E−01 1.84E+01 6.46E−14 2.50E−13 2.96E−15 −5.89E−16 0.00E+00 −5.89E−16 Step 1.22E+04 2.99E+03 5.21E+03 0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 Quartic 4.23E+00 1.26E+00 2.44E+00 3.38E−03 1.53E−03 1.08E−03 3.82E−05 3.31E−05 7.52E−07 Schwefel2.22 2.35E+01 2.85E+00 1.70E+01 0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 Schwefel1.2 4.03E+03 1.23E+03 1.42E+03 2.54E−05 6.16E−05 5.57E−16 6.32E−20 9.61E−20 3.84E−22 Dixon−Price 4.47E+04 2.20E+04 6.18E+03 9.36E−08 3.84E−07 3.00E−16 5.82E−22 1.14E−21 1.58E−24 Penalized 1.17E+08 6.17E+07 1.51E+07 2.34E−16 1.39E−16 6.51E−17 1.91E−16 8.14E−17 5.93E−17 Evolution of min values for different functions (1) Fig 6.
0.056338 - Evolution of min values for different functions (1).
0.050633 - Evolution of min values for different functions (2) Fig 7.
0.056338 - Evolution of min values for different functions (2).
0.064171 - Comparison with other algorithms Results of PGSO algorithm have been compared with the results presented by Karaboga and Akay (2009) of differential evolution (DE), particle swarm optimization (PSO), genetic algorithm (GA) and artificial bee colony algorithm (ABC).
0.069565 - Karaboga and Akay (2009) set the dimensions was 30 and values less than E−12 were reported as 0.
0.088889 - The mean values found are given in Table 7.
0.083885 - The results show that all the algorithms provide good performance for Sphere function except GA. All algorithms have no good performances for Rosenbrock, Schwefel2.26 and Quartic functions.
0.021164 - But, PGSO outperforms other four algorithms.
0.046377 - On Step, Schwefel2.22 and Schwefel1.2 functions, PSO, DE, ABC and PGSO have got the global optimal values.
0.049383 - On Ackley function, DE, ABC and PGSO show equal performance and find the global optimum values, while PSO, GA demonstrated worse performance than them.
0.024024 - ABC and PGSO have better performance than other three algorithms on Griewank, Rastrigin, Dixon-Price and Penalized functions.
0.135021 - For all the benchmark functions, the PGSO outperform other methods.
0.141298 - To sum up, the proposed algorithms are very efficient and effective algorithms with excellent quality and robustness for unimodal, multimodal functions.
0.000000 - Table 7.
0.024242 - Comparison with other algorithms.
0.000000 - GA PSO DE ABC PGSO Sphere 1.11E+03 0.00E+00 0.00E+00 0.00E+00 0.00E+00 Rosenbrock 1.96E+05 1.51E+01 1.82E+01 8.88E−02 2.12E−04 Griewank 1.06E+01 1.74E−02 1.48E−03 0.00E+00 0.00E+00 Rastrigin 5.29E+01 4.40E+01 1.17E+01 0.00E+00 0.00E+00 Schwefel2.26 9.76E+02 5.56E+03 2.30E+03 1.30E−02 3.82E−04 Ackley 1.47E+01 1.65E−01 0.00E+00 0.00E+00 0.00E+00 Step 1.17E+03 0.00E+00 0.00E+00 0.00E+00 0.00E+00 Quartic 1.81E−01 1.16E−03 1.36E−03 3.00E−02 9.06E−05 Schwefel2.22 1.10E+01 0.00E+00 0.00E+00 0.00E+00 0.00E+00 Schwefel1.2 7.40E+03 0.00E+00 0.00E+00 0.00E+00 0.00E+00 Dixon−Price 1.22E+03 6.67E−01 6.67E−01 0.00E+00 0.00E+00 Penalized 1.25E+02 7.68E−03 2.20E−03 0.00E+00 0.00E+00
0.043956 - This paper presented improvement GSO algorithm which is incorporated with ABC and PSO.
0.163948 - The proposed IGSO algorithm applied two new movement stratifies:the greedy acceptance movement rule and the movement formulas which are inspired by ABC and PSO.
0.146719 - In order to verify the feasibility and the performance of the proposed algorithms, 12 high dimensional numerical benchmark functions were tested.
0.255213 - The critical parameters of the proposed algorithms were tested in the uniform design experiments.
0.238189 - The performances of the proposed algorithms were compared and analyzed.
0.178711 - From the simulation results it was concluded that the proposed IGSO algorithms were more effective in finding better solutions than the existing classical GSO.
0.110577 - Our future work will generalize the proposed IGSO algorithm to solve combinatorial and discrete optimization problem.

[Frase 242] The critical parameters of the proposed algorithms were tested in the uniform design experiments.
[Frase 113] If the fitness of new position is no change when all parameters are changed, the the glowworm give up the new position and keep the old one.
[Frase 39] In the paper, we proposed new strategies for changing the position of the glowworms.
[Frase 85] Our modifications to GSO are based on (1) changes to the acceptance criteria for the movement and (2) changes to the formula for the movement.
[Frase 1] Glowworm swarm optimization (GSO) algorithm is the one of the newest nature inspired heuristics for optimization problems.
