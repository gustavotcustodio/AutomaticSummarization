We formulate a regularized penalty function solution for the multi-objective constrained problem. We prove that exists a solution of the original problem with minimal weighted norm which is unique. We suggest a projection-gradient algorithm for computing the penalty function. We prove the convergence of the gradient method and the rate of convergence. We present a method that make the Pareto frontier more useful as decision support system.

0.211357 - In this paper, we present a novel approach for computing the Pareto frontier in Multi-Objective Markov Chains Problems (MOMCPs) that integrates a regularized penalty method for poly-linear functions.
0.247625 - In addition, we present a method that make the Pareto frontier more useful as decision support system: it selects the ideal multi-objective option given certain bounds.
0.124700 - We restrict our problem to a class of finite, ergodic and controllable Markov chains.
0.147334 - The regularized penalty approach is based on the Tikhonov’s regularization method and it employs a projection-gradient approach to find the strong Pareto policies along the Pareto frontier.
0.092157 - Different from previous regularized methods, where the regularizator parameter needs to be large enough and modify (some times significantly) the initial functional, our approach balanced the value of the functional using a penalization term (μ) and the regularizator parameter (δ) at the same time improving the computation of the strong Pareto policies.
0.125602 - The idea is to optimize the parameters μ and δ such that the functional conserves the original shape.
0.109015 - We set the initial value and then decrease it until each policy approximate to the strong Pareto policy.
0.185195 - In this sense, we define exactly how the parameters μ and δ tend to zero and we prove the convergence of the gradient regularized penalty algorithm.
0.119912 - On the other hand, our policy-gradient multi-objective algorithms exploit a gradient-based approach so that the corresponding image in the objective space gets a Pareto frontier of just strong Pareto policies.
0.165720 - We experimentally validate the method presenting a numerical example of a real alternative solution of the vehicle routing planning problem to increase security in transportation of cash and valuables.
0.065292 - The decision-making process explored in this work correspond to the most frequent computational intelligent models applied in practice within the Artificial Intelligence research area.
0.069767 - Brief review Multi-criterion optimization is a well-established research area with an extensive collection of solution concepts and methods presented in the literature (for an overview see, for example, Collette & Siarry, 2004; Dutta & Kaya, 2011; Ehrgott & Gandibleux, 2002; Eichfelder, 2008; Mueller-Gritschneder, Graeb, & Schlichtmann, 2009; Roijers, Vamplew, Whiteson, & Dazeley, 2013; Zitzler, Deb, & Thiele, 2000).
0.072072 - The most common notion of multi-criterion optimization is that of efficient (or Pareto optimal) solutions, namely those solutions that cannot be improved upon in all coordinates (with strict improvement is at least one coordinate) by another solution.
0.040404 - Regrettably efficient solutions are non-unique.
0.138464 - Then, different methods have been proposed to identify one optimal solution, such that of computing the scalar combinations of the objective functions.
0.124562 - Alternatively, an optimization method that leads to a well-defined solution is the constrained optimization problem, where one criterion is optimized subject to explicit constraints on the others.
0.081514 - In practice decision makers often find it hard to specify such preferences, and would prefer a representation method based-on an intelligent system able to exemplify the range of such possible alternatives.
0.059590 - Many researchers utilized versatile computational intelligent models such as Pareto based techniques for handling this types of problems (Angelov, Filev, & Kasabov, 2010).
0.077922 - In the context of Markov decision processes (MDP) serious works have been developed to compute efficient solutions.
0.096203 - Durinovic, Lee, Katehakis, and Filar (1986) presented a MOMDP based on the average reward function to characterize the complete sets of efficient policies, efficient deterministic policies and efficient points using linear programming.
0.085958 - Beltrami, Katehakis, and Durinovic (1995) considered a MOMDP formulation for deploying emergency services considering the area average response time and the steady-state deterioration in the ability of each district to handle future alarms originating in its own region.
0.080757 - Wakuta and Togawa (1998) suggested a policy iteration algorithms for finding all optimal deterministic stationary policies concerned with MOMDP with no discounting as well as discounting.
0.090801 - Chatterjee, Majumdar, and Henzinger (2006) considered MDPs with multiple discounted reward objectives showing that the Pareto curve can be approximated in polynomial time in the size of the MDP.
0.117586 - Etessami, Kwiatkowska, and Yannakakis (2007) studied efficient algorithms for multi-objective model checking problems which runs in time polynomial in the size of the MDP.
0.109030 - Clempner and Poznyak (2016c) provided a method based on minimizing the Euclidean distance is proposed for generating a well-distributed Pareto set in multi-objective optimization for a class of ergodic controllable Markov chains.
0.080997 - Clempner (2016) proposed to follow the Karush–Kuhn–Tucker (KKT) optimization approach where the optimality necessary and sufficient conditions are elicited naturally for a Pareto optimal solution in MOMDP.
0.161047 - Clempner and Poznyak (2016a) presented a multi-objective solution of the Pareto front for Markov chains transforming the original multi-objective problem into an equivalent nonlinear programming problem implementing the Lagrange principle.
0.171741 - Clempner and Poznyak (2016d) suggested a novel method for computing the multi-objective problem in the case of a metric state space using the Manhattan distance.
0.081937 - Learning schemes for constrained MCs were presented by Poznyak, Najim, and Gomez-Ramirez (2000), based on the theory of stochastic learning automata.
0.101547 - Pirotta, Parisi, and Restelli (2015) provided an algorithm to exploit a gradient–based approach to optimize the parameters of a function that defines a manifold in the policy parameter space so that the corresponding image in the objective space gets as close as possible to the Pareto frontier.
0.069612 - As well as, Vamplew, Dazeley, Barker, and Kelarev (2009) studied the benefits of employing stochastic policies to MO tasks and examined a particular form of stochastic policy known as a mixture policy proposing two different methods.
0.095628 - Motivation Let us consider a general vector optimization problem as (1) where is the optimization variable, is a proper cone, is the objective function, are the inequality constraint functions, and are the equality constraint functions.
0.096774 - Here the proper cone is used to compare the objective values.
0.086741 - The optimization problem presented in Eq (1) is a convex optimization problem if: i) the objective function f(x) is -convex, ii) the inequality constraint functions gi(x) are convex, and iii) the equality constraint functions hi(x) are affine.
0.076923 - Let us consider the set of achievable objective values of feasible points, The values are to be compared using the inequality meaning that x1 is ‘better than or equal’ in value to x2.
0.107421 - In this sense, if the set has a minimum (optimal point of the problem given in Eq (1)) then there is a feasible point x* such that for all feasible xi.
0.053130 - When an optimization problem has an optimal point it is unique: if x* is an optimal point then f(x*) can be compared to the objective at every other feasible point, and is better than or equal to it.
0.082935 - A point x* is optimal if and only if it is feasible and where is the set of values that are worse than, or equal to, f(x*).
0.144703 - We say that a feasible point x is Pareto optimal (or efficient) if f(x) is a minimal element of the set of achievable values .
0.161153 - In this case we say that f(x) is a Pareto optimal point for the optimization problem given in Eq (1).
0.077922 - Thus, a point x* is Pareto optimal if it is feasible and, for any feasible x, implies .
0.069119 - A point x is Pareto optimal if and only if it is feasible and (2) where is the set of points that are better than or equal to f(x*), i.e., the achievable value better than or equal to f(x*) is f(x*) itself.
0.035088 - An optimization problem can have many Pareto optimal points.
0.107005 - Every Pareto optimal point is an achievable objective value that lies in the boundary of the set of feasible points.
0.104497 - The set of Pareto optimal points satisfies where denotes the boundary of the set of feasible points.
0.134415 - Let us consider the scalar optimization problem (3) The weight vector λ is a free parameter that allows to obtain (possibly) different Pareto optimal solutions of the optimization problem (1).
0.112779 - Geometrically, this means that a point x is optimal for the scalarized problem if it minimizes λ⊺f(x) over the feasible set, if and only if for any feasible xi.
0.076104 - If the optimization problem given in Eq (1) is convex, then the scalarized problem given in Eq (3) is also convex, since λ⊺f(x) is a scalar-valued convex function.
0.107280 - This means that we can find Pareto optimal points of a convex optimization problem by solving a convex scalar optimization problem.
0.070175 - Note that this problem may have non-unique solution.
0.091860 - Tikhonov’s regularization (Tikhonov, Goncharsky, Stepanov, & A.G., 1995; Tikhonov & Arsenin, 1977) is one of the most popular approaches to solve discrete ill-posed of the minimization problem (4) The method looks for establishing an approximation of x by replacing the minimization problem (4) by a penalized problem of the form with a regularization parameter δ > 0.
0.108696 - The term penalizes large values of x, and result in a sensible solution in cases when minimizing the first term only does not.
0.122479 - The parameter δ is computed to obtain the right balance making both, the original objective function and the term small.
0.079394 - We can described the regularization problem as an optimization problem (5) Using the scalarization method to solve the problem given in Eq (5) we have that We have that the function fδ(x) is strictly convex if the Hessian matrix is positive semi-definite or, equivalently, δ should provide the inequality or, equivalently, δ should provide the inequality (6) However, in this case δ should be large enough and it can modify (sometimes significantly) the shape of the original functional.
0.125992 - Main contribution To address this shortcoming, this paper provides a novel approach for solving the multi-objective Markov chains problem using a regularizing poly-linear functions based on a penalty function optimization approach.
0.152486 - This paper makes the following contributions: • Based on the fact that the unconstrained optimization problem has a unique solution only if the functional is strongly convex, we formulate the expected properties for the regularized penalty function for the constrained problem including equality and inequality constraints.
0.288357 - • Immediately, we present the main result on the extremal points of the penalty functions proving that there exists a solution of the original problem with minimal weighted norm which is unique.
0.320023 - • Then, we suggest a projection-gradient algorithm for computing the penalty function.
0.281265 - We prove the convergence of the gradient method and the rate of convergence of the parameters μ, δ and γ.
0.195635 - • In addition, we present a version of the same algorithm without inequality constraints.
0.131231 - • Next, we formulate the MOMCP introducing basic notions on Markov chains and presenting the poly-linear Markov cost functions for justifying why the regularizing poly-linear functions solution approach is necessary.
0.173628 - • We formulate the original problem considering the c-variable method for the introduction of linear constraints over the poly-linear functions in the MOMCP.
0.247625 - • We present a method that make the Pareto frontier more useful as decision support system selecting the ideal multi-objective option given certain bounds (Clempner & Poznyak, 2015).
0.196337 - • Finally, we experimentally validate the method presenting a numerical example of a real alternative solution of the vehicle routing planning problem.
0.120205 - Organization of the paper The remainder of this paper is organized as follows.
0.116777 - The next Section provides the controllable Markov chains theory and the formulation of the MOMCP.
0.103874 - Section 3 presents a decision support method that consists on determining a scalar λ* given specific bounds.
0.102490 - Bounds are restrictions imposed by the decision maker over the Pareto front that establish a specific decision area where the strategies can be selected.
0.139896 - Section 4 presents the penalty function regularized optimization method which balanced the value of the functional using a penalization term (μ) and the regularizator parameter (δ) at the same time improving the computation of the strong Pareto policies.
0.207602 - Section 5 proves the convergence of the gradient regularized penalty algorithm.
0.134567 - Section 6 validates the method presenting a numerical example of a real alternative solution of the vehicle routing planning problem to increase security in transportation of cash and valuables.
0.012821 - Section 7 concludes and discusses future work.
0.088308 - To make the paper more accessible, the long technical proofs are placed in the appendix.
0.096436 - Markov chains Let S be a finite set, called the state space, consisting of finite set of states .
0.088123 - A Stationary Markov chain is a sequence of S-valued random variables s(n), satisfying the Markov condition (Clempner & Poznyak, 2014).
0.074766 - The Markov chain can be represented by a complete graph whose nodes are the states, where each edge (s(i), s(j)) ∈ S2 is labeled by the transition probability.
0.068457 - The matrix determines the evolution of the chain: for each , the power Πk has in each entry (s(i), s(j)) the probability of going from state s(i) to state s(j) in exactly k steps.
0.077882 - Definition 1 A controllable Markov chain (Poznyak et al., 2000) is a 4-tuple (7) where: • S is a finite set of states, ; • A is the set of actions.
0.097561 - For each s ∈ S, A(s) ⊂ A is the non-empty set of admissible actions at state s ∈ S .
0.083894 - Without loss of generality we may take ; • is the set of admissible state-action pairs, which is a measurable subset of S × A; • is a stationary controlled transition matrix, where represents the probability associated with the transition from state s(i) to state s(j) under an action a(k) ∈ A(s(i)), .
0.049844 - Definition 2 A Markov Decision Process is a pair (8) where: • MC is a controllable Markov chain (7) • is a cost function, associating to each state a real value.
0.088573 - The Markov property of the decision process in (8) is said to be fulfilled if The strategy (policy) represents the probability measure associated with the occurrence of an action a(n) from state .
0.060927 - The elements of the transition matrix for the controllable Markov chain can be expressed as Let us denote the collection {d(k|i)(n)} by Dn as follows A policy is said to be local optimal if for each n ≥ 0 it maximizes the conditional mathematical expectation of the utility function under the condition that the history of the process is fixed and can not be changed hereafter, i.e., it realizes the “one-step ahead” conditional optimization rule where is the utility function at the state .
0.126142 - Poly-linear Markov cost function The multi-objective optimization problem for Markov chains is described as follows.
0.088319 - The problem consists of cost-functions (denoted by ) and begins at the initial state sl(0) which (as well as the states further realized by the process) is assumed to be completely measurable.
0.109577 - Each of the cost-functions l is allowed to randomize, with distribution over the pure action choices ∈ and .
0.058480 - From now on, we will consider only stationary strategies .
0.056474 - In the ergodic case when all Markov chains are ergodic for any stationary strategy the distributions exponentially quickly converge to their limits satisfying Each cost-function is depending on the states and actions of all the other cost-functions, is given by the values so that the “average cost function” Jl in the stationary regime can be expressed as where is a matrix with elements (9) satisfying (10) and Notice that by (9) it follows that (11) In the ergodic case for all .
0.058480 - Theindividual aim of each cost-function is 2.3.
0.129975 - Problem formulation Multi-objective optimization is concerned with the problem of optimizing several functions simultaneously.
0.057554 - A precise mathematical statement was given by Pareto (1896); 1897) and Germeyer (1971, 1976).
0.083333 - In the single objective case, an optimum is defined as a policy d* where a given cost-function Jl(cl) assumes its minimum.
0.142625 - In the sense of Pareto, in multi-objective optimization we consider several functions where the minimal optima for a given function is different from the minimal optima of the remaining functions (Germeyer, 1971, 1976).
0.074201 - Definition 3 A multi-objective Markov chain is a tuple (12) where: • MC is a controllable Markov chain (7) • is a vector function whose components are used to define the different cost criteria.
0.139580 - Remark 1 The multi-objective control problem is to find a policy c* that minimizes .
0.120805 - To study the existence of Pareto policies we shall first follow the well-known “scalarization” approach.
0.120805 - Thus, given a n-vector λ > 0 we consider the scalar (or real-valued) cost-function J.
0.088530 - The Pareto set can be defined as (Germeyer, 1971, 1976) (13) such that The Pareto front is defined as the image of under J as follows We consider the usual partial order for n-vectors x and y, the inequality x ≤ y means that xi ≤ yi for all .
0.117155 - We have that A sequence converging to x is said to converge in the direction if there is a sequence of positive numbers in such that in → 0 and Let be a subset of .
0.086785 - The tangent cone to at is the set of all the directions in which some sequence in converges to x.
0.076923 - Definition 4 Let be a subset of .
0.055258 - A vector in is said to be 1. a Pareto point of if there is no such that x < x*; 2. a weak Pareto point of if there is no such that x < <x*; 3. a proper Pareto point of if x* is a Pareto point and, in addition, the tangent cone to at x* does not contain vectors y < 0.
0.056225 - Definition 5 A policy d* is said to be is a Pareto policy (or Pareto optimal) if there is no policy d such that J(d) < J(d*), and similarly for weak or proper Pareto policies.
0.083980 - Let ‖·‖ be the Euclidean norm in and let be the map defined as This is a utility function (or a strongly monotonically increasing function) for the MOMCP in the sense that if d and d′ are such that J(d) < J(d′), then ϱ(d) < ϱ(d′).
0.077325 - Definition 6 A policy d* is said to be strong Pareto optimal (or a strong Pareto policy) if it minimizes the function ϱ that is, Remark 2 As ϱ is a utility function, it is clear that a strong Pareto policy is Pareto optimal, but of course the converse is not true.
0.085661 - The problem is how to compute in order to generate a Pareto front where d is a strong Pareto policy (seeDefinition 6).
0.112608 - The decision support method consists on determining a scalar λ* and the corresponding strategies d*(λ*) given specific min and max bounds that belong to the Pareto front (Clempner & Poznyak, 2015).
0.099843 - Bounds correspond to restrictions imposed by the decision maker over the Pareto front that establish a specific decision area where the strategies can be selected.
0.099904 - The optimal that corresponds to that with minimal distance to the utopian point.
0.080357 - The method is described as follows: Let define the min and max allowed bounds as follows (14) Suppose that these bounds are a priory given as (see Fig 1) Fig 1.
0.057582 - Decision support method: bounds specification.
0.111025 - Let define the optimal that corresponds to the solution of the problem (15) (Jl* corresponds with the utopian point) subject to Lemma 7 The problem(15)formulated above is feasible iff 1.
0.000000 - (16) 2.
0.074074 - (17)⌀ The optimal individual utility is given by As well as, the optimal global utility Let us represent the problem above in the form of a poly-linear function optimization with linear constraints, namely, We will construct the matrix using the ergodicity constraints defined in (10) (18) Then, we have that Developing the formulas and multiplying by for each component we have where is the Kronecker’s delta, then is defined as follows where and is as follows
0.093130 - Poly-linear optimization problem formulation Consider the following poly-linear programming problem (19) Introducing the “slack” vectors with nonnegative components, that is, uj ≥ 0 for all the original problem (19) can be rewritten as (20) Note that this problem may have non-unique solution and .
0.140507 - Define by X*⊆Xadm the set of all solutions of the problem (20).
0.091204 - Penalty functions approach Following Zangwill (1969) and Garcia and Zangwill (1981) consider the penalty function (21) where the parameters k and δ are positive.
0.116838 - Obviously, the unconstraint on x the optimization problem (22) has a unique solution since the optimized function (21) is strongly convex (Poznyak, 2008) if δ > 0.
0.036697 - Note also that where and (23) 4.3.
0.092226 - Expected property of RPFA Proposition 8 If the penalty parameter μ tends to zero by a particular manner, then we may expect that x*(μ, δ) and u*(μ, δ), which are the solutions of the optimization problem tend to the set X*of all solutions of the original optimization problem(20), that is, (24) where ρ{a; X*} is the Hausdorff distance defined as Below we define exactly how the parameters μ and δ should tend to zero to provide the property (24).
0.105025 - The main result on the extremal points of the penalty functions Theorem 9 Let us assume that 1) the bounded set X*of all solutions of the original optimization problem(20)is not empty and the Slater’s condition holds, that is, there exists a point such that (25) 2) The parameters μ and δ are time-varying, i.e., such that (26) Then (27) where x** ∈ X*is the solution of the original problem (20) with the minimal weighted norm which is unique, i.e., (28) and (29) Proof See Appendix A □ We also need the following lemma.
0.098243 - Lemma 10 Under the assumptions of the Theorem above there exist positive constants Cμ and Cδ such that (30) Proof See Appendix A □
0.077236 - The recurrent procedure Consider the following recurrent procedure for finding the extremal point z** = : (31) where and 5.2.
0.141702 - Main result on the convergence of the projection gradient method Theorem 11 convergence of the gradient method If (32) then (33) Proof See Appendix A □ 5.3.
0.098883 - Special selection of the parameters Let us select the parameters of the algorithm (31) as follows: (34) To guarantee the convergence of the suggested procedure, by the property and by the conditions (32) we should have (35) 5.4.
0.151661 - On the rate of the convergence Let us prove the following simple result.
0.072770 - Lemma 12 Suppose that for a nonnegative sequence {un} the following recurrent inequality holds (36) where numerical sequences {αn} and {βn} satisfies (37) Then (38) Proof Lemma 12 For it follows which by the same Theorem 16.14 in Poznyak (2008) implies (38).
0.105072 - □ By (34) and (A.23) we have As the result, and for we get if v ∈ (0, 1] satisfies (39) or, equivalently, v ≤ .
0.095619 - So, the rate of convergence for will be estimated by the following relation This leads to the following conclusion: the best rate of the convergence to zero is defined as (40) where Since ≥ = > 1 we have Under constrains (39) and (35) the maximal upper estimate is achieved when 2γ = = 1 implying γ = ≤ = and .
0.067340 - Finally we get (41) 5.5.
0.118152 - Optimization without inequality constrains Without the constrains of the inequality type we may take and the recurrent algorithm become to be as follows: Conditions to the parameters
0.128999 - This example presents a real alternative solution of the vehicle routing planning problem to increase security in transportation of cash and valuables (see, Beltrami et al., 1995 for multi-objective Markov decision in urban modeling).
0.069444 - Companies in the arena are focused in the physical transfer of cash, jewels, coins, etc.
0.115410 - As a result of the type of the transported goods the security vehicles are frequently exposed to attacks along their routes.
0.063830 - Crime is a significant challenge.
0.077110 - In addition, the risk rates and the losses are different from sector to sector.
0.115852 - A conflict arises because higher risk exposures allow a reduction of the travel cost.
0.186881 - We suggest a bi-objective formulation using the proposed method with the goal of reducing both the risk and the travel cost.
0.125786 - The problem is weighted according to which a maximum amount of valuables can be transported the security vehicle.
0.089907 - It is important to note that each customer must be assigned to exactly one of the routes and the vehicle capacity must not be exceeded.
0.063234 - Assuming that a vehicle picks up cash and valuables at certain places i visited along route, a risk index φ for each criminal l can be defined as follows where is the risk of criminal to attack place i and action k along a given route and, is the maximum risk able to undertaken by a criminal to attack place i.
0.097989 - The risk method assesses the probabilities for the actions of the attackers and it is defined as follows (42) It is interesting to remark that in our case we used i and j as the sequence between two consecutive places (states) that measure the probability of an attack on a specific roadway segment.
0.096220 - In terms of Markov chains we observe the current state i ∈ S. Then, by optimizing the risk using Eq (42) is selected an optimal action .
0.087279 - Then two things happen: a cost Jijk is incurred and, the system at time moves to a new state j ∈ S with probability π(i, j|k).
0.029963 - Let us consider and .
0.090807 - Then, fixing and the Pareto front for 1000 points is shown in Fig 2 which represents the routing plans that should both be safe and efficient.
0.101308 - The Pareto front allows the decision-maker to deal with two critical problems: a) the minimization of the traveled cost-time of a security vehicle and, b) the reduction of the expected exposure of the transported goods to robberies.
0.060606 - This is not a simple assignment.
0.080579 - It involves the conflict between objectives and the difficulty of selecting the routes which implicate to visit and to collect valuables along several places every day.
0.130720 - The corresponding value of the vector λ and the joint strategy for is as follows: Fig 2.
0.024465 - Regularized Pareto front: security routing against vehicle attack.
0.085788 - Route 1 (color cyan) Route 2 (color blue) Route 3 (color red) Route 4 (color green) Route 5 (color magenta) In Fig 3 we show the security routing bounds for decision making over the Pareto front.
0.099843 - Bounds correspond to restrictions imposed by the decision maker over the Pareto front that establish a specific decision area where the strategies can be selected.
0.111952 - By computing the minimum distance to the utopian point using Eq (15) we have that Route 3 (color red) fulfill the requirements.
0.000000 - Fig 3.
0.112559 - Security routing bounds for decision making over the Pareto front.
0.069106 - Many real-world problems involve the optimization of multiple criteria or conflicting objectives often need to be balanced simultaneously.
0.072072 - The most common notion of multi-criterion optimization is that of efficient (or Pareto optimal) solutions, namely those solutions that cannot be improved upon in all coordinates (with strict improvement is at least one coordinate) by another solution.
0.040404 - Unfortunately efficient solutions are non-unique.
0.112735 - In this sense, when dealing with multiple objectives, it is often assumed that the relative importance of the objectives is known a priori.
0.152590 - This paper addressed a novel approach using a gradient method for computing the Pareto frontier in MOMCPs that integrated a regularized penalty method for poly-linear functions restricted to a class of finite, ergodic and controllable Markov chains.
0.086785 - An advantage with other approaches in the field is that the regularizator parameter (δ) allows to find the strong Pareto policies.
0.124916 - Our policy-gradient multi-objective algorithms exploit a gradient-based approach so that the corresponding image in the objective space gets a Pareto frontier of just strong Pareto policies.
0.140351 - A disadvantage is that the computational complexity is increased.
0.208532 - We presented the convergence conditions and computed the estimate rate of convergence of variables μ and δ corresponding to the regularized penalty method.
0.140671 - The resulted equation in this nonlinear system is an optimization problem for which the necessary and efficient condition of a minimum was solved using the projected gradient method.
0.284738 - We proved the convergence of the gradient regularized penalty algorithm.
0.126984 - We also considered the variable method for introducing the equality constraints that ensure the result belongs to the simplex and it satisfies ergodicity constraints.
0.189239 - We also proposed a decision support method for the multi-objective optimization that allows to select the ideal goal between the conflicting objectives.
0.202176 - We experimentally validated the method presenting a numerical example of a real alternative solution of the vehicle routing planning problem.
0.091398 - The vehicle routing planning problem arise in many decision-making situations.
0.117647 - We considered this problem in the context of multiple criteria model that takes into account two types of criteria, which are used in decision maker’s preferences.
0.138165 - When formulating our multiple criteria model we assumed that the decision bounds represent the higher priority area for the decision maker.
0.113537 - While we recognize the existence of other approaches, in this paper we focused on those routing selection problems that match the above structure of decision area where the strategies can be selected.
0.081907 - In terms of future work, there exist a number of challenges left to address.
0.143864 - One interesting technical challenge is that of extending the regularization method for the Lagrange principle.
0.078431 - Moreover, the main future goal is to apply this technique to game theory in terms to ensure the existence of a unique equilibrium point (Clempner & Poznyak, 2016b).
0.104207 - An interesting empirical challenge would be to run a long-term real controlled experiment and evaluate the behavior of the regularized solution proposed in this paper.
0.067511 - Appendix A.
0.081170 - Proofs of Theorems and Lemmas Theorem 9 a) First, let us prove that the Hessian matrix H associated with the penalty function (23) is strictly positive definite for any positive μ and δ, i.e., we prove that for all and (A.1) To prove that, by the Schur lemma (Poznyak, 2008), it is necessary and sufficient to prove that (A.2) We have By the Schur lemma implying, which holds for any δ > 0 by the condition (26) since So, H > 0 which means that the penalty function (23) is strongly convex and, hence, has a unique minimal point defined below as x*(μ, δ) and u*(μ, δ).
0.086430 - b) By the strictly convexity property (A.1) for any and any vector ≔ for the function we have (A.3) Selecting in (A.3)x ≔ x* ∈ X* (x* is one of admissible solutions such that ) and we obtain Dividing both sides of this inequality by δn we get (A.4) Notice also that from (A.3), taking and it follows implying which means that the sequence is bounded.
0.106278 - In view of this and taking into account that by the supposition (26) from (A.4) it follows (A.5) From (A.5) we may conclude that (A.6) and where is a partial limit of the sequence which, obviously, may be not unique.
0.152288 - The vector is also a partial limit of the sequence .
0.108787 - c) Denote by the projection of to the set Xadm, namely, (A.7) and show that (A.8) From (A.6) we have implying where the vector inequality is treated in component-wise sense.
0.089485 - Therefore Introduce the new variable (A.9) where by the Slater condition (25) 0 < νn ≔ < 1.
0.143519 - For new variable we have and therefore In view of that ≤ ≤ which proves (A.8).
0.066847 - d) The last step is to prove the inequality (A.10) From (A.4) we get (A.11) By the strong convexity property we have (see Corollary 21.4 in Poznyak, 2008) ≥ 0 for any which, in view of the property (A.8), implies and Since any polynomial function is Lipschitz continuous on any bounded compact set, we can conclude that 6. which gives = which by (A.11) leads to (A.12) Dividing both side of the inequality (A.12) by and in view (A.8) we finally obtain (A.13) This, by (26), for n → ∞ leads to (A.10).
0.098492 - Finally, for any x* ≤ X* it implies This inequality exactly represents the necessary and sufficient condition that the point x* is the minimum point of the function on the set X*.
0.079049 - Obliviously, this point is unique and has a minimal norm among all possible partial limits .
0.039683 - Theorem is proven.
0.067341 - □ Lemma 10 The necessary and sufficient conditions for the points to the extremal points of the function are as follows: (A.14) where is the Lagrange function for the problem (22) defined for λn, x(i) ≥ 0, λu, x(j) ≥ 0 as Multiplying the first equation in (A.14) by and the second one by in view of the complementary slackness conditions we derive implying (A.15) By the construction of the regularized penalty function it follows that (A.16) Indeed, if it is not the case, then can not be the optimal point since the function is more than its value when In view of this, the identity (A.15) is equal to (A.17) implying and The last identity can be represented as (A.18) where = 1 and Notice that in the last identity, by the boundedness property of Xadm, ≤ c = const and the matrix are of the following structure where adj is the matrix adjoined to A and Aji is the cofactor to the element aij.
0.026882 - Since this matrix is nonsingular it follows that aij∣k ≠ 0.
0.088465 - The matrix has the same structure, namely, In view of that the vector (A.18) has the following structure (A.19) where and The structure (A.19) together with the equality (A.16) directly implies (30).
0.096573 - □ Theorem 11 In view of (31) it follows (A.20) By the inequalities (see the inequalities (21.17) and (21.36) in Poznyak, 2008) we can conclude that ≤ + where .
0.083158 - We also have Then, in view of Lemma (10), from (A.20) for we obtain or, equivalently, (A.21) where (A.22) Using the inequality ≤ + r ∈ (0, 1), θn > 0 for and = , ρ ∈ (0, 1), the inequality (A.21) can be reduced to the following one (A.23) By Theorem 16.14 in Poznyak (2008) if which is equivalent to (33).
0.039683 - Theorem is proven.
0.000000 - □

[Frase 57] • Immediately, we present the main result on the extremal points of the penalty functions proving that there exists a solution of the original problem with minimal weighted norm which is unique.
[Frase 2] In addition, we present a method that make the Pareto frontier more useful as decision support system: it selects the ideal multi-objective option given certain bounds.
[Frase 56] This paper makes the following contributions: • Based on the fact that the unconstrained optimization problem has a unique solution only if the functional is strongly convex, we formulate the expected properties for the regularized penalty function for the constrained problem including equality and inequality constraints.
[Frase 1] In this paper, we present a novel approach for computing the Pareto frontier in Multi-Objective Markov Chains Problems (MOMCPs) that integrates a regularized penalty method for poly-linear functions.
