We use decision tree models to classify patients according to their criticality. Various data preprocessing and modeling activities are carried out to reduce features from data set. Different decision tree models are generated by considering equal and unequal instance distribution from classes. Performance improves if we consider equal distribution of instances from each class.

0.170803 - In this research we have used decision tree induction algorithm on Hospital Surveillance data to classify admitted patients according to their critical condition.
0.098361 - Three class labels, low, medium and high, are used to distinguish the criticality of the admitted patients.
0.184957 - Several decision tree models are developed, evaluated, and compared with different performance metrics.
0.065957 - Finally an efficient classifier is developed to classify records and make decision/predictions on some input parameters.
0.052910 - The models developed in this research could be helpful during epidemic when huge number of patients arrive daily.
0.064677 - Due to rush of duty doctors and scarcity of required number of physicians, it is hard to diagnose every patient.
0.054902 - Any computer application could be helpful to diagnose and measure the criticality of the newly arrived patient with the help of the historical data kept in the surveillance database.
0.037453 - The application would ask few questions on physical condition and on history of disease of the patient and accordingly determines the critical condition of the patient as low, medium or high.
0.032129 - A diarrhoeal disease surveillance system was established at ICDDR,B, Dhaka, Bangladesh (ICDDR,B, 2008) in 1979 and extended to the Matlab hospital at Comilla, Bangladesh in 2003.
0.050314 - The surveillance system collects information on clinical, epidemiological and demographic characteristics of patients.
0.053872 - A systematic 2% sub-sample of patients attending Clinical Research and Service Centre (CRSC) and all patients from the Health and Demographic Surveillance System (HDSS) area attending the Matlab hospital are enrolled into the surveillance program.
0.043614 - Trained personnel interview the patients and/or their attendants to obtain information on socioeconomic and demographic characteristics, housing and environmental conditions, feeding practices, particularly among infants and young children, and on the use of drugs and fluid therapy at home.
0.062147 - Clinical characteristics, anthropometric measurements, treatments received at the facility, and outcomes of patients are also recorded.
0.080169 - Extensive microbiological assessments of faecal samples (microscopy, culture, and ELISA) of patients are performed to identify diarrhoeal pathogens and to determine antimicrobial susceptibility of bacterial pathogens.
0.042424 - The program stores important information that helps hospital clinicians to provide care to patients.
0.038760 - It also enables the centre to detect the emergence of new pathogens and responds to early identification of outbreaks and their locations to suggest the Government of Bangladesh to take preventive and other control measures and to monitor the changes in the characteristics of patients and antimicrobial susceptibility of bacterial pathogens.
0.016260 - Collected information is representative of the population.
0.038596 - Hence, it provides an important data repository for conducting epidemiological studies, validation of results of clinical studies, helps develop new research ideas and study design, and introduce improved patient-care strategies and preventive programs.
0.073830 - Motivation When patients arrive at hospital, an initial diagnosis is carried out by the duty physician to find out the criticality of the patients condition.
0.070617 - According to the criticality of patient’s physical condition the duty doctor takes necessary action.
0.038348 - Sometimes when there is epidemic like in the year 2008 during flood, when 1000 patient on an average admitted in the hospital, it is hard to diagnose every patient satisfactorily due to rush of duty doctors and scarcity of required number of physicians.
0.053640 - Any computer application could be helpful here to diagnose and measure the criticality of the newly arrived patient with the help of the historical data kept in the surveillance database.
0.037453 - The application would ask few questions on physical condition and on history of disease of the patient and accordingly determines the critical condition of the patient as low, medium or high.
0.028070 - Objective Hospital Surveillance database contain records that store patient’s initial physical condition, his/her history of disease, as well the final outcome of treatment and duration of his/her stay in the hospital.
0.000000 - The outcome field has the following values stored: 1 = Cured, 2 = Illness cont, 3 = Died, 4 = Absconded, 5 = Others, 9 = Unknown.
0.064460 - In this research, we consider only the records of the patients with outcome = 1 as records with other outcome (outcome < > 1) are incomplete in most of the cases.
0.029851 - However, we do not find any attribute field that stores the initial diagnosis for “Criticality” with value: low, mid, high.
0.077592 - We use an intuition here that the duration of stay in the hospital would indicate to the criticality of the patient.
0.061728 - We create a derived attribute “Criticality” by banning the duration of stay field as follows: 0 to ⩽48 h: Low, 48> to ⩽96 h: Mid, >96 High.
0.166197 - Contribution In this research we have used decision tree induction algorithm on Hospital Surveillance data to classify admitted patients according to their critical condition.
0.184957 - Several decision tree models are developed, evaluated, and compared with different performance metrics.
0.065957 - Finally an efficient classifier is developed to classify records and make decision/predictions on some input parameters.
0.070423 - Several research works have been conducted to identify new, unexpected and interesting patterns from hospital infection control and public health surveillance data.
0.036145 - In Brossette (1998), Ma, Tsui, Hogan, Wagner, and Ma (2003) and Moser, Jones, and Brossette (1999) data mining techniques are used for monitoring emerging infections and antimicrobial resistance.
0.036364 - The monitoring and detection of nosocomial infections is a important problem arising in hospitals.
0.028070 - A hospital-acquired or nosocomial infection is a disease that develops after admission into the hospital and it is the consequence of a treatment, not necessarily a surgical one, performed by the medical staff.
0.060109 - Nosocomial infections are dangerous because they are caused by bacteria which have dangerous (critical) resistance to antibiotics.
0.000000 - This problem is very serious all over the world.
0.019900 - In order to support them in this complex task, a system have been developed, called MERCURIO (Lamma et al., 2006).
0.053140 - The objectives of this system are the validation of microbiological data and the creation of a real time epidemiological information system.
0.031175 - The system is useful for laboratory physicians, because it supports them in the execution of the microbiological analyses; for clinicians, because it supports them in the definition of the prophylaxis, of the most suitable antibiotic therapy and in monitoring patients’ infections; and for epidemiologists, because it allows them to identify outbreaks and to study infection dynamics.
0.039216 - In order to achieve these objectives data mining techniques have been adopted.
0.026144 - Data mining techniques have been used for improving the system knowledge base.
0.047619 - In order to verify the reliability of the tasks performed by MERCURIO and the usefulness of the knowledge discovery approach, a test was performed based on a dataset of real infection events.
0.028986 - In the validation task MERCURIO achieved an accuracy of 98.5%, a sensitivity of 98.5% and a specificity of 99%.
0.024242 - In the therapy suggestion task, MERCURIO achieved very high accuracy and specificity as well.
0.045977 - Patterns embedded in large volumes of clinical data may provide important insights into the characteristics of patients or care delivery processes, but may be difficult to identify by traditional means.
0.080550 - Research have been carried out to develop method using data mining that can recognize patterns in these large data sets.
0.048889 - In Brossette and Hymel (2008), examples have been presented of this capability in which, data mining has been successfully applied to hospital infection control.
0.066667 - The Data Mining Surveillance System (DMSS) uses data from the clinical laboratory and hospital information systems to create association rules linking patients, sample types, locations, organisms, and antibiotic susceptibilities.
0.024691 - Changes in association strength over time, signal epidemiologic patterns potentially appropriate for follow-up, and additional heuristic methods identify the most informative of these patterns for alerting.
0.058608 - Data mining is the process of discovering interesting knowledge, such as patterns, associations, changes, anomalies and significant structures, from large amounts of data stored in databases, data warehouses, or other information repositories.
0.063670 - In Healthcare, association rules are considered to be quite useful as they offer the possibility to conduct intelligent diagnosis and extract invaluable information and build important knowledge bases quickly and automatically.
0.045977 - The problem of identifying new, unexpected and interesting patterns in medical databases in general, and diabetic data repositories in specific, is considered in a research (Stilou, Bamidis, Maglaveras, & Pappas, 2001).
0.086580 - In this paper the apriori algorithm to a database containing records of diabetic patients and attempted to extract association rules from the stored real parameters.
0.050228 - The results indicate that the methodology followed may be of good value to the diagnostic procedure, especially when large data volumes are involved.
0.031746 - The followed process and the implemented system offer an efficient and effective tool in the management of diabetes.
0.033755 - Another research paper (Houston et al., 1999) discusses several data mining algorithms and techniques that have been developed at the University of Arizona Artificial Intelligence Lab.
0.033670 - In this paper those algorithms and techniques are implemented into several prototypes, one of which focuses on medical information developed in cooperation with the National Cancer Institute (NCI) and the University of Illinois at Urbana-Champaign.
0.053333 - An architecture have been proposed for medical knowledge information systems that will permit data mining across several medical information sources and discuss a suite of data mining tools that have been developed to assist NCI in improving public access to and use of their existing vast cancer information collections.
0.068273 - Research work has also been done to demonstrate and test usefulness and performance of data mining tools and techniques being applied to academic research conducted on Asthma patients.
0.000000 - (Bereznicki et al., 2008; Tseng, Chao-Hui, Lee, & Chia-Yu Chen, 2008).
0.010582 - The prediction of survival of Coronary Heart Disease (CHD) has been a challenging research problem for medical society.
0.053640 - The objective of research presented in Xing Yanwei, Jie Wang, and Zhihong Zhao (2007) was to develop data mining algorithms for predicting survival of CHD patients based on 1000 cases.
0.077270 - A clinical observation and a 6-month follow up were carried out to include 1000 CHD cases.
0.027211 - The survival information of each case is obtained via follow up.
0.054726 - Based on the data, three popular data mining algorithms were employed to develop the prediction models using the 502 cases.
0.051643 - Also 10-fold cross-validation methods were used to measure the unbiased estimate of the three prediction models for performance comparison purposes.
0.083107 - The results indicated that the SVM is the best predictor with 92.1% accuracy on the holdout sample artificial neural networks came out to be the second with 91.0% accuracy and the decision trees models came out to be the worst of the three with 89.6% accuracy.
0.049808 - The comparative study of multiple prediction models for survival of CHD patients along with a 10-fold cross-validation provided an insight into the relative prediction ability of different data.
0.034014 - Research have been done to develop an artificial intelligence-based data mining engine (CureHunter) (CureHunter- precision medical data mining, 2008) that can autonomously search all the known biomedical research journals, collate the published drug efficacy evidence for specific diseases and present it in a format that is available in real-time (10–20 s) for patients and physicians to review.
0.041298 - With integration into existing physician record management systems, physicians can use (free of charge) the drug research interface and obtain up-to-date summarized clinical effectiveness information on a wide range of drugs and diseases while the patient is sitting in the room!
0.042254 - For patients, it tries to answer the question: what does the scientific community think the best treatment options for disease Y are?
0.033816 - Patients simply need to enter the disease they wish to know more about in the search box on the front page.
0.043360 - They utilize the Mesh-based ontological terms to help narrow their search down to the specific disease they are searching for and CureHunter returns to them: (a) Key Drugs and Agents for the treatment of that disease, (b) Other Related Diseases, and (c) Key Therapies for that disease.
0.035294 - When one thinks how difficult it would be for physicians to realistically and comprehensively review the literature on all drugs they prescribe, this kind of engine has significant potential.
0.039216 - The data mining engine has been in development for a few years now, and after two prototyping iterations was released to the public as a beta in July 2007.
0.020513 - The CureHunter Corporation is still at early startup stage and is still seeking venture funding and/or a partner.
0.052941 - In a data mining research paper (Buntinx, Truyen, Embrechts, Moreel, & Peeters, 1992) on medical records, data have been collected on 320 patients complaining to their general practitioner of a new episode of chest pain, discomfort or oppression.
0.046948 - Relationships were examined between initial signs and symptoms and a follow-up diagnosis after a period of 2 weeks to 2 months.
0.039216 - The data were analysed with CART, a statistical decision theory software package.
0.026144 - In the first run, the number of misclassifications by CART was 56%.
0.052288 - After regrouping of the data and diagnostic categories, there were 37% misclassifications.
0.048224 - The most discriminating variable turned out to be pain on palpation.
0.031153 - When comparing each of five diagnostic groups to all others, it was found that a positive predictive value of 27% for gastrointestinal diseases, 72% for cardiovascular disorders, 69% for respiratory diseases, 58% for psychopathology and 73% for chest wall pathology.
0.026455 - The CART methodology needs further investigation and testing before any clinical application will be possible in general practice.
0.067624 - In Mair, Smidt, Lechleitner, Dienstl, and Puschendorf (1995) a study was carried out to find an accurate algorithm for the diagnosis of acute myocardial infarction in nontraumatic chest pain patients on presentation to the emergency department.
0.030303 - In a prospective clinical study, the diagnostic performances of clinical symptoms were compared, presenting ECG, creatinine kinase, creatine kinase MB activity and mass concentration, myoglobin, and cardiac troponin T test results of hospital admission blood samples.
0.088199 - By classification and regression trees, a decision tree for the diagnosis of acute myocardial infarction was developed.
0.024793 - The research was conducted at Emergency room of a Department of Internal Medicine (University Hospital) on 114 nontraumatic chest pain patients: 26 Q-wave and 19 non-Q-wave myocardial infarctions, 49 patients with unstable angina pectoris, and 20 patients with chest pain caused by other diseases.
0.048889 - In this study an algorithm was found that could accurately separate the myocardial infarction patients from the others on admission to the emergency department.
0.021858 - Therefore, this classifier could be a valuable diagnostic aid for rapid confirmation of a suspected myocardial infarction.
0.038961 - In another research paper (Hadzikadic et al., 1995) two classification models were presented, one based on concept formation and the other using standard logistic regression.
0.068783 - The models were first explained in some detail and then evaluated on the same population of trauma patients.
0.050314 - The goal of both systems is to predict the outcome of those patients.
0.087719 - The results are summarized and explained in terms of differing algorithms of the two models.
0.038647 - We could solve a classification problem by asking a series of carefully crafted questions about the attributes of the test record.
0.044444 - Each time we receive an answer, a follow-up question is asked until we reach a conclusion about the class label of the record.
0.067054 - The series of questions and their possible answers can be organized in the form of a decision tree, which is a hierarchical structure consisting of nodes and directed edges.
0.070209 - Classifying a test record is straightforward once a decision tree has been constructed.
0.069264 - Starting from the root node, we apply the test condition to the record and follow the appropriate branch based on the outcome of the test.
0.028169 - This will lead us either to another internal node, for which a new test condition is applied, or to a leaf node.
0.048485 - The class label associated with the leaf node is then assigned to the record.
0.061672 - Decision tree induction algorithm TDIDT (Top-Down Induction of Decision Tree) algorithm has formed the basis for many classification systems, two of the best known being ID3 and c4.5.
0.146016 - Decision trees are generated by repeatedly splitting on the values of attributes.
0.000000 - This process is known as recursive partitioning.
0.053073 - In the standard formulation of the TDIDT algorithm there is a training set of instances.
0.089947 - Each instance belongs to an object class, which is described by the values of a set of attributes.
0.000000 - The basic algorithm can be given in just a few lines as shown in Fig 1.
0.000000 - The TDIDT algorithm Fig 1.
0.000000 - The TDIDT algorithm.
0.013605 - At each non-leaf node an attribute is chosen for splitting.
0.000000 - This could be any attribute, except that the same attribute must not be chosen twice in the same branch.
0.000000 - However this harmless restriction has a very valuable effect.
0.028070 - Each split on the value of an attribute extends the length of the corresponding branch by one term, but the maximum possible length for a branch is M terms where there are M attributes.
0.032520 - Hence the algorithm is guaranteed to terminate.
0.000000 - There is one important condition which must hold before the TDIDT algorithm can be applied.
0.069652 - This is the adequacy condition: no two instances with the same values of all attributes may belong to different classes.
0.026144 - This is simply a way of ensuring that training set is consistent.
0.016064 - A major problem with the TDIDT algorithm is that it is underspecified: no method is given to select the attribute A on which the split will be done.
0.032520 - There are some techniques for selecting attributes.
0.014184 - However, some of them may be more useful than others.
0.048309 - Making a good choice of attributes to split on at each stage is crucial to the success of the TDIDT approach.
0.045307 - Choosing the attributes To know the order in which attributes much be chosen to split the data, we need some measure that would allow us to compare the attributes on some scale and choose one above the other.
0.035556 - One of the measures for selecting the “best” question or attribute is based on the level of Impurity in the resulting classes of data.
0.043290 - Impurity could be defined as the amount of uncertainty present in the data and that the attribute which reduces the impurity most should be chosen.
0.032125 - Given probability p, some of the impurity measures are: • Gini Index: 2p (1 − p) • Entropy: −[p log p + (1 − p) log (1 − p)] • Misclassification Rate: 1 − max(p, 1 − p) In general, when the dataset could be divided into two classes, then p is the proportion of instances in the database that has one value for the target attribute and 1 − p is the proportion of instances in the database that has the second value for the same target attribute.
0.053140 - Generalization of the impurity measures In the previous section, we assumed the instances in the database could be classified two-ways.
0.040816 - When the number of classes becomes three or more i.e.
0.021645 - C1, C2 and C3, where • P(C1) = p • P(C2) = q • P(C3) = 1 − p − q then the impurity measures could be generalized as follows.
0.000000 - • Gini Index • Entropy Or • Misclassification Rate 3.3.
0.042534 - Using gain ratio for attribute selection Whatever formula we use for the task of attribute selection, introduces an inductive bias, i.e.
0.013468 - a preference for one choice rather than other, which is not determined by purely by the data itself but by external forces, such as our preference for simplicity or familiarity with something (Hadzikadic et al., 1995).
0.000000 - Such bias can be helpful or harmful, depending on the dataset.
0.020513 - We can choose a method that has a bias that we favor, but we cannot eliminate inductive bias altogether.
0.000000 - There is no neutral, unbiased method.
0.051282 - Clearly it is important to be able to say what is introduced by any particular method of selecting attributes.
0.041026 - For many methods this is not easy to do, but for one of the best-known methods we can.
0.031746 - Using entropy can be shown to have a bias towards selecting attributes with a large number of values.
0.059803 - In order to reduce effect of bias resulting from the use of information gain, a variant known as Gain Ratio was introduced by the Australian academic Ross Quinlan in his influential system C4.5.
0.049751 - Gain Ratio adjusts the information gain for each attribute to allow for the breath and uniformity of the attribute values.
0.010582 - Gain Ratio is defined by the formula: where Split Information is a value based on the column sums.
0.081967 - Hospital surveillance unit of ICDDR,B keeps surveillance data and data related to patient in SPSS software.
0.070922 - Different types of data are merged into a single file.
0.034074 - From observing the data it is evident that the variables in the dataset could be divided into following 5 groups: Group 1: Social and Behavioral Data: data related to patient’s biological information, economic condition, living condition, his/her education level, habits and practices related to hygiene, social and behavioral attributes, Group 2: Patient History and Primary Diagnosis: health condition of the patient when admitted and any history of previous disease, Group 3: Pathology Reports: diagnosis report of blood, stool and other check-up, Group 4: Antibiogram: isolated pathogens and their sensitivity analysis to different antibiotics, Group 5: Treatment and outcome: treatment, duration of stay and outcome of treatment is also stored in this data set.
0.023392 - These five groups cover the main division of the variables in the Hospital Surveillance data.
0.103909 - Our aim of the study is to classify the patient according to their duration of stay in the hospital.
0.044776 - We assumed that the higher the duration of stay the more critical is the condition of the patient (a-priory).
0.082423 - So we have created a derived variable from duration of stay as criticality (low, medium, high) in the data preprocessing phase.
0.069997 - According to the objective set by us to find out the criticality of the patient on admission we have selected the predictor variables relevant for the study from the groups: Social and Behavioral Data, Patient History and Primary Diagnosis, and the target variable criticality is a derivative variable from the “Duration of Stay” variable in the group: Treatment.
0.014184 - Hospital Surveillance unit has provided us data in SPSS format.
0.000000 - • It has 26,869 records.
0.076023 - • It has data from 1st January 1996 to 31st December 2007, about 12 years’ data.
0.000000 - • It has about 227 attributes/variables.
0.089390 - From our observation we have divided the variables in five major groups which is already been discussed, according to their capture time and characteristics.
0.075591 - Discussing with experts of surveillance unit who have the necessary domain knowledge, according to relevance we have dropped some variables and selected some variable for our study for preprocessing, cleaning, transformation and modeling.
0.064460 - Not all records in the group 1, group 2 and group 5 are not kept in the, fields are kept according to relevance and importance to the study.
0.071582 - These fields are further reduced in the data preprocessing phase.
0.036530 - Through data survey using statistical tools some of these selected fields are further dropped for lack of variance or lack of valuable information.
0.061224 - Using domain knowledge the variable count decreased from 227 to 40.
0.073838 - Frequency Distribution, histogram of the variables and statistical information related to the variables in the selected data set is taken to have a quick understanding about the information content and quality of the data.
0.070782 - We observe that there are missing values in the data and there are skewness and sparsity in some variables, which are dealt in the next section named data preprocessing.
0.017094 - All the records have numerical data.
0.038251 - But most of the records have 37 attributes (out of 40) are categorical in nature i.e.
0.035398 - they contain few distinct integer values, which corresponds to some categorical values, Like variable “outcome” it has 6 distinct integer values and each integer value correspond to a categorical value (1 = Cured, 2 = Illness continued, 3 = Died, 4 = Absconded, 5 = Others, 9 = Unknown).
0.000000 - Remaining 3 attributes have continuous values – durstady, durstahr, agemm.
0.037559 - Data mining is about working with data, which to a greater or lesser degree reflects some real-world activity, event, or object.
0.048309 - Data need to be prepared so that the information enfolded within it is most easily accessed by the data mining tools.
0.057117 - Preparation of data is not a process that can be carried out blindly.
0.052910 - There is no automatic tool that can be pointed at a dataset and told to “fix” the data.
0.013072 - There will remain as much art as science in good data preparation.
0.040404 - Because data preparation techniques cannot be completely automated, it is necessary to apply them with knowledge of their effect on the data being prepared (Pyle Dorian, Data Preparation for Data Mining, Morgan Kaufman, & CD-ROM, 1999).
0.070016 - SPSS software is used from step 1 to 14 and from step 15 to 19 both SPSS and Excel is used to find out missing values, empty values, misclassification error, field transformation, dimensionality reduction i.e., overall data cleaning activities are carried out in the following steps using SPSS and Excel.
0.014815 - Step 1: Data is filtered on the filed “outcome”.
0.000000 - Only record with “outcome” value = 1/2/3 is taken.
0.036036 - Depth of the dataset reduces.
0.000000 - Record count is 25,305.
0.032864 - This is done because by observation it is realized that in cases of records with outcome value = 3/4 records are incomplete.
0.014184 - Step 2: After this filtering activity “outcome” field is deleted.
0.000000 - So the record width decreases.
0.014815 - The number of attributes in the dataset becomes 39.
0.071422 - Step 3: Frequency distribution of “agemm” is taken to find out if there is any missing value.
0.054645 - We found two records with missing values Most of the fields in these records are found blank.
0.000000 - So these two records deleted.
0.000000 - Now record count is 25,303.
0.034548 - Step 4: to reduce sparsity in the variable in “agemm” a new variable “Age” is created from “agemm” – banning “agemm” values in the following way: 0–⩽12: 1 (infants) >12–⩽60: 2 (early childhood) >60–⩽120: 3 (later childhood) >120–⩽180: 4 (adolescent) 180>–⩽720: 5 (adult) >720: 6 (old) “Age” has six discrete value from 1 to 6.
0.059701 - “agemm” is deleted; step 4 and 5 compounded a transformation of continuous variable to a discrete variable: agemm to age.
0.039216 - Step 5: “DurationOfStay” a new field is created form this formula = (durstdy ∗ 24) + dursthr Step 6: “durstdy” and “dursthr” these two fields are deleted; so data width is reduced.
0.000000 - Variable count in the training dataset is now 37.
0.026820 - Step 7: “DurationOfStay” field found empty in 17 instances, so those records are deleted, since our target variable would be created based on this variable this field cannot be empty.
0.000000 - Now record count is 25,286.
0.027491 - Step 8: A new variable “Criticality” is created from “DurationOfStay” – banning “DurationOfStay” in the following way: 0–⩽48: 1 (Low) >48–⩽96: 2 (Mid) >96: 3 (High) Criticality is the target variable in the record.
0.063830 - It has three discrete value ranging from 1 to 3.
0.000000 - Step 9: “DurationOfStay” is deleted.
0.028169 - One variable deleted and in the previous step one variable is created, so overall dimensionality of the dataset remains 37 as before.
0.071582 - Step 10: Frequency distribution of all the variables are taken.
0.040816 - Found that in five records many of the variables are empty.
0.038095 - These records are deleted.
0.000000 - Now record count becomes 25,281.
0.071362 - Step 11: Frequency distribution of the variables are taken again to find out if there is any empty values in any of the variables.
0.031008 - Many variables found to have one missing value.
0.031008 - All these missing values are in two records.
0.036036 - These two records are deleted.
0.000000 - Record count is now 25,279.
0.074797 - Step 12: Frequency distribution of the variables are taken.
0.014184 - No missing value is found for any of the variables.
0.027211 - In some variable some values are found unlabeled, which is deleted.
0.000000 - Record count is now 25,262 in the dataset.
0.027211 - Step 13: Frequency distribution table is taken again for all variables.
0.013605 - Found that a value 6 entered in the “Criticality” field wrongly.
0.000000 - Reason may be unwanted press in the keyboard.
0.000000 - That record is deleted.
0.000000 - Record count is now 25,261.
0.027211 - Step 14: Frequency distribution table is taken again for all variables.
0.000000 - Found no missing or empty value.
0.047619 - All missing and empty values and unlabeled values are dealt with.
0.037736 - Not all attributes may be needed to solve a given data mining problem.
0.031746 - In fact, the use of some attributes may interfere with the correct completion of a data mining task.
0.043716 - The use of other attributes may simply increase the complexity and decrease the efficiency of an algorithm.
0.052209 - This problem is sometimes referred to as dimensionality curse, meaning that there are many attributes (dimensions) involved and it is difficult to determine which ones should be used.
0.055042 - One solution to this high dimensionality problem is to reduce the number of attributes, which is known as dimensionality reduction.
0.038961 - Reducing the number of attributes There are several ways in which the number of attributes (or ‘features’) can be reduced before a dataset is processed.
0.036530 - There are many possible criteria that can be used for determining which attributes to retain, for example: ∘ Only retain the best 20 attributes.
0.016260 - ∘ Only retain best 25% of the attributes.
0.009950 - ∘ Only retain attributes with an information gain that is at least 25% of the highest information gain of any attribute.
0.035088 - ∘ Only retain attribute that reduce the initial entropy of the dataset by at least 10%.
0.007843 - There is no one choice that is best in all situations, but analyzing the information gain values of all the attributes can help make a good choice (Max, 2007).
0.071111 - In the following two steps we have reduced attributes from 40 to 22 by using Entropy calculation for attribute selection based on information gain.
0.053073 - Step 15: Frequency distribution of “Criticality” fields is taken for calculation of initial entropy Estart.
0.070563 - Table 1 contains the frequency distribution of criticality.
0.013072 - Entropy Estart of the dataset is calculated using the formula Table 1.
0.077900 - Frequency distribution of target variable “Criticality”.
0.023423 - Criticality of patient condition Frequency Percent Valid percent Cumulative percent Valid Low 21,510 85.2 85.2 85.2 Mid 2,549 10.1 10.1 95.2 High 1,202 4.8 4.8 100.0 Total 25,261 100.0 100.0 Step 16: Cross-Tab of all predictor variables with the target variable “Criticality” is taken for calculation of Entropy Enew, the average entropy of the training sets resulting from splitting on a specific attribute.
0.033755 - For an Example, cross tabulation of attribute “Sex” is shown with target variable “Criticality” in Table 2 and calculation of Enew for “sex” is also shown.
0.000000 - Table 2.
0.020202 - Crosstab Criticality × Sex.
0.020997 - Count Sex of patients Total Male Female Criticality of Patient Condition ∗ Sex of patients Crosstabulation Criticality of Patient Condition Low 12,573 8937 21,510 Mid 1521 1028 2549 High 707 495 1202 Total 14,801 10,460 25,261 Enew is now calculated by forming a sum as follows.
0.011299 - (a) For every non-zero value V in the main body of the table (i.e.
0.000000 - the part which is colored Vanilla), subtract (V × log2V).
0.000000 - (b) For every non-zero value S in the column sum row (i.e.
0.089517 - the green part), add S × log2S Finally divide the total by number of instances N. Example For attribute “Sex”, Variables with information gain greater than 0.5% are kept for data mining model building: decision tree generation.
0.098039 - In this way we reduce the variables from 37 to 22 variables.
0.031008 - Step 17: Other variables are discarded i.e.
0.057143 - dropped from the dataset.
0.000000 - Dimensionality i.e.
0.032520 - the width of the dataset is reduced.
0.011299 - The count of the records in this dataset is finally 25,261 with variable count 22.
0.062222 - Step 18: This data in SPSS is converted to Excel format to make it ready for the next phase i.e., for Data modeling.
0.046784 - Step 19: Criticality field has discrete data values 1, 2, 3 of integer data type.
0.053498 - They are converted categorical values of strings data type by using the Excel formula: = IF (“Criticality” = 1, “Low”, IF (“Criticality” = 2, “Mid”, IF (“Criticality” = 3, “High”, “Default”))).
0.071795 - This conversion is done because most of the data modeling tools [Sipina, Tanagra] need target values to be categorical.
0.160247 - Data modeling In this research work we use SIPINA (SIPINA, 2008) for generating decision tree model.
0.019608 - SIPINA is a Data Mining Software which implements various supervised learning paradigms.
0.069738 - Modeling activities Step 1: At first all the records in the surveillance dataset that was cleaned in the data preprocessing phase are used in SIPINA.
0.105688 - Step 2: Used C4.5 decision tree classification algorithms for model building.
0.070175 - In the dataset Criticality is the target class and other 21 variables are predictor variables.
0.012579 - A snapshot of C4.5 algorithm in Sipina is presented in Fig 2.
0.000000 - Selecting the C4 Fig 2.
0.000000 - Selecting the C4.5 algorithm.
0.035813 - Step 3: About C4.5 the following is stated in SIPINA website (SIPINA, 2008): It implements mainly two key ideas: gain ratio in order to select the right split attribute; post-pruning according the pessimistic error criterion in order to detect the right size of the tree.
0.009132 - Parameters CL (Confidence level) for pessimistic pruning: It is the confidence level used for the computation of the pessimistic error rate i.e.
0.012121 - the upper bound of the confidence interval of the error rate on a leaf.
0.013333 - Size of leaves: A split is accepted if two leaves at least have size upper than this threshold (Quinlan, 1993; Kohavi et al., 2002).
0.031008 - Step 4: Two parameters need to be passed.
0.018018 - Default value of C.L.
0.044444 - for pessimistic pruning = 25% and Size of Leaves = 2.
0.031008 - These values are kept in the first run.
0.000000 - Selecting those parameters through Sipina is presented in Fig 3.
0.000000 - SIPINA C4 Fig 3.
0.000000 - SIPINA C4.5 passing parameters.
0.031153 - Confidence level is used for prediction of tree error rates and affects the pruning process, the lower the confidence level, the higher the amount of pruning that will take place (Jelena Pješivac-Grbovi’c, Thara Angskun, & George Bosilca, 2007).
0.138146 - These two parameters are changed and adjusted over and over to get an optimal decision tree model.
0.000000 - Step 5: For Sampling default value is “All dataset”.
0.023392 - The sampling selection is kept in its default value: all records are taken for experiment.
0.132756 - The result from the runs: the generated decision tree models are all evaluated and compared for performance in the next section, in the Evaluation and Implementation phase.
0.042900 - For evaluation purpose, we use dataset that have been processed in the previous stage.
0.149672 - We build decision trees on two data sets.
0.068273 - First dataset contain all records from database and the other contains selected records from database such that all classes (high, low and mid) have equal number of records.
0.105552 - Model 1 (all instances from Dataset 1 used in classification) At first we use the dataset that have been processed in preprocessing step.
0.076542 - The following parameters are set in Sipina to generate a Decision Tree model:- Learning Method: C4.5 (Quinlan – 1993) Confidence Level = 25 Leaf size = 2 Sampling = 0 (All Dataset) General information about the tree (Fig 4): Nodes: 29 Leaves: 15 Max Depth: 7 Decision tree generated by C4 Fig 4.
0.131136 - Decision tree generated by C4.5.
0.100191 - 15 decision rules are generated.
0.035088 - For Class variable criticality the Confusion Matrix for Dataset 1 is given in Table 3.
0.000000 - Table 3.
0.000000 - Confusion Matrix for Dataset 1.
0.006116 - Actual Classified High Mid Low High 118 (a) 2 (b) 1082 (c) Mid 77 (d) 3 (e) 2469 (f) Low 51 (g) 0 (h) 21459 (i) The accuracy (AC) is the proportion of the total number of predictions that were correct.
0.011673 - It is determined using the equation: (1) Here, AC = 85.43 % The recall or true positive rate (TP) is the proportion of positive cases that were correctly identified, as calculated using the equation: Here, TPHIGH = 9.817% Here, TPMID = 0.118 % Here, TPLOW = 99.76% The false positive rate (FP) is the proportion of negatives cases that were incorrectly classified as positive, as calculated using the equation: FP(HIGH) = 90.18% FP(MID) = 99.88% P(LOW) = 0.237% Precision (P) is the proportion of the predicted positive cases that were correct, as calculated using the equation: P(HIGH) = 47.97% P(MID) = 60% P(LOW) = 85.8% Performance metrics for model 1 is presented in Fig 5.
0.044444 - It clearly shows a bias towards criticality class “low”.
0.021053 - The accuracy determined using Eq 1 is not giving adequate performance measure, since number of “Low” criticality cases is much greater than the rest of the cases in the dataset (Michalski, Bratko, & Kubat, 1998).
0.066917 - Even though the classifier shows poor performance to classify “High” and “Low” cases, due to i element in the equation 5.1 i.e.
0.021858 - True Positive cases of “Low” value is so high that Accuracy becomes high (wrongly) for this classifier.
0.062016 - Performance parameters for the Model 1 Fig 5.
0.068376 - Performance parameters for the Model 1.
0.087312 - Model 2 ( all instances from Dataset 2 is used for classification) As we discussed in the previous section about the low performance of the tree built on the data set 1, we did not proceed further with the analysis with this dataset.
0.066143 - We realize that we have to reprocess the dataset to remove some bias to the lower value and also need to reduce dimensionality to some extend to in hope to increase the overall performance of the classier.
0.094737 - The following data processing steps are followed: Step 1: From frequency distribution table we can see that instance of class “Low” has 21,510, “Mid” has 2,549 and High has 1,202 records.
0.026144 - Percentage-wise “Low” = 85.2%, Mid = 10.1% and High = 4.8%.
0.027211 - Discussing with domain expert we realize that this percentage is acceptable.
0.011299 - Among the admitted patients this ratio Low : Mid : High = 17 : 2 : 1 represents somewhat true picture.
0.078612 - Since for our research purpose this ratio or proportion is not much relevant and we are more concerned about the attributes of the dataset which represent the physical condition of the patients, we use random selection to select 1,202 records from “Mid” and Low” class, same as the instance count of class “High”: to make a balance sample out of the biased surveillance dataset.
0.065916 - Step 2: Frequency Distribution of the Dataset for all variables are taken.
0.059701 - Depth of the dataset is now reduced–it has now 3607 records, with equal number of High, Mid, Low classes.
0.028986 - We have following observation: (i) One obvious fact is that variability of the values of the field “chemothy” is very high.
0.000000 - That filed is removed.
0.038635 - (ii) Since the number of records has been dropped down, distribution of the variable have changed drastically.
0.062893 - So we need to check for the Information Gain of the variables again.
0.062893 - So we need to take Cross-tabulation criticality with all the remaining variables.
0.014184 - Step 3: Cross-tabulation is taken (Criticality × All other variables).
0.027397 - Calculated information gain Applying the previous selection criteria of attributes (Select attribute with Information Gain ⩾ 0.5%), three of the variables are dropped.
0.069182 - Now variables count to 18, 17 predictor variables and 1 target/class variable.
0.052043 - Now again after this second time data cleaning and data preprocessing we start to model the dataset using SIPINA with C4.5 decision tree classification algorithm with the same default parameters as the first run: Learning Method: C4.5 (Quinlan – 1993) Confidence Level = 25 Leaf size = 2 Sampling = 0 (All Dataset) General information about the tree: Nodes: 61 (32 Nodes increased from 29) Leaves: 31 (16 Leaves increase from 15) Max Depth: 12 ( 5 Max Depth increased from 7) If we compare to the previous tree this new tree with the new dataset has 32 more nodes, 16 more leaves and Max Depth has increase by 5 levels, i.e., the new decision tree is double in size compared to the previous tree.
0.000000 - So complexity which depends on the terminal nodes has considerable increased.
0.023474 - This shows problem of overfitting, which would result in excessively large rule sets with very low predictive power for previously unseen data.
0.070125 - Truly a large number of rules are generated: 31 decision rules; they are not mentioned since they are due to overfitting; they are bound to be very much specialized, with less generalization capacity: good for classification of training set but bad on unseen instances, i.e.
0.000000 - misclassification error is less, but prediction error would be high (Ignizio, 1991).
0.090909 - We need a smaller and simpler tree in order to increase its predictive accuracy.
0.071895 - For this purpose we need to employ tree optimization technique, tree pruning.
0.071895 - We need a tradeoff between misclassification error and tree complexity (Roman, 2004).
0.025890 - Before proceeding further with tree pruning, let us take the confusion matrix for the new tree; and also calculate AC, TPHIGH, TPMID and TPLOW and compare the new values with the values for the previous tree (Fig 4).
0.035088 - For class variable criticality the Confusion Matrix for Dataset 2 is presented in Table 4.
0.000000 - Table 4.
0.000000 - Confusion Matrix for Dataset 2.
0.019417 - Actual Classified High Mid Low High 680 (a) 416 (b) 106 (c) Mid 30 (d) 944 (e) 228 (f) Low 14 (g) 65 (h) 1123 (i) From the table it is visible that the classification accuracy has increased.
0.061033 - Even record count is reduced in the new dataset, TP count for High and Mid class have increased to a considerable amount.
0.034632 - Fig 6 shows the classification accuracy is increased i.e., the misclassification error is decreased with more complex tree penalizing generalization capacity of the tree.
0.055059 - Performance parameters for Data Set 2 Fig 6.
0.060806 - Performance parameters for Data Set 2.
0.064747 - Now for Dataset 2: AC = 76.18% TPHIGH = 56.57% TPMID = 78.54% TPLOW = 93.43% FP(HIGH) = 43.43% FP(MID) = 21.46% FP(LOW) = 6.57% P(HIGH) = 93.92% P(MID) = 66.25% P(LOW) = 77.076% We can construct the following tables to represent features of the datasets (Table 5) and comparison of classification performance of the decision trees generated from the datasets (Table 6).
0.000000 - Table 5.
0.142715 - Two datasets used for decision tree model learning.
0.069024 - Dataset Description Classes Attributes Instances Categorical Continuous Training set Test set Dataset 1 Varying Distribution of Class instances 3 22 25,261 Dataset 2 Equal Distribution of class instances 3 18 3,606 Table 6.
0.065041 - Comparison of classification accuracy between 2 models.
0.036570 - Dataset Test set (instances) Correctly Classified Accuracy % TPHIGH % TPMID % TPLOW % Dataset 1 25,261 (Learning Set) 21,580 85.43 9.817 0.118 99.76 Dataset 2 3,607 (Learning Set) 2,747 76.18 56.57 78.54 93.43 A comparison between model 1 and model 2 is presented in Fig 7.
0.098969 - We can observe that we have a considerable development in classification of high and mid classes, when we used the Dataset 2 with equal distribution of class instances instead of using Dataset 1 with class distribution of high variability for learning the decision tree.
0.056738 - Graphical comparison of classification accuracy between two Models Fig 7.
0.062016 - Graphical comparison of classification accuracy between two Models.
0.076305 - Model 3 (Dataset 1 is divided into training and testing set) Now the instances of Dataset 1 and Dataset 2 are divided into training set and test set.
0.032448 - Training and testing with Dataset 1: Initial parameters same as before: Learning method: C4.5 (Quinlan – 1993) Confidence level = 25 Leaf size = 2 Sampling = 0 (All Dataset) General information about the tree: Nodes: 31 Leaves: 16 Max Depth: 9 16 decision rules generated.
0.037736 - Confusion matrix for learning dataset for model 1 is presented in Table 7.
0.000000 - Classification accuracy AC = 85.15% TPHIGH = 5.39% TPMID = 1.45% TPLOW = 99.83% FP(HIGH) = 94.61% FP(MID) = 98.56% FP(LOW) = 0.168% P(HIGH) = 53.33% P(MID) = 57.58% P(LOW) = 85.37% Table 7.
0.000000 - Confusion Matrix for Dataset 1 (Training).
0.022989 - Actual Classified High Mid Low High 32 7 555 Mid 17 19 1279 Low 11 7 10703 Confusion matrix for testing dataset for model 1 is presented in Table 8.
0.000000 - Classification accuracy AC = 85.5% TPHIGH = 5.59% TPMID = 0.89% TPLOW = 99.68% FP(HIGH) = 94.41% FP(MID) = 99.11% FP(LOW) = 0.315% P(HIGH) = 42.5% P(MID) = 28.21% P(LOW) = 85.96% Table 8.
0.000000 - Confusion Matrix for Dataset 1 (Testing).
0.057143 - Actual Predicted High Mid Low High 34 18 556 Mid 22 11 1201 Low 24 10 10755 So we can see that classification accuracy and prediction accuracy of this tree is not satisfactory for the “High” and “Mid” classes.
0.061722 - If we analyze the ROC (Receiver Operating Characteristics) curve taken with parameter “age” we can also find that the classifier performs poorly for class “High” and “Mid” (the curve is far away from the best possible line (0, 1), it is under the diagonal i.e.
0.035088 - random guessing line, it has the lesser prediction capacity then flipping a coin (0.5), only it case of predicting class “Low”, it works well, the line is nearer to the (0, 1) line.
0.000000 - Figs.
0.044444 - 9a–c, depict the ROC curve for different classes.
0.032520 - ROC Curve for Class=“High” Fig 9a.
0.036036 - ROC Curve for Class = “High”.
0.032520 - ROC Curve for Class=“Mid” Fig 9b.
0.036036 - ROC Curve for Class = “Mid”.
0.032520 - ROC Curve for Class=“Low” Fig 9c.
0.036036 - ROC Curve for Class = “Low”.
0.147362 - Model 4 (Dataset 2 is divided into training and testing dataset) Finally, we generate a decision tree using 50% of the instances from data set 2 for learning and rest for testing.
0.069264 - With the increase in size of the tree, misclassification error is decreased and in case of maximum tree, misclassification error will be equal to 0.
0.100808 - Now In case of dataset 2 we generate a decision tree using 50% of the instances for learning and rest for testing prediction accuracy on unseen instance (1803).
0.065359 - Selecting training and testing dataset from SIPINA is presented in Fig 8.
0.030303 - Selecting active examples in SIPINA for learning and inactive examples will be… Fig 8.
0.044776 - Selecting active examples in SIPINA for learning and inactive examples will be used for testing prediction accuracy on unseen instances.
0.110117 - Complex decision tree poorly perform on independent data.
0.094450 - Performance of decision tree on independent data is called true prediction power of the tree.
0.054645 - Therefore, the primary task is to find the optimal proportion between the tree complexity and misclassification error.
0.026490 - The task is achieved through cost-complexity function: where R(T): misclassification error of the tree T, : complexity measure, : total number of terminal nodes in the tree, α: parameter is found through the sequence of in-sample testing when a part of learning sample is used to build the tree, the other part of the data is taken as testing sample (Ignizio, 1991).
0.095078 - We adjust the initial parameters of the C4.5 decision Tree classification algorithm in SIPINA to find an optimal proportion between the tree complexity and misclassification error.
0.054545 - We run the learning process over and over by adjusting the parameters: “C.L.
0.056106 - for pessimistic pruning” and “Leaf Size” several times and tried to arrive at an optimal tree, where the tree may have more misclassification error but a greater generalization capacity and consequently increased prediction accuracy on unseen instances.
0.055016 - Final (optimal) tree: Initial parameters: Learning method: C4.5 (Quinlan – 1993) Confidence level = 90 Leaf size = 25 Sampling = 0 (All Dataset) Confidence level (C.L) changed from default value 25–90 and Leaf Size from 2 to 25.
0.104408 - This leads to through more pruning a decision tree model with less number of terminal nodes i.e., less complexity (Fig 10).
0.124588 - Decision tree generated by C4 Fig 10.
0.126833 - Decision tree generated by C4.5 using Dataset 2 (Data is divided into traiing and testing data).
0.022312 - General information about the tree (Fig 10): Nodes: 13 Leaves: 7 Max Depth: 6 Seven (7) decision rules are generated: Rule 1: IF cofe1m ⩾ 1.50 THEN Criticality in [High] with accuracy 0.8161 on (142, 2, 30) Rule 2: IF cofe1m < 1.50 and dial1m < 1.50 THEN Criticality in [Low] with accuracy 0.7451 on (40, 64, 304) Rule 3: IF cofe1m < 1.50 and dial1m ⩾ 1.50 and diadur < 1.50 and dehydr < 0.50 THEN Criticality in [Low] with accuracy 0.8306 on (12, 30, 206) Rule 4: IF cofe1m < 1.50 and dial1m ⩾ 1.50 and diadur < 1.50 and dehydr ⩾ 0.50 THEN Criticality in [Mid] with accuracy 0.4818 on (59, 66, 12) Rule 5: IF cofe1m < 1.50 and dial1m ⩾ 1.50 and diadur ⩾ 1.50 and thirst < 1.50 THEN Criticality in [Mid] with accuracy 0.6051 on (159, 308, 42) Rule 6: IF cofe1m < 1.50 and dial1m ⩾ 1.50 and diadur ⩾ 1.50 and thirst ⩾ 1.50 and diadur < 2.50 THEN Criticality in [Mid] with accuracy 0.5302 on (87, 114, 14) Rule 7: IF cofe1m < 1.50 and dial1m ⩾ 1.50 and diadur ⩾ 1.50 and thirst ⩾ 1.50 and diadur ⩾ 2.50 THEN Criticality in [High] with accuracy 0.8661 on (97, 10, 5) Confusion matrix for training dataset of optimal tree is presented in Table 9.
0.000000 - Classification accuracy AC = 68.61% TPHIGH = 40.10% TPMID = 82.15% TPLOW = 83.20% FP(HIGH) = 59.90% FP(MID) = 17.85% FP(LOW) = 16.80% P(HIGH) = 83.57% P(MID) = 56.68% P(LOW) = 77.74% Table 9.
0.000000 - Confusion Matrix for Dataset 2 (Training).
0.022989 - Actual Classified High Mid Low High 239 305 52 Mid 12 488 94 Low 35 68 510 Confusion matrix for testing dataset of optimal tree is presented in Table 10.
0.000000 - Classification accuracy AC = 68.50% TPHIGH = 41.91% TPMID = 79.44% TPLOW = 79.44% FP(HIGH) = 58.09% FP(MID) = 20.56% FP(LOW) = 15.45% P(HIGH) = 87.29% P(MID) = 57.02% P(LOW) = 74.89% Table 10.
0.000000 - Confusion Matrix for Dataset 2 (Testing).
0.054561 - Actual Predicted High Mid Low High 254 296 56 Mid 14 483 111 Low 23 68 498 If we compare between the performances of the DT Model learned from Dataset 1 and the DT Model learned from Dataset 2 (Comparison of Performance Table 11, Fig 11), and see the ROC Graph (Fig 12) plotted for the points (TPHigh, FPHigh), (TPMid, FPMid), (TPLow, FPLow), ROC curves (Figs.
0.058608 - 13a–c) taken with parameter “age” and observe it we can clearly see that we have got a more optimized model which has optimal proportion between the tree complexity and misclassification error.
0.077295 - This model has higher generalization capacity, i.e., it can predict with higher accuracy about the class of new unseen instances.
0.000000 - Table 11.
0.106918 - Comparison of performance between two models learned from Dataset 1 and Dataset 2.
0.018998 - Dataset Dataset 1 Dataset 2 Test Set (Instances) 12,631 18,03 Correctly Classified 10,800 1,235 Incorrectly Classified 1,831 568 Overall Accuracy % 85.5 68.5 TPHIGH % 5.592 41.91 TMID % 0.891 79.44 TPLOW % 99.68 84.55 FPHIGH % 94.41 58.09 FPMID % 99.11 20.56 FPLOW % 0.315 15.45 PHIGH % 42.5 87.29 PMID % 28.21 57.02 PLOW % 85.96 74.89 Comparison of prediction accuracy between two models Fig 11.
0.065041 - Comparison of prediction accuracy between two models.
0.012121 - Comparison of prediction accuracy using ROC graph: points (FP, TP) shown for… Fig 12.
0.074627 - Comparison of prediction accuracy using ROC graph: points (FP, TP) shown for low, mid and high classes in two models.
0.032520 - ROC Curve for Class=“High” Fig 13a.
0.036036 - ROC Curve for Class = “High”.
0.032520 - ROC Curve for Class=“Mid” Fig 13b.
0.036036 - ROC Curve for Class = “Mid”.
0.032520 - ROC Curve for Class=“Low” Fig 13c.
0.036036 - ROC Curve for Class = “Low”.
0.029240 - We can observe that the ROC curve has considerable showing better performance in prediction accuracy.
0.027397 - Most of the time the curve is over the random guess line (the diagonal line connecting point (0, 0) to point (1, 1).
0.059017 - The primary purpose of Hospital Surveillance data is to forecast emerging epidemics so that precautionary measures can be taken to reduce the level of damage caused by an epidemic.
0.040293 - Traditionally time series analysis or cluster analysis has been used in this arena to recognize any cyclic patterns in epidemic waves and to generate regression formula which can be used for forecasting.
0.191899 - In this thesis we have used data mining tools and techniques to classify patient using decision tree models generated from Historical Data of Surveillance System.

[Frase 1] In this research we have used decision tree induction algorithm on Hospital Surveillance data to classify admitted patients according to their critical condition.
[Frase 150] According to the objective set by us to find out the criticality of the patient on admission we have selected the predictor variables relevant for the study from the groups: Social and Behavioral Data, Patient History and Primary Diagnosis, and the target variable criticality is a derivative variable from the “Duration of Stay” variable in the group: Treatment.
[Frase 275] The result from the runs: the generated decision tree models are all evaluated and compared for performance in the next section, in the Evaluation and Implementation phase.
[Frase 3] Several decision tree models are developed, evaluated, and compared with different performance metrics.
