Thermodynamic performance modeling of variable speed refrigeration system was carried out using artificial neural networks. Backpropagation learning algorithm with two different variants used in the network. Logistic sigmoid transfer function was used. Eighty percent of data patterns for training and twenty percent for testing procedure were used. The best fitting training data set was found to be with eight neurons in the hidden layer. Statistical error values were found to be within acceptable uncertainties. Predicted values were very close to actual values.

0.149619 - This study presents thermodynamic performance modeling of an experimental refrigeration system driven by variable speed compressor using artificial neural networks (ANNs) with small data sets.
0.088348 - Controlling the rotational speed of compressor with a frequency inverter is one of the best methods to vary the capacity of refrigeration system.
0.066370 - For this aim, an experimental refrigeration system was designed with a frequency inverter mounted on compressor electric motor.
0.060606 - The experiments were made for different compressor electric motor frequencies.
0.083333 - Instead of several experiments, the use of ANNs had been proposed to determine the system performance parameters based on various compressor frequencies and cooling loads using results of experimental analysis.
0.234934 - The backpropagation learning algorithm with two different variants was used in the network.
0.152459 - In order to train the neural network, limited experimental measurements were used as training and test data.
0.228808 - The best fitting training data set was obtained with eight neurons in the hidden layer.
0.149775 - The results showed that the statistical error values of training were obviously within acceptable uncertainties.
0.164514 - Also the predicted values were very close to actual values.
0.038385 - Refrigeration systems remove heat from an enclosed space, or from a substance, and move it to a place where it is unobjectionable.
0.059219 - The primary purpose of refrigeration systems is lowering the temperature of the enclosed space or substance and then maintaining that lower temperature.
0.029412 - These systems work at part-load for much of their life generally regulated by on-off cycles of the compressor, working at the nominal frequency of 50 Hz, imposed by a thermostatic control which determines high energy consumption.
0.064244 - Furthermore, the inefficient use of electricity to supply the refrigeration compressors is considered as an indirect contribution to the greenhouse gases emitted in the atmosphere; these emissions can be reduced by improving the energy conversion efficiency of the above mentioned systems (Aprea, Mastrullo, Renno, & Vanoli, 2004a).
0.059320 - Because the refrigeration systems are widely used in a wide variety of commercial activities, any efficiency improvement of refrigeration systems would represent a significant energy economy (Buzelin, Amico, Vargas, & Parise, 2005).
0.063205 - Theoretically the most efficient method of refrigeration capacity control is the variable speed compressor which continuously matches the compressor refrigeration capacity to the load (Aprea, Mastrullo, & Renno, 2004b).
0.039971 - This method of refrigeration capacity control, which consists in varying the compressor speed to continuously match the compressor refrigeration capacity to the load, has been analyzed during the last years (Aprea et al., 2004b; Aprea & Renno, 2004; Aprea, Rossi, Greco, & Renno, 2003; Buzelin et al., 2005; Chaturvedi, Chen, & Kheireddine, 1998; Haberschill, Gay, Aubouin, & Lallemand, 2002; Koury, Machado, & Ismail, 2001; Miller, 1988; Park & Jung, 2007; Park, Kim, & Cho, 2002; Perreira & Parise, 1993; Rasmussen, Ritchie, & Arkkio, 1997; Wicks, 2000).
0.085980 - During the design procedure of refrigeration systems, the engineers seek to find a solution, which gives maximum efficiency with minimum energy consumption and cost.
0.076494 - The optimum system is not often easily found and a lot of variations and/or simulations are required in order to decide which combination gives the best financial benefit.
0.106880 - The primary objective of this study is to develop a new approach based on artificial neural networks to determine the performance of variable speed experimental refrigeration system in terms of thermodynamics and energy consumption aspects.
0.119437 - For this purpose, an experimental plant was built up to investigate the performance of the variable speed refrigeration system and mathematical modeling analysis was carried out by the use of ANNs to determine thermodynamic performance of the system using results of experimental analysis.
0.078886 - The advantage of this type of modeling is that, it is easier to make accurate forecasts about the system performance using small data sets instead of making a lot of experiments.
0.064903 - The experimental variable speed refrigeration system is made up of a semi hermetic compressor, an evaporator, a condenser and an externally equalized thermostatic expansion valve.
0.009732 - Evaporator and condenser are air cooled finned tube type heat exchangers.
0.042781 - Evaporator is located in a specially designed cold room which is heated by means of electrical heaters for simulating refrigeration load.
0.017544 - Condenser is located in an isolated channel so is not affected by outside conditions.
0.045139 - To fix the air temperature constant which is entering the condenser channel, electrical heaters are located at the entrance of the channel.
0.022599 - Cold room is 2.1 m in height, 1.2 m in width and 1.35 m in length.
0.023810 - It is also isolated with styrofoam.
0.063694 - For simulating the refrigeration load, 18 flat electrical heaters are used for homogeny heat distribution.
0.055096 - Each heater is 0.045 kW in power and the power of heaters is controlled by means of a variable transformer and the consumed power monitored with the help of a wattmeter.
0.049180 - The compressor speed varied with a frequency inverter.
0.053476 - The device is 0.75 kW and the efficiency of the device is 95.1% according to the manufacturer’s catalogs.
0.055491 - All electrical parts of the refrigeration system are placed into a control panel (Fig 1).
0.013072 - Experimental system Fig 1.
0.014493 - Experimental system.
0.075075 - To evaluate the system performance by regulating the compressor capacity by means of an inverter, temperature and pressure measurement were made from specified points of the experimental system.
0.037037 - Refrigerant mass flow rate is measured using a flow meter that is designed for refrigerant R404a.
0.082863 - Also air humidity at the inlet and outlet of the condenser channel were measured using compact humidity measuring instrument.
0.069182 - Temperature measurements were collected from 12 points of the system, pressure measurements were collected from 7 points and refrigerant mass flow rate measured after the condenser.
0.058140 - All the measurement devices are connected to a data logger which has 20 channels for collecting the data.
0.094488 - Also the data logger was connected to a computer.
0.034188 - Experiments were made up as four groups.
0.021978 - At each group of experiments, compressor electrical motor frequency adjusted as 35 Hz, 40 Hz, 45 Hz and 50 Hz.
0.093502 - Minimum frequency range was selected to be 35 Hz for avoiding problems for the compressor lubricating by splash.
0.000000 - Additionally, compressor vibrations increase at lower frequencies.
0.009390 - At each adjusted frequency, cold room refrigeration duty simulated by electrical heaters.
0.048662 - Experimental setup was operated for each adjusted frequency and cooling loads.
0.102777 - All measurements were made for every 5 s and the data was collected to computer by means of data logger.
0.063346 - Artificial neural networks (ANNs) consist of large numbers of computational units connected in a massively parallel structure and works similar to a human brain (Chouai, Laugier, & Richon, 2002).
0.039939 - Because of their simple and unlimited structure, they have a wide operating area in artificial intelligence applications such as mathematics, engineering applications, energy systems, medicine, economics, etc.
0.019465 - Their advantages are not only eliminating, and estimating and also learning.
0.000000 - (Kizilkan, Şencan, & Yakut, 2006).
0.097138 - Today neural networks can be trained to solve problems that are difficult for conventional computers or human beings.
0.080842 - Throughout the toolbox emphasis is placed on neural network paradigms that build up to or are themselves used in engineering, financial and other practical applications.
0.066071 - Neural networks are composed of simple elements operating in parallel.
0.010929 - These elements are inspired by biological nervous systems.
0.070484 - As in nature, the network function is determined largely by the connections between elements.
0.107153 - A neural network can be trained to perform a particular function by adjusting the values of the connections (weights) between elements.
0.058379 - Commonly neural networks are adjusted, or trained, so that a particular input leads to a specific target output.
0.021858 - Such a situation is shown in Fig 2.
0.059219 - There, the network is adjusted, based on a comparison of the output and the target, until the network output matches the target.
0.081635 - Typically many such input/target pairs are used, in this supervised learning, to train a network (Şencan, 2007).
0.063211 - Schematic diagram of neural networks Fig 2.
0.069178 - Schematic diagram of neural networks.
0.105881 - There are different learning algorithms that can be applied to train a neural network.
0.078220 - It is very difficult to know which training algorithm will be the fastest for a given problem, and the best one is usually chosen by trial and error.
0.081964 - The most popular of them is the backpropagation algorithm, which has different variants.
0.110103 - Backpropagation algorithm was created by generalizing the Widrow–Hoff learning rule to multiple-layer networks and nonlinear differentiable transfer functions.
0.079380 - Standard backpropagation is a gradient descent algorithm, in which the network weights are moved along the negative of the gradient of the performance function.
0.080586 - The term backpropagation refers to the manner in which the gradient is computed for nonlinear multilayer networks (Şencan & Kalogirou, 2005).
0.087974 - The architecture of neural network unit is shown in Fig 3.
0.048158 - Each artificial neural unit consists of inputs (xn), weights (Wn), summation function (Σ), activation function (α) and outputs (y).
0.028169 - The figure illustrates how the information is processed through a single node.
0.037559 - The node receives weighted activations of other nodes through its incoming connections.
0.000000 - First, these are added up (summation).
0.067864 - The result is then passed through an activation function, the outcome being the activation of the node.
0.071066 - For each of the outgoing connections, this activation value is multiplied by the specific weight and transferred to the next node (Kalogirou, 2000).
0.082455 - The architecture of neural network unit Fig 3.
0.089869 - The architecture of neural network unit.
0.078015 - The input layer feeds data to the network, therefore it is not a computing layer since it has no weights or activation function.
0.048662 - The output layer represents the output response to a given input.
0.027211 - Here, x is input vector which can be expressed xT = [x1, x2, … , xn].
0.032922 - W is the vector which is including the weights and represented as WT = [W1, W2, … , Wn].
0.037559 - The node receives weighted activation of other nodes through its coming connections.
0.011905 - First, these are added (summation function).
0.067864 - The result is then passed through an activation function, the outcome being the activation of the node.
0.081218 - For each of the outgoing connections, this activation value is multiplied with the specific weight and transferred to the next node (Kalogirou, 2000).
0.143709 - For the main objective of this work, ANNs was used to investigate the performance of the variable speed refrigeration system.
0.075758 - There were six inputs and six outputs for the system.
0.023952 - The input parameters are compressor frequency, cooling load, condenser and evaporator temperatures and condenser and evaporator pressures.
0.071066 - The output parameters were compressor power consumption, refrigerant mass flow rate, experimental and theoretical COP values, exergetic efficiency and irreversibility of the system.
0.103642 - The range of input and output values were given in Table 1.
0.080052 - In Fig 4, the selected ANN structure was shown and consisting of an input layer, a hidden layer and an output layer.
0.143697 - The number of neurons in hidden layer was selected to be in the range of 4–8 for determining the best approach.
0.054945 - The input layer had six neurons for the input parameters and there are six output neurons for the output parameters.
0.000000 - Table 1.
0.047619 - Thermodynamic property ranges of system parameters.
0.017220 - Property Range Inputs parameters Compressor frequency (ƒ), Hz 35–50 Cooling load (QC), kW 0.1–0.7 Condenser temperature (TC), °C 30–35 Evaporator temperature (TE), °C −15 to −10 Condenser Pressure (PC), kPa 300–400 Evaporator Pressure (PE), kPa 1400–1600 Outputs parameters Compressor power (WC), kW 0.5–0.8 Refrigerant mass flow rate ( ), kg/s 0.006–0.007 COPexperimental 1–1.5 COPtheoretical 1.5–2.5 Exergetic efficiency (ε) 0.5–0.6 Irreversibility (I), kW 0.6–0.8 The ANNs structure for the model Fig 4.
0.059524 - The ANNs structure for the model.
0.137509 - Feed-forward backpropagation learning algorithm was used for learning algorithm with one hidden layer.
0.055556 - Inputs and outputs were normalized between the ranges of 0 and 1 by the equation below (Sözen, Arcaklioğlu, & Menlik, 2010).
0.082863 - (1) The training function variants selected for the algorithm were Levenberg–Marquardt (LM), and Scaled Conjugate Gradient (SCG) algorithms.
0.125935 - Adaption learning function was selected to be gradient descent with momentum weight and bias learning function (LEARNGDM) which was standard for the network.
0.168829 - Logistic sigmoid (logsig) transfer function has been used in the hidden and output layer.
0.077479 - Neurons in input layer have no transfer function.
0.098947 - Computer program has been performed under MATLAB environment using neural network toolbox.
0.142360 - In the training part of the performance modeling of the experimental system, an increased numbers of neurons were used in single hidden layer (Karatas, Sozen, & Dulek, 2009).
0.115156 - The training of the network was accomplished by adjusting the weights and was carried out through training sets and training cycles (epochs).
0.095238 - The goal of the learning procedure was to find the optimal set of weights, which in an ideal case would produce the right output for any input.
0.110692 - The output of the network was compared with a desired response to produce an error.
0.112642 - The performance of the network was measured in terms of a desired signal and the criterion for convergence.
0.076805 - For one sample, the absolute fraction of variance (R2) and the root mean square error (RMSE) were determined with the equations given below (Akdag, Komur, & Ozguc, 2009).
0.040516 - (2) (3) In addition mean absolute percentage error (MAPE) and coefficient of variation (cov) are defined as follows, respectively (Akdag et al., 2009; Arcaklioglu, Erisen, & Yilmaz, 2004; Li & Liu, 2009; Sozen, Ozalp, & Arcaklioglu, 2007) (4) (5) In above equations, t is the target value, o is the output value and p is the number of patterns.
0.168565 - For the performance modeling of the variable speed experimental refrigeration system, different algorithms and different number of hidden neurons were used.
0.121204 - The data set for the input and output values were consisted of 80 data patterns obtained from experimental system.
0.215571 - Eighty percent of these data patterns were used for training and twenty percent were used for testing procedure.
0.082841 - In order to determine the output parameters, logistic sigmoid (logsig) transfer function used here is given by; (6) where; (7) In the above equations for Ei, the first two values are the multiplication of the input parameters (In) with their weights at location, and the last constant value (bn) represents the bias term.
0.051896 - The subscript i represents the number of hidden neuron (Karatas et al., 2009; Kizilkan et al., 2006).
0.142713 - The results of the training in terms of statistical error values such as R2, RMSE, MAPE and cov from the variable speed refrigeration system for different algorithms and 4–8 neurons in the hidden layer were presented in Table 2.
0.096397 - As seen from the tablet that the best approach which had minimum error was obtained by LM algorithm with eight neurons for all algorithms.
0.082367 - Also it must be noted that in both training function case, by increasing the number of hidden neurons, the training accuracy improves, as well as approaching the best algorithm.
0.000000 - Table 2.
0.100973 - Statistical error values of training.
0.011614 - Statistical error Training function – neurons Output parameters WC, kW , kg/s COPexperimental COPtheoretical ε I, kW R2 LM4 0.989691529 0.989946 0.98964 0.98954 0.989527 0.989859 LM5 0.992053575 0.992103 0.991722 0.992155 0.991997 0.992004 LM6 0.995526359 0.995628 0.995531 0.995656 0.995653 0.995586 LM7 0.997989424 0.997987 0.997987 0.99799 0.997983 0.997983 LM8 0.999997522 0.999997 0.999999 0.999999 0.999999 0.999998 SCG4 0.989309592 0.989543 0.989332 0.989427 0.989651 0.98975 SCG5 0.991471445 0.991529 0.991136 0.990917 0.991129 0.991596 SCG6 0.994605154 0.994333 0.994454 0.994281 0.993754 0.994731 SCG7 0.996852395 0.996876 0.996607 0.996793 0.99663 0.996811 SCG8 0.998440289 0.998686 0.99856 0.998631 0.99855 0.998522 RMSE LM4 0.011354492 0.006775 0.012786 0.013471 0.012725 0.008577 LM5 0.007842882 0.006722 0.013754 0.005398 0.008436 0.008732 LM6 0.006706782 0.004346 0.006974 0.001743 0.001839 0.005043 LM7 0.002789554 0.000938 0.000976 0.000152 0.001421 0.001466 LM8 0.00064616 0.000873 0.000148 0.000102 0.000090 0.000483 SCG4 0.015028239 0.012204 0.01568 0.013803 0.010141 0.009099 SCG5 0.013080453 0.012275 0.017753 0.018942 0.015856 0.011392 SCG6 0.011325284 0.014634 0.014122 0.015425 0.018922 0.009299 SCG7 0.006955776 0.001333 0.011991 0.008283 0.01036 0.007834 SCG8 0.009203962 0.002094 0.007164 0.004759 0.006605 0.00757 MAPE LM4 0.037882194 0.023785 0.043854 0.047722 0.045461 0.034243 LM5 0.028485818 0.023366 0.047585 0.020346 0.025541 0.034531 LM6 0.020188347 0.013033 0.020765 0.006033 0.007129 0.017426 LM7 0.002161308 0.00296 0.003663 0.000556 0.004092 0.005243 LM8 0.001131733 0.002646 0.000514 0.000313 0.000291 0.001431 SCG4 0.049285373 0.045087 0.054667 0.050761 0.03442 0.035393 SCG5 0.043826203 0.043438 0.075325 0.067984 0.06058 0.044791 SCG6 0.038109656 0.050913 0.049327 0.053983 0.069545 0.034743 SCG7 0.022745647 0.021631 0.037307 0.027624 0.032648 0.024971 SCG8 0.029535975 0.008259 0.026083 0.016305 0.019419 0.025072 cov LM4 0.022686922 0.013963 0.023022 0.025767 0.026603 0.017278 LM5 0.015687369 0.013855 0.024806 0.010353 0.01761 0.017588 LM6 0.013403636 0.007145 0.012553 0.003343 0.003846 0.010142 LM7 0.001291998 0.001936 0.001757 0.00029 0.002972 0.002948 LM8 0.000557876 0.001801 0.000266 0.000196 0.00019 0.000971 SCG4 0.030055683 0.025187 0.028215 0.026463 0.021225 0.018328 SCG5 0.026217632 0.025402 0.031929 0.036221 0.033156 0.022941 SCG6 0.0226691 0.030262 0.02538 0.029532 0.039651 0.018694 SCG7 0.013907167 0.013065 0.02158 0.01588 0.021677 0.015751 SCG8 0.018410038 0.004321 0.012894 0.00913 0.013814 0.015214 The decrease of the mean square error (MSE) during the training process for eight neurons showed the best approach for LM8 algorithm as shown in Fig 5.
0.127999 - The performance of training and test sets of the established ANN model was given in Fig 6.
0.105530 - It was observed in the figure that the model agrees well with the training and test data as it learns the relationship between the input and output variables.
0.082725 - Training performance for LM8 algorithm in terms of MSE Fig 5.
0.089239 - Training performance for LM8 algorithm in terms of MSE.
0.114962 - Comparison of the measured and predicted values for LM algorithm with eight… Fig 6.
0.132843 - Comparison of the measured and predicted values for LM algorithm with eight neurons.
0.094387 - A comparison between the actual data obtained from experiments and predicted values from ANNs were presented in Table 3.
0.107830 - From this table, the real performance of the simulation of the system can be comprehended since these were not used for training.
0.088028 - As can be seen, the differences between actual and predicted values are very small.
0.063049 - It is clear from this table that the established ANNS model gives a very accurate modeling of the experimental data instead of setting up several experiments.
0.000000 - Table 3.
0.068909 - Comparison between the actual and predicted values.
0.002675 - Inputs Outputs WC, kW , kg/s COPexperimental COPtheoretical ε I, kW ƒ, Hz QC, kW Actual Test Actual Test Actual Test Actual Test Actual Test Actual Test 35 0.7 0.583 0.5823 0.0065 0.006493 1.305 1.30377 2.141 2.1398 0.594 0.5934 0.644 0.643 35 0.5 0.581 0.5804 0.0065 0.006493 1.326 1.32511 2.178 2.1762 0.594 0.5936 0.641 0.640 35 0.3 0.571 0.5705 0.0065 0.006493 1.333 1.33198 2.284 2.2825 0.598 0.5982 0.636 0.636 35 0.1 0.576 0.5753 0.0065 0.006493 1.348 1.34667 2.294 2.2921 0.599 0.5992 0.631 0.630 40 0.7 0.635 0.6346 0.0067 0.006691 1.219 1.21867 1.983 1.9809 0.587 0.5867 0.685 0.685 40 0.5 0.632 0.6310 0.0067 0.006694 1.229 1.22812 1.996 1.9941 0.588 0.5875 0.691 0.690 40 0.3 0.635 0.6341 0.0067 0.006694 1.304 1.30348 2.317 2.3149 0.589 0.5892 0.692 0.691 40 0.1 0.63 0.6295 0.0067 0.006692 1.327 1.32664 2.362 2.3595 0.591 0.5911 0.695 0.694 45 0.7 0.684 0.6833 0.0069 0.006893 1.170 1.16971 2.022 2.0202 0.580 0.5795 0.741 0.740 45 0.5 0.683 0.6823 0.0069 0.006893 1.203 1.20276 2.156 2.1544 0.581 0.5813 0.740 0.739 45 0.3 0.674 0.6733 0.0069 0.006893 1.223 1.22180 2.156 2.1545 0.582 0.5815 0.736 0.735 45 0.1 0.68 0.6792 0.0069 0.006893 1.242 1.24105 2.243 2.2408 0.584 0.5838 0.729 0.728 50 0.7 0.745 0.7443 0.00715 0.007142 1.092 1.09161 1.930 1.9280 0.573 0.5727 0.800 0.799 50 0.5 0.742 0.7412 0.00715 0.007142 1.121 1.12058 2.006 2.0041 0.573 0.5732 0.799 0.798 50 0.3 0.739 0.7381 0.00715 0.007142 1.164 1.16342 2.170 2.1678 0.576 0.5755 0.792 0.792 50 0.1 0.738 0.7373 0.00715 0.007142 1.194 1.19299 2.279 2.2773 0.577 0.5768 0.790 0.789
0.157585 - In this study, performance analysis of variable speed refrigeration system with artificial neural networks was successfully carried out with small experimental data.
0.169787 - Feed-forward backpropagation learning algorithm with two different training functions has been used.
0.088407 - Logistic sigmoid (logsig) transfer function has been selected.
0.136127 - The results of the training were given in terms of statistical error values such as R2, RMSE, MAPE and cov for 4–8 neurons in the hidden layer.
0.096317 - It was found that that the neural network approach showed higher efficiency instead of establishing several experiments.
0.060264 - Also from the results, the system performance can be determined for the unmeasured compressor frequencies such as 36 Hz.
0.075068 - This study points out that a significant energy saving can be supplied by determining the optimum compressor frequency with neural Networks.
0.099659 - This methodology can provide simplicity in the thermodynamic analysis of refrigeration systems.

[Frase 120] The results of the training in terms of statistical error values such as R2, RMSE, MAPE and cov from the variable speed refrigeration system for different algorithms and 4–8 neurons in the hidden layer were presented in Table 2.
[Frase 115] For the performance modeling of the variable speed experimental refrigeration system, different algorithms and different number of hidden neurons were used.
[Frase 117] Eighty percent of these data patterns were used for training and twenty percent were used for testing procedure.
[Frase 8] The best fitting training data set was obtained with eight neurons in the hidden layer.
