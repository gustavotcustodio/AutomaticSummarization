Development of a learning method for optimization of network topology of ANN. Use of PSO trained ANN in channel equalization. PSO trained ANN equalizers performs better than PSO based equalizers as well as ANN based equalizers.

0.168785 - In this paper, we apply Artificial Neural Network (ANN) trained with Particle Swarm Optimization (PSO) for the problem of channel equalization.
0.148583 - Existing applications of PSO to Artificial Neural Networks (ANN) training have only been used to find optimal weights of the network.
0.083796 - Novelty in this paper is that it also takes care of appropriate network topology and transfer functions of the neuron.
0.068376 - The PSO algorithm optimizes all the variables, and hence network weights and network parameters.
0.115055 - Hence, this paper makes use of PSO to optimize the number of layers, input and hidden neurons, the type of transfer functions etc.
0.171429 - This paper focuses on optimizing the weights, transfer function, and topology of an ANN constructed for channel equalization.
0.165873 - Extensive simulations presented in this paper shows that, as compared to other ANN based equalizers as well as Neuro-fuzzy equalizers, the proposed equalizer performs better in all noise conditions.
0.066502 - Adaptive channel equalizers play an important role in recovering digital information from digital communication channels.
0.069841 - In Voulgaris and Hadjicostis (2004), the authors have proposed optimal preprocessing strategies for perfect reconstruction of binary signals from a dispersive communication channels.
0.000000 - Touri et al.
0.069841 - have developed (Touri, Voulgaris, & Hadjicostis, 2006) deterministic worst case frame work for perfect reconstruction of discrete data transmission through a dispersive communication channel.
0.092794 - Few adaptive equalizers have been suggested using soft computing tools such as ANN, PPN and the FLANN in late 90 s. It has been reported that these methods are best suited for nonlinear and complex channels.
0.077196 - Chebyshev Artificial Neural Network has also been proposed for nonlinear channel equalization (Patra, Poh, Chaudhari, & Das, 2005).
0.074074 - The drawback of these methods is that the estimated weights may likely fall to local minima during training.
0.066667 - Joint-processing adaptive nonlinear equalizer based on a pipelined recurrent neural network (JPRNN) using a modified real-time recurrent learning (RTRL) algorithm is proposed in Zhao et al.
0.000000 - (2011a).
0.064327 - Adaptive decision feedback equalizer (DFE) with the combination of finite impulse response (FIR) filter and functional link neural network (CFFLNNDFE) is introduced in Zhao et al.
0.000000 - (2011b).
0.074074 - In both of these papers it is shown that improvement in performance is in expense of increased complexity.
0.000000 - To overcome this complexity, Zhao et al.
0.106782 - (2011c), Zhao and Zhang (2009), Zhao, Zeng, He, Jin and Li (2012), Zhao, Zeng and Zhang (2010a), Zhao, Zeng, Zhang and Li (2010b) proposed some of typical forms of ANN based equalizers.
0.084291 - Despite of complexity problems, ANN remains as one of best tools for the problem of equalization (Abiyev, Okyay, Tayseer, & Fakhreddin, 2011; Panigrahi, Santanu, & Sasmita, 2008a; Zhao & Zhang, 2009; Zhao et al., 2010a; Zhao et al., 2010b; Zhao et al., 2012; Zhao et al., 2011a, 2011b, 2011c).
0.088359 - However, because traditional training algorithms fail in many cases, there is a recent trend to train ANN with bio-inspired optimization algorithms for different applications (Chau, 2006; Yogi, Subhashini & Satapathy, 2010).
0.083333 - PSO has been an increasingly applied in the area of computational intelligence.
0.080000 - Also, it lends itself as being applicable to a variety of optimization problems.
0.126984 - Evolutionary algorithms are also successful in the training of Artificial Neural Networks (ANN) Yogi et al.
0.000000 - (2010), Lee and Lee (2012), Lin and Liu (2009), Lin and Chen (2011), Hong (2008), Potter, Venayagamoorthy, and Kosbar (2010).
0.170752 - Interestingly, applications of PSO to ANN training in these works have only been used to find optimal weights of a given network.
0.079365 - But, there is also a need for the appropriate topology and transfer functions of the neuron.
0.076389 - The PSO algorithm optimizes all the variables, and hence capable of optimizing network weights and network parameters (may be variables).
0.157143 - As a result, we can make use of PSO to optimize all parameters of a network, i.e.
0.051282 - the number of layers, input and hidden neurons, the type of transfer functions etc.
0.165863 - Hence, this paper focuses on optimizing the weights, transfer function, and topology of an ANN constructed for channel equalization.
0.042328 - Recurrent Neural Networks (RNN) used in Potter et al.
0.115885 - (2010) for channel prediction using hybrid and variants of PSO uses only for weight optimization.
0.078421 - Moreover, here complexity becomes very high because of use of so many optimization algorithms and becomes dead slow because of use of Differential Evolution (DE).
0.160468 - However, the advancement made in this paper over existing work can be seen as optimizing the weights, transfer function, and topology of an ANN based channel equalizer.
0.103077 - Though grammatical swarm optimization (de Mingo López, Blas, & Arteta, 2012) can be used to obtain a neural network topology, topology of ANN model used in this paper is decided using training algorithm discussed in later sections in this paper.
0.095238 - The real essence of the proposed equalizer is its performance that outperforms contemporary ANN (Zhao et al., 2011a, 2011b, 2011c) based and Neuro-fuzzy (Abiyev et al., 2011; Panigrahi et al., 2008a) equalizers available in the literature.
0.050794 - The organization of the paper is as follows: Section 2 discusses the problem statement followed by the proposed system model in Section 3.
0.049383 - For performance evaluation, the simulation study is carried out which is dealt in Section 4.
0.060606 - Finally conclusion of the paper is outlined in Section 5.
0.077295 - A model of digital transmission system is depicted in Fig 1.
0.046784 - Model of digital transmission system Fig 1.
0.052288 - Model of digital transmission system.
0.052910 - Impulse response of channel & co-channel can be represented as (Panigrahi, Santanu, & Sasmita, 2008b) (1) Here pi and ai,j are length and tap weights of ith channel impulse response.
0.025397 - We assume a binary communication system, which would make the analysis simple, though it can be extended to any communication system in general.
0.051680 - The transmitted symbols xi(n), for channel and co-channel are drawn from a set of independent, identically distributed (i.i.d) dataset comprising of {±1} and these are mutually independent.
0.014572 - This satisfies the condition (2) (3) where E[⋅] represents the expectation operator and (4) The channel output scalars can be represented as (5) Here d(n) desired received signal dco(n) is interfering signal and η(n) is noise component assumed to be Gaussian with variance and uncorrelated with data.
0.058120 - The desired and interfering signal can be represented as (6) (7) The task of the equalizer is to estimate the transmitted sequence x0(n − k) based on channel observation vector, y(n) = [y(n), y(n − 1), ..., y(n − m + 1)]T, where m is order of equalizer and k is decision delay.
0.050505 - The error e(n) can be expressed as: (8) Since e2(n) is always positive and represents the instantaneous power of the difference signal, chosen as cost function instead of e(n).
0.048048 - The objective of an adaptive algorithm is to change the equalizer weights iteratively so that e2(n) is minimized iteratively and subsequently reduced to zero.
0.170565 - System model for the proposed equalizer in this paper is a multi layer ANN, where the network is trained for optimized value with use of PSO.
0.145594 - System model for the proposed equalizer consists of an ANN where all its neurons trained with PSO.
0.053333 - Technical advantages of the paper are evidenced from its novelty and performance results.
0.083333 - PSO has been an increasingly applied in the area of computational intelligence.
0.080000 - Also, it lends itself as being applicable to a variety of optimization problems.
0.185185 - PSO is also successful in the training of Artificial Neural Networks (ANN).
0.121212 - This section first provides a brief idea on PSO and ANN for the ease of the reader in following two subsections.
0.141270 - Then, the PSO model of Ribeiro and Schlansker (2004) as proposed for channel equalization discussed in next subsection.
0.085470 - Artificial Neural Networks Artificial Neural Networks (ANN), are artificial models of the human brain.
0.063492 - Human brain is capable of adapting to changing situations and learns quickly in the correct context.
0.088889 - ANN works on simulation of the human brain.
0.102564 - At their basic level ANN consist of an interconnected network of neurons and synapses.
0.111111 - Neurons are the fundamental element of an ANN.
0.000000 - Neurons accepts inputs from other neurons and produce an output by firing their synapse.
0.050794 - Neurons perform a weighted sum on all of their inputs and then the result goes through a transfer function to produce an output.
0.078431 - ANNs are organized into layers.
0.000000 - There is an input layer, an output layer and sometimes one or more hidden layers.
0.114943 - The hidden layers are the root of the ANN that performs the actual computations of the network.
0.050794 - A network is comes into force when it is given with a set of inputs and the output layer produces the desired result.
0.049383 - Weights of neurons may be different.
0.034188 - Similarly, transfer function of different neurons may be different (usually they are the same).
0.170435 - Training of ANN is required to facilitate the proper arrangement of a network.
0.140741 - Training neural networks Out of several methods of training the ANN, back-propagation is the most common one.
0.107280 - An ANN is trained by a set of data that consists of inputs and a desired output.
0.052288 - The training steps are: 1.
0.022222 - Read in the inputs and expected outputs 2.
0.032922 - Compute the result by weighted sum of inputs and passing through the transfer functions 3.
0.000000 - Compare the result with desired result 4.
0.022222 - Compute and update fitness value based on comparison.
0.037037 - Repeat steps 2 and 3 until all training points are finished 6.
0.042328 - Adjust weights in the appropriate direction to optimize fitness.
0.050926 - Repeat 1–6 until acceptable fitness value is found The Back-propagation method Rumelhart, Geoffey, & Ronald, 1986 is a gradient type adjustment for weight modification and may take an extremely long time to train a network.
0.179324 - In this paper, we suggest the use of PSO as a training algorithm.
0.047619 - Transfer functions Each of the neurons associated with a transfer function that operates on the input.
0.084175 - The input of a neuron is the weighted sum of its inputs.
0.022222 - A good transfer function is the sigmoid function.
0.014815 - The sigmoid function (9) maps the input to the range [0, 1], and given as: (9) 3.2.
0.060606 - Particle Swarm Optimization PSO Kennedy & Eberhart, 1995, del Valle, Venayagamoorthy, Mohagheghi, Hernandez, & Harley, 2008 is a population based search algorithm and is inspired by natural habits of bird flocking and fish schooling.
0.216117 - In this paper, PSO is used to train ANN to be used as a channel equalizer.
0.055556 - PSO exploits the cooperation aspect and applies it to engineering optimization problems.
0.063492 - The particles simply follow a predefined set of rules.
0.061303 - PSO computes the particles based on a fitness function and finds a particle with a good solution.
0.040404 - The particle with the best fitness is chosen as teacher.
0.042328 - All other particles then learn from this best particle.
0.041667 - No two particles are same and still each learns the attributes of other that will help to improve their fitness.
0.074074 - PSO model A Particle Swarm is a population of individuals, where, each one influences the neighbors, have contribution in some features for the problem.
0.049383 - Any single particle can be a possible solution, defined by its position, of the problem.
0.015873 - Particles move though the problem space and adjust their path based on influences from other particles.
0.035556 - Each particle is randomly initialized to a certain position in the problem space.
0.093190 - The number of dimensions in the problem space is equal to the number of components there are to optimize.
0.057348 - If, and represent position and velocity vector, a particle updates its position according to the Euler integration equation for physical movement given as: (10) Velocity of the particle is computed from its current velocity (randomly initialized) and the velocity of the best particle in its neighborhood, including two stochastic variables.
0.044444 - One of the two stochastic variables takes care of the portion of the velocity vector corresponding to it’s previous velocity, while the other one takes care of the portion corresponding to the velocity of the best particle.
0.021164 - The resultant, generally, is a constant Kennedy & Eberhart, 1995.
0.037037 - Updation rule of the velocity vector is: (11) Here, ρ1 and ρ2 are two random constants corresponding to social and cognitive behavior of particle.
0.059829 - For the problem given, a population of particles (around 20–50) are randomly initialized.
0.053640 - Then they allowed moving in the problem space in search of an optimal or near optimal solution.
0.015873 - The particles continue to move till they reach the desired position, i.e., global best solution.
0.207570 - PSO seems to be a better method of training ANN as compared to the traditional methods.
0.142995 - PSO does not just train one network, rather, trains a network of networks.
0.133862 - PSO forms a number of ANN and initializes all weights to arbitrary random values and trains each one.
0.074074 - PSO compares fitness each network’s.
0.047619 - The network with the best fitness is chosen to be the teacher network (the global best).
0.051282 - This network trains the others to update themselves forgetting their personal error or fitness.
0.023392 - Each neuron contains a position and velocity.
0.075188 - The position corresponds to the weight of a neuron.
0.000000 - The velocity is used to update and control the position (weight).
0.063492 - If a particular neuron is far away from the global best position, then it will learn adjustment of its weight from a neuron which is closer to the global best.
0.020202 - Here, the particles are the individual networks not the neurons.
0.088889 - The number of neurons in the network defines the dimension of the hyperspace.
0.074074 - Hence, location of the network in the problem hyperspace is effectively defined by the positions of each neuron in a network.
0.071111 - There may be a number of maxima and minima in the problem hyperspace.
0.043011 - Particles swarm around in the hyperspace, updating their position as seen from best position found by their neighbor particles.
0.044444 - Finally, a particle will reach the optimal position.
0.000000 - When they reach that point, it will continue to move towards the global optima.
0.017094 - Other particles will quickly learn this and adjust their positions accordingly towards this optima.
0.057971 - This ensures that a team of particles covered the optima area.
0.062016 - Training of the network stops, if this optimum fitness is acceptable, Failing to reach the global optimal positions, the position of the neurons are once again randomized and the swarm restarts.
0.131313 - There are number of solutions for a real-number ANN.
0.059829 - In PSO, it is assured that the network will never converge to false maxima.
0.062222 - For solution hunting, PSO takes on two major methods, namely, exploration and exploitation.
0.021164 - Exploration is the generalized search for maxima and minima.
0.018519 - This occurs with a larger population moving over the entire problem space.
0.020202 - Exploitation is the convergence on a particular maxima or minima.
0.030303 - Then, exploration starts to examine it for a second time.
0.040404 - Update during exploitation is with lesser speed than in exploration.
0.013072 - This is done by a smaller step size and also taken care that there won’t be overstep to a possible optima.
0.034188 - This part is an addition to original PSO and monitored by an annealing factor.
0.041667 - As used in this paper, this annealing factor starts with 1 and decreases while moving towards optimal position, i.e.
0.000000 - taking smaller steps.
0.177609 - Constructing a network swarm This research focuses around training ANN with PSO.
0.091919 - To do this, a population of networks must be constructed.
0.121864 - Here, each ANN, as per the weights of the network ingredients, is treated as a particle in problem space.
0.070175 - Usually, a 20-networks population works well.
0.042328 - PSO neighborhoods is formed by this population and initialized.
0.071717 - This construction is called the topology of the swarm system.
0.083333 - Training the swarm of neural networks The training process is as follows: • Examine over the training data and record the sum of the network errors, for each network.
0.076923 - • To get the best network in the problem space, compare all of the errors.
0.015873 - • If any network has reached the minimum error desired, record its weights and exit the program.
0.083333 - • Else, run PSO to update position and velocity vectors of each network.
0.000000 - • Repeat from step 1.
0.072604 - If the required fitness achieved by a particle, indicating that a solution has obtained, then this particle changes from being a student searching solution to a teacher in production of ANN.
0.107440 - Channel Equalization is a complex problem with a large number of control variables.
0.070707 - ANN has proven to be excellent tools for the problem.
0.121083 - The proposed solution is to construct and train ANN to predict the channel state.
0.086420 - The training data consisted of [±1].
0.053333 - A network was constructed which used these values as inputs to the network.
0.064198 - Network fitness, given in (12) was determined to be the mean square of errors for the entire training set, where error is defined to be the recorded symbol minus the network predicted symbol.
0.055556 - (12) A network is is deemed usable once it has met some minimal requirements for performance.
0.045351 - The requirement used was the statistical calculation known as the multiple correlation coefficients and is given as: (13) This measurement subtracts, from unity, the network fitness divided by the square of the mean subtracted by each output.
0.038647 - As the network becomes more accurate, the resulting value approaches one.
0.167508 - As mentioned earlier, a multi layer ANN is used for channel equalizer.
0.131313 - PSO is used to train the parameters of the network.
0.057971 - Each of the parameters, i.e., weight, topology, transfer function etc.
0.149465 - were trained with PSO as mentioned earlier that PSO trains the network as a whole.
0.104822 - While going for a pseudo-code, it is assumed that ANN first acts as an administrator to provide resources (i.e., which parameter is to be optimized) to PSO which acts as a teacher for student ANN learning the equalization problem.
0.040404 - Pseudo-code for the problem is provided in Fig 2.
0.053640 - In the pseudo-code, N and M represent number of particles and number of hidden nodes respectively.
0.102694 - There is a 5:1 ratio of ants to candidate network topologies.
0.070175 - Pseudo-code for proposed equalizer Fig 2.
0.078431 - Pseudo-code for proposed equalizer.
0.061303 - For the simulations, value of N is set at 25 and value of M initialized with 5.
0.013468 - The velocity factors are 0.8 for the inertial constant, 2 for the cognitive constant, and 2 for the social constant.
0.034188 - Number of iterations and allowable error are set at 1000 and 10−3, respectively.
0.119048 - For Evaluation of the performance of the proposed equalizer, simulation examples are presented in this section.
0.022222 - Two different comparisons are used in these examples.
0.038647 - The widely used channel Liang & Zhi, 2004 is used for simulations.
0.036036 - The system transfer function of this 3rd order channel model is: (14) This channel have system zeros at 0.6 and 0.75 ± j0.85.
0.086516 - To illustrate the effect of nonlinearity on the equalizer performance, nonlinear channel models with the following nonlinearity are introduced.
0.051282 - (15) In the examples, the channel input signals are selected as i.i.d.
0.000000 - sequences having zero mean.
0.027778 - The additive channel noise is modeled as complex white Gaussian processes with zero-mean and independent to the channel input.
0.031250 - It is noted that the symbol error rate (or, Bit Error Rate (BER)) is the final test for communication performance evaluation, and that the equalizer output noise variance is directly related to the symbol error rate via the complementary error function (The noise is assumed to have a Gaussian probability density function.).
0.068376 - More specifically, the probability of BER is related to SNR at the equalizer output.
0.114801 - Comparison with other neural network based equalizers For comparisons with other neural network based equalizers available in the literature, equalizers proposed in Zhao et al.
0.000000 - (2011a, 2011b, 2011c), i.e., PFLADFRNN Zhao et al.
0.000000 - (2011a), JPRNN Zhao et al.
0.000000 - (2011b) and CFFLNNDFE Zhao et al.
0.031746 - (2011c) were simulated along with proposed equalizer to evaluate BER under similar conditions discussed above and resulting plot is shown in Fig 3.
0.059259 - Simulation runs up to the point when we receive 100 symbol errors at the output of the equalizer.
0.016461 - The BER has been evaluated as 100 divided by total symbols sent during the simulation.
0.177870 - Convergence characteristics of ANN based equalizers are shown in Fig 4. log10(BER) vs Fig 3. log10(BER) vs. SNR (in dB) performance for ANN based equalizers.
0.000000 - MSE vs Fig 4.
0.192704 - MSE vs. iterations: convergence performance for ANN based equalizers.
0.044444 - It is seen from Fig 3 that the proposed equalizer outperforms equalizers proposed in literature, PFLADFRNN, JPRNN and CFFLNNDFE at all noise conditions.
0.134392 - From Fig 4, it is further confirmed that the proposed equalizer converges better that other ANN based equalizers.
0.067901 - Comparison with Neuro-fuzzy equalizers For comparisons with Neuro-fuzzy equalizers available in the literature, type-2 TSKFNS as discussed in Abiyev et al.
0.083333 - (2011) and Neuro-fuzzy equalizer trained with GA of Panigrahi et al.
0.032680 - (2008a) simulated along with proposed equalizer to evaluate BER under similar conditions discussed above and resulting plot is shown in Fig 5.
0.059259 - Simulation runs up to the point when we receive 100 symbol errors at the output of the equalizer.
0.000000 - The BER has been evaluated 100 divided by total symbols sent during the simulation.
0.147173 - Convergence characteristics of ANN based equalizers are shown in Fig 6. log10(BER) vs Fig 5. log10(BER) vs. SNR (in dB) performance for different equalizers.
0.000000 - MSE vs Fig 6.
0.088889 - MSE vs. iterations: convergence performance for different equalizers.
0.038095 - It is seen from Fig 5 that the proposed equalizer outperforms both type-2 TSKFNS and GA trained NFN at all noise conditions.
0.059259 - From Fig 6, it is further confirmed that the proposed equalizer converges better that other Neuro-fuzzy equalizers.
0.183838 - This paper proposed an ANN based equalizer trained with PSO.
0.205474 - As compared to other ANN based equalizers as well as Neuro-fuzzy equalizers the proposed equalizer performs better in all noise conditions.
0.292926 - Contributions of the paper can be outlined as: • Development of a learning method for optimization of network topology of ANN • Use of PSO trained ANN in channel equalization • This article paves a way for future works utilizing other nature inspired algorithms for training of ANN based equalizes.
0.200662 - • This article paves a way for future works utilizing grammatical swam, that also can be used fo optimization of ANN topology, for training of ANN based equalizes.
0.122642 - • This article also paves a way for future works utilizing PSO and other nature inspired algorithms for training of ANN for other practical applications those can be formulated either as an optimization or as a classification problem.

[Frase 210] Contributions of the paper can be outlined as: • Development of a learning method for optimization of network topology of ANN • Use of PSO trained ANN in channel equalization • This article paves a way for future works utilizing other nature inspired algorithms for training of ANN based equalizes.
[Frase 211] • This article paves a way for future works utilizing grammatical swam, that also can be used fo optimization of ANN topology, for training of ANN based equalizes.
[Frase 7] Extensive simulations presented in this paper shows that, as compared to other ANN based equalizers as well as Neuro-fuzzy equalizers, the proposed equalizer performs better in all noise conditions.
[Frase 1] In this paper, we apply Artificial Neural Network (ANN) trained with Particle Swarm Optimization (PSO) for the problem of channel equalization.
