Male fertility has decreased in part due to environmental factors and life habits. Laboratory approach is the usual although expensive procedure to assess semen quality. We compare three AI methods as an alternative to predict male fertility. We obtain a prediction accuracy of 86% from environmental factors and lifestyle. The efficiency and clearness of these three methods suggests their clinical use.

0.069239 - Fertility rates have dramatically decreased in the last two decades, especially in men.
0.114919 - It has been described that environmental factors, as well as life habits, may affect semen quality.
0.025445 - Artificial intelligence techniques are now an emerging methodology as decision support systems in medicine.
0.143970 - In this paper we compare three artificial intelligence techniques, decision trees, Multilayer Perceptron and Support Vector Machines, in order to evaluate their performance in the prediction of the seminal quality from the data of the environmental factors and lifestyle.
0.103586 - To do that we collect data by a normalized questionnaire from young healthy volunteers and then, we use the results of a semen analysis to asses the accuracy in the prediction of the three classification methods mentioned above.
0.080665 - The results show that Multilayer Perceptron and Support Vector Machines show the highest accuracy, with prediction accuracy values of 86% for some of the seminal parameters.
0.075055 - In contrast decision trees provide a visual and illustrative approach that can compensate the slightly lower accuracy obtained.
0.179360 - In conclusion artificial intelligence methods are a useful tool in order to predict the seminal profile of an individual from the environmental factors and life habits.
0.086758 - From the studied methods, Multilayer Perceptron and Support Vector Machines are the most accurate in the prediction.
0.069808 - Therefore these tools, together with the visual help that decision trees offer, are the suggested methods to be included in the evaluation of the infertile patient.
0.058006 - After the publication of a meta-analysis directed by Elisabeth Carlsen (Carlsen, Giwercman, Keiding, & Skakkebaek, 1992), there is a debate about a possible decline in semen quality.
0.060761 - Numerous studies show a decrease in semen parameters during the past two decades (Auger, Kunstmann, Czyglik, & Jouannet, 1995; Splingart et al., 2011; Swan, Elkin, & Fenster, 1997, 2000), affecting the male fertility potential.
0.069364 - Between the factors considered to explain this decline, there is an increase in the incidence of male reproductive diseases (Irvine, 2000; Splingart et al., 2011), but also has been suggested the effect of environmental or occupational factors (Giwercman & Giwercman, 2011; Wong, Zielhuis, Thomas, Merkus, & Steegers-Theunissen, 2003), to a certain lifestyle (Agarwal, Desai, & Ruffoli, 2008; Martini, 2004).
0.076696 - To evaluate the male partner, clinicians use the data obtained from semen analysis (Kolettis, 2003), and they compare the obtained results with the corresponding reference value established by World Health Organization (WHO, 1999).
0.061906 - Semen analysis is a good predictor of the male fertility potential (Bonde et al., 1998; Guzick et al., 2001; Slama et al., 2002; Zinaman, Brown, Selevan, & Clegg, 2000), and is also necessary to evaluate candidates to become semen donors (Barratt, Clements, & Kessopoulou, 1998; Carrell, Cartmill, Jones, Hatasaka, & Peterson, 2002; Ecochard, Cottinet, Mathieu, Rabilloud, & Czyba, 1999; Society British Andrology, 1999).
0.070175 - There is a high variability in the testicular function of an individual (Keel, 2006), so it is recommended to interpret the results taking in account certain factors (i.e., fever, toxic exposure) that can modify the semen parameters (Rowe & Comhaire, 2000).
0.145440 - In this paper, assuming the influence of environmental factors and life habits in semen quality, we compare the prediction accuracy of three different artificial intelligence (AI) methods, Multilayer Perceptron (MLP), Support Vector Machines (SVM) and decision trees (DT), to determine the best Decision Support Systems (DSS) that can help in the evaluation of male fertility potential.
0.077670 - The current advances and progresses in the field of AI have led to the emergence of expert systems and DSSs for economics, linguistics, management science, mathematical modelling, psychology, etc.
0.052367 - There are good classifiers in the AI such as artificial neural network (ANN), DT (Polat & Gnes, 2009b), SVM (Conforti & Guido, 2010) or even hybrid methods that combine ANNs and fuzzy logic into fuzzy neural networks (FNN) (Kahramanli & Allahverd, 2008) which are widely used to aid medical diagnosis by means of decision support systems construction.
0.079365 - Moreover, the use of AI has also become widely accepted in medical applications.
0.058651 - These methods include advantages as (a) Ease of optimisation, resulting in cost-effective and flexible non-linear modelling of large data sets; (b) Accuracy for predictive inference, with potential to support clinical decision making; (c) These models can make knowledge dissemination easier by providing explanation, for instance, using rule extraction or sensitivity analysis (Lisboa & Taktak, 2006).
0.066675 - The amount of information used in order to predict the male fertility potential makes really useful the application with AI methods not only for improving the accuracy but also to select the best features as, very often, the number of feature to deal with is huge (Gil & Johnsson, 2010a, 2010b, 2011; Gil, Johnsson, Garcia Chamizo, Paya, & Fernandez, 2009, 2011; Polat & Gnes, 2009a; Subashini, Ramalingam, & Palanivel, 2009).
0.064257 - This will lead to the knowledge discovery in databases, data mining or the process of extracting patterns from large data sets.
0.133054 - The main objective of this paper is to compare a number of classification methods applied to male fertility data sets.
0.120739 - The approach chosen to address the problem is by the use of different AI methods, in particular DT, MLP and SVM.
0.064257 - A comparative study of those will give us insight into the merits of the different methods when used on this problem.
0.084142 - The remaining part of the paper is organized as follows: First we start defining the materials and methods of the study (study population and the variables of the questionnaire).
0.115677 - Then we continue with a brief description of the AI methods used in this paper: MLP, SVM and DT.
0.053950 - Then we proceed by describing the design of our proposal and the experiments carried out in the result section; the available data as well as a detailed explanation of the different values of our database; Then we continue by describing the subsequent testing carried out in order to analyse the results; Finally we draw the relevant conclusions.
0.035320 - Study population The study was performed with young healthy volunteers among the students of the University of Alicante.
0.055096 - 100 volunteers’ between 18 and 36 years old participated in the study.
0.057315 - After being informed they were asked to provide a semen sample after 3 to 6 days of sexual abstinence, and a semen analysis according to World Health Organization (WHO) guidance was performed.
0.034314 - Those with previous known reproductive alterations (i.e., Varicocele) were excluded of the statistical analysis.
0.088613 - Variables of the questionnaire On the day of the analysis, the volunteers were asked to fulfill a questionnaire about life habits and health status.
0.064516 - Table 1 shows the name, a description and the range of values of the variables used in the study as well as the values normalized.
0.071247 - Table 1 shows the treatment we have done to the different database input fields.
0.050725 - We have converted the input data into a range of normalization according to the follow rules: (a) Numerical variables such as age, Year of analysis, the number of exposures or Average number of cigarettes per day are normalized onto the interval (0, 1).
0.068027 - For instance the average number of cigarettes per day is indicated in the second column (Values) as a range between the minimum 0 and the maximum 25.
0.030011 - This means that the persons that consumes 25 cigarettes is normalized to the value 25/25 = 1 whereas an individual that consumes 14 cigarettes per day is normalized to the value 14/25 = 0.56 (b) The variables with only two independent attributes are prearranged with binary values (0, 1).
0.058608 - We have many of those representations.
0.061489 - (c) The variables with three independent attributes, such as “Vaccines received”, “High fevers in the last year” and “Smoking habit” are prearranged using the ternary values (−1, 0, 1).
0.048565 - For example, the last one “Smoking habit” will take −1 for never, 0 represents occasional and 1 daily.
0.039548 - (d) The variables with four independent attributes, such as “Season in which the analysis was performed” or “Marital status” are prearranged using the four different and equal distance values (−1, −0.33, 0.33, 1).
0.048709 - For instance, “Marital status” will take −1 unmarried and without a partner, −0.33 represents unmarried with partner, 0.33 is the value married and 4 for something else.
0.000000 - Table 1.
0.066066 - List of variables with their descriptions and their values range.
0.031746 - Variable description Values (min–max) Normalized Season in which the analysis was performed.
0.033126 - (1) winter, (2) spring, (3) Summer, (4) fall (−1, −0.33, 0.33, 1) Age at the time of analysis.
0.023217 - 18–36 (0, 1) Body mass index (BMI) 17–34 (0, 1) Marital status (1) Unmarried and without a partner, (2) unmarried with partner, (3) married, (4) other.
0.009009 - (−1, −0.33, 0.33, 1) Children (1) Yes, (2) no (0, 1) Birth city (1) Alacanti (2) marina baixa, (3) marina alta, (4) the Comtat, (5) l’Alcoy, (6) l’alt Vinalopo, (7) el vinalopo Mitj, (8) baix Vinalopo, (9) la vega baja, (10) other cities.
0.018229 - (0, 1) City of the family home (1) Alacanti (2) marina baixa, (3) marina alta, (4) the Comtat, (5) l’Alcoy, (6) l’alt Vinalopo, (7) el vinalopo Mitj, (8) baix Vinalopo, (9) la vega baja, (10) other cities.
0.017880 - (0, 1) City of residence during the course (1) Alacanti (2) marina baixa, (3) marina alta, (4) the Comtat, (5) l’Alcoy, (6) l’alt Vinalopo, (7) el vinalopo Mitj, (8) baix Vinalopo, (9) la vega baja, (10) other counties.
0.031008 - (0, 1) Number of siblings.
0.013514 - 0–6 (0, 1) Birth defect, genetic disorder or any hereditary disease (1) Yes, (2) no (0, 1) Childish diseases (i.e., chicken pox, measles, mumps, polio) (1) Yes, (2) no (0, 1) Vaccines received (1) Children calendar, (2) i Children calendar and tetanus, (3) only tetanus.
0.012920 - (−1,0,1) Allergy (1) Yes, (2) no (0, 1) Accident or serious trauma (1) Yes, (2) no (0, 1) Surgical intervention (1) Yes, (2) no (0, 1) Significant diseases (1) Yes, (2) no (0, 1) High fevers in the last year (1) less than three months ago, (2) more than three months ago, (3) no (−1,0,1) Chemotherapy (1) Yes, (2) no (0, 1) Radiations (i.e., X-rays) (1) Yes, (2) no (0, 1) Time since radiation exposure (1) Less than 3 months, (2) More than 3 months.
0.014099 - (0, 1) Number of exposures 0–20 (0, 1) Frequency of alcohol consumption (1) several times a day, (2) every day, (3) several times a week, (4) once a week, (5) hardly ever or never (0, (1) Alcohol consumption Grams per week (0–312) (0, 1) Smoking habit (1) never, (2) occasional (3) daily.
0.024775 - (−1,0,1) Average number of cigarettes per day 0–25 (0, 1) Years of exposure to cigarettes 0–19 (0, 1) Coffee consumption (1) none, (2) 1 or 2 coffees a day, (3) 3 to 5 coffees a day, (4) more than 5 coffees a day.
0.016939 - (−1, −0.33, 0.33, 1) Drugs consumption (1) Yes, (2) no (0, 1) Number of hours spent sitting per day 1–16 (0, 1) Average hours sleep per day 6–12 (0, 1) Wearing tight clothes (1) Yes, 92) no (0, 1) Warm baths (1) Yes, (2) no (0, 1) Average hours per week playing sports 0–40 (0, 1) Average ejaculations per week 1–14 (0, 1) The variables with a range of more than four independent attributes (i.e., some of them have 10 different values) are considered numerical variables as in the option (a).
0.043771 - Decision trees A decision tree is a classifier in the form of a tree structure (see Fig 1), where each node is either a leaf node, which indicates the value of the target attribute (class) of examples, or a decision node, which specifies some test to be carried out on a single attribute-value, with one branch and sub-tree for each possible outcome of the test.
0.033003 - General scheme of a decision tree Fig 1.
0.036630 - General scheme of a decision tree.
0.068627 - This technique is very appreciated for the doctors as it follows similar rules of procedure.
0.051724 - Decision trees are powerful and popular tools for classification and prediction.
0.095979 - In contrast to other AI methods, such as MLP and SVM (that both will be explained later), the advantages of decision trees is due to the fact that they represent rules.
0.017660 - Rules can readily be expressed so that persons can understand them or even directly used in a database.
0.088926 - In some areas of applications, the accuracy of a classification or prediction is the only thing that matters.
0.052910 - In such situations we do not necessarily concern about how the model works.
0.073529 - However, in other situations, the ability to explain the reason for a decision, is crucial.
0.096420 - For instance, in medicine in general, and to predict the male fertility potential in particular, one has describe the the factors involved, so that other physicians can utilize this knowledge in launching a successful prediction.
0.041096 - This domain experts must recognize and approve this discovered knowledge, and for this we need good descriptions.
0.062893 - Modeled after the structure of a tree, decision trees (DTs) are able to provide a good explanation applicable to the life habits of the populations and therefore interpret problems very much according to the principles of mathematical and statistical principles (Risso & Brida, 2010).
0.052083 - A decision tree may be used to classify a given example (see Fig 1); one begins at the root and follows the path provided by the answers to the questions in the internal nodes until a leaf is reached.
0.049689 - The DT algorithm family includes classical algorithms, such as ID3 (Quinlan, 1986), C4.5 (Quinlan, 1993), and CART (Breiman, 1984).
0.062802 - DT is a fairly mature technique which includes models such as C5.0, C&RT, CHAID, QUEST and ID3, the difference of which is in the derivations of formula, such as entropy and information gain, to determine the attributes that result in splitting.
0.014733 - However, most decision trees consist of many nodes, which, under certain circumstances, hinder analysis or interpretation of information (Aitkenhead, 2008; Tokatli, Kurt, & Ture, 2009).
0.073059 - The classification and regression trees (CART or C&RT) method of Breiman (1984) generates binary decision trees.
0.060032 - In the real world, the chances of biased binary outcomes are few, but the binary method allows for easy interpretation and analysis (Prasher et al., 2003; Tokatli et al., 2009).
0.143141 - Thus, the study uses the binary method in the DT to interpret the life habits in prediction of male fertility, and attempts to deduce a correlated prediction factor.
0.047160 - ANN – multilayer perceptron An MLP (Bishop, 2005; Haykin, 1998; Ripley, 1996) consists of multiple layers of neurons, generally three (where each layer fully connected to the next one): an input layer that receives external inputs, one hidden layer, and an output layer which generates the classification results (see Fig 2).
0.063725 - The architecture of the MLP network (input layer, hidden layer and output layer) Fig 2.
0.068783 - The architecture of the MLP network (input layer, hidden layer and output layer).
0.031963 - The input layer represents the input data (the input data set is described in Section 3.1).
0.045662 - The usage of a hidden layer enables the representation of data sets that are not linearly separable.
0.057348 - The output layer represents the classification result and it contains as many outputs as the problem has classes, although here only one neuron is shown.
0.076336 - The weights and the threshold of the MLP are calculated during an adaptation process.
0.037267 - Except for the input layer every neuron in the other layers is a computational element with a nonlinear activation function.
0.044260 - The principle of the network is that when data are presented at the input layer, the network neurons run calculations in the consecutive layers until an output value is obtained at each of the output neurons.
0.022989 - This output will indicate the appropriate class for the input data.
0.066298 - Each neuron (see Fig 3) in the input and the hidden layers is connected to all neurons in the next layer by weighted connections.
0.068376 - The neurons of the hidden layers (see Fig 3) compute weighted sums of their inputs and adds a threshold.
0.066225 - The resulting sums are used to calculate the activity of the neurons by applying a sigmoid activation function.
0.040712 - A neuron in the hidden or the output layer in the MLP Fig 3.
0.044077 - A neuron in the hidden or the output layer in the MLP.
0.049020 - In the experimentation section the number of hidden neurons of the MLP will be established.
0.040800 - This process is defined as follows: (1) where νj is the linear combination of inputs x1, x2, … ,xp, and the threshold θj, wji is the connection weight between the input xi and the neuron j, and fj is the activation function of the jth neuron, and yj is the output.
0.066066 - The sigmoid function is a common choice of activation function.
0.072398 - It is defined as: (2) A single neuron in the MLP is able to linearly separate its input space into two subspaces by a hyperplane defined by the weights and the threshold.
0.045662 - The weights define the direction of this hyperplane whereas the threshold term θj offsets it from origo.
0.082148 - In order to train the MLP we use a supervised learning method denominated backpropagation (Rumelhart, Hinton, & Williams, 1986), which is a gradient descent method, for the adaptation of the weights.
0.037825 - This means the network is presented with input examples as well as the corresponding desired output.
0.036145 - This algorithm runs as follows: All the weight vectors w are initialized with small random values from a pseudorandom sequence generator.
0.042359 - Then and until the convergence (i.e., when the error E is below a preset value) we repeat the three basic steps: • The weight vectors wi are updated by (3) • Where (4) • Compute the error E(t + 1), where t is the iteration number, w is the weight vector, and η is the learning rate.
0.042670 - The backpropagation algorithm adapts the weights and the thresholds of the neurons in a way that minimizes the error function E (5) where yp is the actual output and dp the desired output for input pattern p. The minimization of E can be accomplished by gradient descent, i.e., the weights are adjusted to change the value of E in the direction of its negative gradient.
0.036832 - The exact updating rules can be calculated by applying derivatives and the chain rule (for the weights between the input and the hidden layer).
0.049020 - Support Vector Machines In this section, the basic concept of SVM will be briefly described.
0.009456 - More thorough descriptions can be found in (Burges, 1998; Hsu, Chang, & Lin, 2003; Theodoridis & Koutroumbas, 2008).
0.071970 - A typical two class problem as Fig 4 shows is similar to the problem of diagnosing urological patients as either ill or healthy.
0.056738 - The mapping between input space and feature space in a two class problem with… Fig 4.
0.056738 - The mapping between input space and feature space in a two class problem with the SVM.
0.033816 - Mapping the training data non-linearly into a higher dimensional feature space via function φ. H1 and H2 are parallel since they have the same normal w and perpendicular distance from the origin, ∣±1 − b∣/∥w∥, and that no training points fall between them.
0.049020 - The support vectors are the gray triangles and circles respectively located on H1 and H2.
0.088300 - The distance from w to these support vectors is 1/∥w∥ and the margin is simply 2/∥w∥.
0.039360 - For a two class classification problem, it is necessary to first try to estimate a function using training data, which are l N-dimensional patterns xi and class labels yi, where (6) such that f will classify new samples (x,y) correctly.
0.043528 - Given this classification problem the SVM classifier, as described by Vapnik (1995), Guyon, Boser, and Vapnik (1992) and Cortes and Vapnik (1995), satisfies the following conditions: (7) which is equivalent to (8) Here training vectors xi are mapped into a higher dimensional space by the function φ.
0.038394 - The equations of (2) construct a hyperplane wTφ(xi) + b = 0 in this higher dimensional space that discriminates between the two classes showed in Fig 4.
0.071970 - Each of the two half-spaces defined by this hyperplane corresponds to one class, H1 for yi = +1 and H2 for yi = −1.
0.045375 - Therefore the SVM classifier corresponds to decision functions: (9) Thus the SVM finds a linear separating hyperplane with the maximal margin in this higher dimensional space.
0.069276 - The margin of a linear classifier is the minimal distance of any training point to the hyperplane which is the distance between the dotted lines H1 and H2 and the solid line showed in Fig 4.
0.046300 - The points x which lie on the solid line satisfy wTφ(xi) + b = 0, where w is normal to the hyperplane, ∣b∣/∥w∥ is the perpendicular distance from the hyperplane to the origin, and ∥w∥ is the Euclidean norm of w. 1/∥w∥ is the shortest distance from the separating hyperplane to the closest positive (negative) example.
0.047619 - Therefore, the margin of a separating hyperplane will be 1/∥w∥ + 1/∥w∥.
0.064394 - To calculate the optimal separating plane is equivalent to maximizing the separation margin or distance between the two dotted lines H1 and H2.
0.040084 - It has to be considered that H1: wTφ(xi) + b = 1 and H2: wTφ(xi) + b = −1 are parallel since they have the same normal w and perpendicular distance from the origin, ∣1 − b∣/∥w∥ for H1 and ∣ −1 − b∣/∥w∥ for H2, and that no training points fall between them.
0.072650 - Thus we expect the solution for a typical two dimensional problem to have the form shown in Fig 4.
0.048023 - Those training points which gives equality in (3) are lying on one of the hyperplanes H1 and H2 are called support vectors and they are indicated in Fig 4 by means of a gray color.
0.042885 - In practice, a separating hyperplane may not exist, i.e., if a high noise level causes a large overlap of the classes.
0.035760 - To allow for the possibility of examples violating the edges of the margin (when perfect separation is not possible) (Martin, Fowlkes, & Malik, 2003), one introduces slack variables (Bennett & Mangasarian, 1992; Cortes & Vapnik, 1995; Scholkopf, Smola, Williamson, & Bartlett, 2000; Vapnik, 1995).
0.046816 - (10) in order to relax the constraints to (11) Fig 4 shows the basic idea of SVM, which is to map the data into some other dot product space (called the feature space) F via a nonlinear map (12) and perform the above linear algorithm in F. As can be seen, this only requires the evaluation of dot products.
0.084595 - (13) Clearly, if F is high-dimensional, this approach is not feasible and we have to find a computationally cheaper way.
0.066225 - The key observation then is to substitute it for a simple kernel K that can be evaluated efficiently.
0.038251 - Kernel substitution is a method for using a linear classifier algorithm to solve a non-linear problem by mapping the original non-linear observations into a higher-dimensional space (as it is indicated in Fig 4), where the linear classifier is subsequently used; this makes a linear classification in the new space equivalent to non-linear classification in the original space.
0.021248 - That transformation is done using Mercer’s theorem (Williamson, Smola, & Scholkopf, 2001), which states that any continuous, symmetric, positive semi-definite kernel function K(x, x′) can be expressed as a dot product in a high-dimensional space.
0.068966 - In that point the tools to construct nonlinear classifiers are defined.
0.044888 - We substitute Φ(xi) for each training example xi, and perform the optimal hyperplane algorithm in F. Because we are using kernels, we will thus end up with nonlinear decision function of the form (14) where K(x, x′) is called the Kernel function, whose value is equal to the inner product of two vectors x and x′ in the feature space; namely, K(x, x′) = φ(x)Tφ(x′).
0.030534 - Through the Kernel function, all computations are performed in the low-dimensional input space.
0.027397 - Any function that satisfies the Mercers condition (Courant & Hilbert, 1953) can be used as the Kernel function.
0.045802 - There are many possibilities to define a function to be used as a Kernel.
0.057315 - However, typical examples of kernels used in SVM, which have been successfully applied to a wide variety of applications, are linear, polynomials, radial basic functions and hyperbolic tangent: (15) (16) (17) (18)
0.045802 - Semen parameters The semen analysis procedures are standardized by a WHO publication (WHO, 1999).
0.052356 - This publication establishes the reference value for the different semen parameters and the nomenclature (Eliasson et al., 1970) that describes the deviations from these reference values.
0.022989 - This nomenclature includes: • Normozoospermia: all values are over the lower limit.
0.040230 - • Asthenozoospermia: percentage of progressively motile spermatozoa below the lower reference limit.
0.042042 - • Oligozoospermia: total number of spermatozoa below the lower reference limit.
0.040230 - • Teratozoospermia: percentage of morphologically normal spermatozoa below the lower reference limit.
0.033058 - Of all men surveyed, almost 38% had some alteration in sperm parameters.
0.033167 - By studying in detail the individuals with altered parameters the most frequently found seminal alteration was asthenozoospermia in 47.37% of cases, followed by oligozoospermia in 21.05%.
0.078431 - The combined changes astenoteratozoospermia and oligoasthenozoospermia shown a frequency of 10.53% in both cases.
0.049020 - Finally, the less frequent alteration was teratozoospermia, present in 2.63% of the cases alterations.
0.106796 - In this study we use these laboratory results of the seminal analysis as a control to evaluate the accuracy in the prediction of the diagnosis of the ANN developed.
0.075650 - This ANN was only based in the answers to the questionary of the individuals who participate.
0.093135 - Development of the AI methods 3.2.1.
0.081816 - Decision tree In this section we explain the algorithm C4.5, which is the one we have decided to use for the performance of the decision tree.
0.057274 - C4.5 is an extension of the basic ID3 algorithm designed by Quinlan (Quinlan, 1986, 1993) to address some issues not dealt with by ID3, such as avoiding overfitting the data, handling training data with missing attribute values and improving computational efficiency among other less important.
0.037736 - The ID3 algorithm can be summarized as follows: 1.
0.044077 - Take all unused attributes and count their entropy concerning test samples 2.
0.010178 - Choose attribute for which entropy is minimum (or, equivalently, information gain is maximum) 3.
0.066515 - Make node containing that attribute The decision of choosing DT as the first method is due on one hand that it is the most useful technique for physicians that find on it a similar way that they use in their usual procedure.
0.071625 - Furthermore, DT will help us to reduce the number of input data.
0.060032 - Research indicates that application of data dimensionality reduction as a pre-step to the classification procedure does improve the classification accuracy (Fu & Wang, 2003; Raymer, Punch, Goodman, Kuhn, & Jain, 2000).
0.037267 - Furthermore, feature selection can also provide a better understanding of the underlying process that generated the data (Guyon & Elisseeff, 2003).
0.080808 - It is beneficial, and also demanded in many cases, to limit the number of input features when building classification systems in order to have a good predictive and less computationally intensive model (Zhang, 2000).
0.034483 - Fig 5 shows the execution carried out with a decision tree.
0.044118 - This figure could help to establish what are the fields more important for the output.
0.062331 - In fact, once the tree is obtained we have carried out other tests where the input data are reduced to only the fields that appear in the tree of the figure, that in our case are five.
0.031746 - This figure shows the execution carried out with a decision tree Fig 5.
0.034483 - This figure shows the execution carried out with a decision tree.
0.044118 - This figure could help to establish what are the fields more important for the output.
0.069382 - MLP For the construction of the MLP architecture we have proceeded as follows: (a) Layer 1 corresponds directly to the input vector, that is, all the parameters/fields of the input record.
0.029304 - (b) Layer 2 (the hidden layer).
0.057708 - The number of hidden neurons for this layer is the most elaborated question in the network’s architecture.
0.071247 - This number represents a trade off between performance and the risk of over fitting.
0.060332 - In fact, the number of neurons in a hidden layer will significantly influence the ability of the network to generalize from the training data to unknown examples (Pal & University of Nottingham, 2002).
0.062657 - In order to keep a good generalization and consequently a high accuracy, without getting over fitting, the experiments carried out showed that a low number of neurons for this layer lead to a poor performance for both training and test sets.
0.068100 - At the opposite edge, a high number of neurons performs very well for training and test sets, although the risk of over fitting is high.
0.048193 - The various tests carried out between these extreme options leads to meet the optimal solution for this layer with 21 neurons.
0.027778 - (c) Layer 3 (the output layer) (Classification).
0.072072 - It has two outputs for classification/prediction: normal and altered.
0.066277 - The learning algorithm used is backpropagation with adaptive learning rate, constant momentum and an optimized algorithm based on the gradient descent method.
0.037736 - The backpropagation training parameters are shown in Table 2.
0.000000 - Table 2.
0.000000 - Backpropagation training parameters.
0.000000 - Parameters Value Learning rate 0.01 Adaptive learning rate 0.1 Constant momentum 0.2 Epochs 100–1000-10000 Minimum performance gradient 3.2.3.
0.039427 - SVM In this study, Radial Basis Functions Kernel (RBF) function has been adopted because we believe that it is a suitable choice for our problem.
0.034483 - The RBF kernel nonlinearly maps samples into a higher dimensional space.
0.045549 - Thus, unlike the linear kernel, it can handle the case when the relation between class labels and attributes is nonlinear.
0.044260 - Furthermore, the linear kernel is a special case of RBF as (Keerthi & Lin, 2003) shows that the linear kernel with a penalty parameter C has the same performance as the RBF kernel with some parameters (C,γ).
0.030534 - In addition, the sigmoid kernel behaves like RBF for certain parameters (Lin & Lin, 2003).
0.045977 - Moreover, the number of hyperparameters influences the complexity of model selection.
0.036036 - The polynomial kernel has more hyperparameters than the RBF kernel.
0.039604 - Finally, the RBF kernel has less numerical difficulties.
0.054422 - One key point is 0 < Kij ⩽ 1 in contrast to polynomial kernels of which kernel values may go to infinity or zero while the degree is large.
0.052980 - Performance Frequently, the complete data set is divided into two subsets: the training set and the test set.
0.069808 - Here, the training set is used to determine the system parameters, and the test set is used to evaluate the diagnosis accuracy and the network generalization.
0.100985 - Cross-validation has been widely used to assess the generalization of a network.
0.044199 - The cross-validation estimate of accuracy determining by the overall number of correct classifications divided by the total number of examples in the dataset.
0.048148 - (19) where n is the size of the dataset S, xi is the example of S, yi is the target of xi, and Si is the probable target of xi by the classifier function I.
0.074074 - Therefore: (20) Specifically, for this study we have applied a ten-fold cross-validation method for the performance assessment of every network.
0.056225 - The data has been divided in ten sets (S1, S2, …, S10) and the ten experiments performed are shown in Fig 6.
0.031008 - Cross validation method Fig 6.
0.035088 - Cross validation method.
0.047281 - The cycle simulates the behavior of a wheel turning each of the portion at a time.
0.098713 - In other words, for the first experiment we have S2 up to S10 as the training parts and S1 is the test part.
0.045662 - This procedure will continue for 10 times; each of them it will permute the 10 different parts.
0.052980 - The test data are chosen randomly from the initial data and the remaining data form the training data.
0.052009 - The method is called 10-fold cross validation since this process has been performed ten times.
0.034483 - The function approximation fits a function using the training set only.
0.075368 - Then the function approximation is asked to predict the output values for the data in the testing set.
0.044177 - The errors it makes are accumulated to provide the mean absolute test set error, which is used to evaluate the model.
0.112943 - The method to evaluate these four AI methods is to obtain some measures as classification accuracy, sensitivity, specificity, positive predictive value, negative predictive value and a confusion matrix.
0.048565 - A confusion matrix (Kohavi & Provost, 1998) contains information about actual and predicted classifications done by a classification system.
0.097466 - The experimentation has been done three times in order to analyze the predictive value of each one of the seminal parameter studied.
0.076923 - The reason to do that is that Sperm Concentration, Motility and Morphology are pathologies which have a different origin.
0.022039 - Therefore, the individuals with those different alterations can not be grouped together.
0.062893 - This will be exposed in the following three sections.
0.039427 - Sperm concentration Table 3 shows the confusion matrix provided for our classifiers, the MLP, SVM and DT running a two classes problem: Assisted reproduction database.
0.000000 - Table 3.
0.049242 - Definition of the confusion matrix (for the Assisted reproduction dataset) with the values for every measure of the MLP, SVM and DT classifiers.
0.059361 - The values of the confusion matrix are normal and altered, where altered means the Sperm concentration parameter.
0.038647 - Actual MLP SVM DT Predicted Predicted Predicted P N P N P N P 80 5 83 2 82 3 N 9 6 12 3 13 2 In this case represents the values Normal and Altered, where altered means the Sperm concentration parameter.
0.055016 - Classification accuracy, sensitivity, specificity, positive predictive value and negative predictive value (represented by the Table 4) can be defined by using the elements of the confusion matrix (Table 3).
0.101781 - Every equation has three values according to the classifiers MLP, SVM and DT, respectively.
0.000000 - Table 4.
0.094340 - Equations according to the MLP, SVM and DT classifiers.
0.047968 - MLP (%) SVM (%) DT (%) Classification accuracy (%) 86 86 84 Sensitivity (%) 94.1 97.7 96.5 Specificity (%) 40 20 13.3 Positive predictive value (%) 89.9 87.4 86.3 Negative predictive value (%) 54.5 60 40 Sperm concentration is the seminal parameter that shows the highest prediction accuracy of the three parameter.
0.056225 - Percentage of motile sperm Table 5 shows the values Normal and Altered, where altered means the percentage of motile sperm parameter.
0.055016 - Classification accuracy, sensitivity, specificity, positive predictive value and negative predictive value (represented by the Table 6) can be defined by using the elements of the confusion matrix (Table 5).
0.000000 - Table 5.
0.049242 - Definition of the confusion matrix (for the Assisted reproduction dataset) with the values for every measure of the MLP, SVM and DT classifiers.
0.061810 - The values of the confusion matrix are normal and altered, where altered means percentage of motile sperm parameter.
0.012945 - Actual MLP SVM DT Predicted Predicted Predicted P N P N P N P 60 11 70 1 58 13 N 16 13 23 6 17 12 Table 6.
0.094340 - Equations according to the MLP, SVM and DT classifiers.
0.040520 - MLP (%) SVM (%) DT (%) Classification accuracy (%) 73 76 70 Sensitivity (%) 84.5 98.6 81.7 Specificity (%) 44.8 20.7 41.4 Positive predictive value (%) 79 75.3 77.3 Negative predictive value (%) 54.2 85.7 48 Although the accuracy of motility is slightly lower than concentration we can also predict it with a significant accuracy.
0.045662 - Normal morphology Table 7 shows the values Normal and Altered, where altered means the Normal morphology parameter.
0.055016 - Classification accuracy, sensitivity, specificity, positive predictive value and negative predictive value (represented by the Table 8) can be defined by using the elements of the confusion matrix (Table 7).
0.000000 - Table 7.
0.049242 - Definition of the confusion matrix (for the Assisted reproduction dataset) with the values for every measure of the MLP, SVM, and DT classifiers.
0.061466 - The values of the confusion matrix are normal and altered, where altered means Normal morphology parameter.
0.012945 - Actual MLP SVM DT Predicted Predicted Predicted P N P N P N P 67 25 68 24 66 26 N 6 2 7 1 7 1 Table 8.
0.094340 - Equations according to the MLP, SVM and DT classifiers.
0.040520 - MLP (%) SVM (%) DT (%) Classification accuracy (%) 69 69 67 Sensitivity (%) 72.8 73.9 71.7 Specificity (%) 25 12.5 12.5 Positive predictive value (%) 91.8 90.7 90.4 Negative predictive value (%) 7.4 4 3.7 Although the accuracy of motility is slightly lower than concentration we can also predict it with a significant accuracy.
0.143159 - Morphology seems to be less associated with environmental factors and life habits than the previous two seminal parameters.
0.089947 - In general, the values for specificity and Positive predictive value present lower accuracy.
0.055096 - This happens even when the classification and the confusion matrix are good.
0.055556 - The explanation is in the unbalanced classes.
0.033058 - The altered output has always much less registers than the normal ones.
0.067669 - The excellent results provided in Table 3 by means of the confusion matrix as well as the equations of classification accuracy, sensitivity, specificity, positive predictive value and negative predictive value lead us to a convergence which is proposed in Fig 7.
0.086207 - This figure represents two options of diagnosis: laboratory and AI approach.
0.051755 - This former one is easier (and especially more comfortable and cheaper) since it is only necessary to fill a questionnaire, besides it could be used as a first step to decide whether or not it is useful to do the laboratory approach.
0.047281 - This figure represents how it is possible to reach a diagnosis either with a… Fig 7.
0.054215 - This figure represents how it is possible to reach a diagnosis either with a laboratory approach, which include expensive tests, sometime uncomfortable for the patients, or with an AI approach, which is easier since it is only necessary to fill a questionnaire.
0.068531 - Sometimes, this former approach could be a first step to decide whether or not is useful to do the laboratory approach.
0.143589 - In this paper, we have evaluated the performance of three AI methods, DT, MLP and SVM, and its application in the prediction of the male fertility potential.
0.119252 - Based on the data obtained from 100 volunteers between 18 and 36 years we show the relationship of life habits and environmental factors with semen parameters.
0.045662 - The results have established that all these techniques achieve a high accuracy regarding the different measurement parameters.
0.061810 - Specificity and positive predictive value present lower percentages, even when the classification and the confusion matrix are good.
0.058091 - The possible explanation of this is that the study population shows an imbalanced distribution (Sun, Kamel, Wong, & Wang, 2007), and there are a bigger number of individuals with normal semen parameters than the ones with alterations.
0.057823 - In general MLP and SVM are the methods that achieve the highest accuracy, being SVM the one with the higher Sensitivity whereas MLP obtains superior Specificity values.
0.155370 - Therefore, these two methods seem to be useful tools for the predictions of the seminal parameters from environmental factors and life habits, especially because of their generalization potential with new registers.
0.095421 - However, the slightly lower accuracy of DT is compensated by some advantages, such as to be simple (in a visual and illustrative way) to understand and interpret.
0.050891 - In general, people are able to understand decision tree models after a brief explanation.
0.000000 - It requires little data preparation.
0.000000 - Other techniques often require data normalization.
0.062615 - In addition, in this paper DT has been used also as a first step in data preprocessing in order to reduce the input data.
0.034188 - After this phase, MLP as well as SVM have carried out tests with a lower number of input data.
0.056087 - ANN has been used, in the field of reproductive biology, to predict the results of IVF/ICSI (Kaufmann, Eastaugh, Snowden, Smye, & Sharma, 1997; Kshirsagar et al., 2005; Wald et al., 2005), to assess sperm morphology (Linneberg, Salamon, Svarer, Hansen, & Meyrowitsch, 1994) and to predict the presence of sperm in testes of men with nonobstructive azoospermia (Ma, Chen, Wang, Hu, & Huang, 2011; Samli & Dogan, 2004).
0.133876 - Other previous studies, trying to identify the relationship between environmental factors and semen quality have shown several lacks and limitations.
0.088611 - Most of those problems are related to the complexity of the information treated due to its diversity and heterogeneous data types.
0.035088 - In our opinion, the problem is that previous works (Li, Lin, Li, & Cao, 2011) have approached the study using linear statistical techniques.
0.063018 - In this paper, we develop the use of non linear techniques that may allow a better approach of the complexity of the problem (Almeida, 2002; Cleophas & Cleophas, 2009).
0.090417 - Due to the high accuracy obtained with the different classification methods, they seems to be an alternative to more expensive laboratory tests, at least during the initial moment of the fertility study of the couple, as well as in the selection of semen donors.
0.061947 - It is also important to highlight that the study population of this work presents close characteristics to regular candidates to become semen donors (i.e., young male, university students) (Thorn, et al., 2008).
0.091582 - These AI methods can also help to make decision in the next steps of the infertility treatment, focusing or not on the male partner and avoiding painful and expensive examinations on the female.
0.033898 - The authors have experience in the field of urology by applying several AI techniques in classification tasks with very good results (Gil & Johnsson, 2010b; Gil et al., 2009, Gil, Johnsson, Garcia Chamizo, Paya, & Fernandez, 2011).
0.062112 - This makes it possible to adapt the artificial intelligence to meet the needs of the data analyzed in this study.
0.075167 - In future studies, we will investigate the effect of imbalanced classes on classification performance when developing AI methods, like MLP, SVM and DT, for decision support systems for medical diagnosis (Mazurowski et al., 2008).
0.079783 - Moreover, we suggest the use of machine learning methods, as well as data mining, which may provide a more precise correlation between seminal data and the information collected by means of the questionnaires.
0.078431 - The authors in association with a Reproductive Fertility Clinic are collecting more amount of data.
0.070640 - For this purpose we will create data warehousing where will be easier to merge historical and current data.
0.059406 - The benefit of using data warehousing is twofold.
0.063761 - On one hand, it will improve the accuracy of our systems.
0.068493 - On the other hand, the it will supports performance improvement and it will tend to equilibrate classes.
0.111056 - In conclusion, to our knowledge, this is the first time that MLP, SVM and DT have been compared addressing the problem of the relationship between life habits and semen quality.
0.085583 - MLP and SVM show the highest prediction accuracy, with an accuracy of 86% for the sperm concentration and an accuracy between and 73% 76% for the motility.
0.096795 - Although morphology seems to be less associated with environmental factors and life habits and probably with a major link to genetic alterations (Yatsenko et al., 2011) it also gets better accuracy with MLP and SVM.
0.063725 - The clarity, reduction and simplification of the problem provided by DT must also be highlighted.
0.087767 - We also suggest further studies using this methodology to identify those factors that affect specifically to some of the seminal parameters, to provide some recommendations that can improve the male fertility potential.
0.052805 - 1 These authors equally contributed to this work.

[Frase 17] In this paper, assuming the influence of environmental factors and life habits in semen quality, we compare the prediction accuracy of three different artificial intelligence (AI) methods, Multilayer Perceptron (MLP), Support Vector Machines (SVM) and decision trees (DT), to determine the best Decision Support Systems (DSS) that can help in the evaluation of male fertility potential.
[Frase 4] In this paper we compare three artificial intelligence techniques, decision trees, Multilayer Perceptron and Support Vector Machines, in order to evaluate their performance in the prediction of the seminal quality from the data of the environmental factors and lifestyle.
[Frase 269] Due to the high accuracy obtained with the different classification methods, they seems to be an alternative to more expensive laboratory tests, at least during the initial moment of the fertility study of the couple, as well as in the selection of semen donors.
