The need for a comparison between two Gaussian Mixture Models ( GMMs ) plays a crucial role in various pattern recognition tasks and is involved as a key components in many expert and artificial intelligence ( AI ) systems dealing with real-life problems .
By doing so , we manage to obtain much better trade-off between the recognition accuracy and the computational complexity , in comparison to the measures between GMMs utilizing distances between Gaussian components evaluated in the original parameter space .
We evaluate the proposed GMM measure on artificial , as well as real-world experimental data obtaining a much better trade-off between recognition accuracy and the computational complexity , in comparison to all baseline GMM similarity measures tested .
The Kullbackâ€“Leibler ( KL ) divergence ( Kullback , 1968 ) , as the most natural informational distance between two probability distributions p , q is also the most appropriate to be used as the mentioned similarity measure .

