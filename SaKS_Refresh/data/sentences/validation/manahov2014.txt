We developed various artificial stock markets populated with different numbers of traders using a special adaptive form of the Strongly Typed Genetic Programming (STGP)-based learning algorithm.
We then applied the STGP technique to historical data from three indices – the FTSE 100, S&P 500, and Russell 3000 – to investigate the formation of stock market dynamics and market efficiency.
We used several econometric techniques to investigate the emergent properties of the stock markets.
We have found that the introduction of increased heterogeneity and greater genetic diversity leads to higher market efficiency in terms of the Efficient Market Hypothesis (EMH), demonstrating that market efficiency does not necessarily correlate with rationality assumptions.
We have also found that stock market dynamics and nonlinearity are better explained by the evolutionary process associated with the Adaptive Market Hypothesis (AMH), because different trader populations behave as an efficient adaptive system evolving over time.
Hence, market efficiency exists simultaneously with the need for adaptive flexibility.
Our empirical results, generated by a reduced number of boundedly rational traders in six of the stock markets, for each of the three financial instruments do not support the allocational efficiency of markets, indicating the possible need for governmental or regulatory intervention in stock markets in some circumstances.
A few decades ago, the Efficient Market Hypothesis (EMH) was very widely accepted by academic financial economists.
In broad terms, the EMH postulates that market prices should reflect all available information.
As a consequence, market prices should always be consistent with their fundamental values.
The hypothesis was independently developed by Samuelson (1965), Fama (1963, 1965a, 1965b, 1970).
Samuelson (1965) generated a series of non-linear programming solutions to spatial pricing models with no uncertainty, and proposed that in informationally efficient markets price changes are unpredictable if market prices fully incorporate the information disseminated from all market participants.
Since then, the concept of efficient markets has been applied to many theoretical models and empirical studies of asset prices, generating several controversial debates.
Advocates of the EMH, such as Jensen (1978), have argued that there is no other proposition in economics which has more solid supporting empirical evidence than the EMH.
In the late 1970s, the EMH continued its transition from theory to doctrine.
Thompson (1978), Galai (1978), Charest (1978) and Davidson and Froyen (1982), and in the early 1990s, Nichols (1993), Conrad (1995) and Shanken and Smith (1996) provided evidence that supported the EMH.
Malkiel (2003) suggested that stock markets are more efficient and rather less predictable than many academic studies would have us believe.
By the beginning of the twenty-first century, the academic dominance of the EMH had somewhat diminished.
A group of researchers equipped with anomalous evidence inconsistent with the EMH suggested that the EMH should be replaced by a behavioural finance approach (Haugen, 1999; Koonce, 2001; Schleifer, 2000; Thaler, 1993; Shiller, 2003).
In the behavioural approach investors are not necessarily assumed to be rational utility maximising agents but instead may have behavioural biases.
In this context, it has been empirically observed that financial markets do not process information instantaneously (Chan, Jegadeesh, & Lakonishok, 1996), and that markets can overreact as a result of investor optimism or pessimism (Dissanaike, 1997).
Furthermore, many empirical studies support the fact that markets are predictable even on the basis of past market prices (Brock, Lakonishok, & LeBaron, 1992; Chiarella & He, 2003; Jegadeesh & Titman, 2001).
The controversy intensified and researchers began to question whether the EMH could ever be validated or discredited (Langevoort, 1992).
Some researchers has investigated the properties of agents and their interactions with more sophistication than either the EMH or standard behavioural finance.
The concepts of heterogeneity, bounded rationality, and evolutionary adaptive agents have been explored by Brock and Hommes (1997a, 1997b, 1998a, 1998b), Chiarella and He (2002a, 2002b), Gaunersdorfer and Hommes (2000) and Hommes (2001).
A few years later, in an attempt to accommodate most of the complexities of the real world, Lo (2004) and Lo (2005) proposed the Adaptive Market Hypothesis (AMH).
This hypothesis modifies the EMH paradigm to suggest that the forces that drive prices to their efficient levels are not always dominant and the processes of learning, competition and evolutionary selection pressures govern these forces.
Nevertheless, the majority of the studies related to market efficiency and adaptability have a major shortcoming.
They have failed to investigate the relation between market diversity and market efficiency, and the impact of individual learning and adaptability on the diversity of traders’ expectations.
Chen and Yeh (2001) expressed the view that market size could potentially have a dramatic impact on market efficiency.
The questions our study is trying to answer are whether market organisation influences traders’ strategies and in turn market efficiency, and whether market structure affects individual learning in the AMH.
In this study, we developed ten stock markets, each populated by different numbers of artificial traders, for each of the FTSE 100, S&P 500 and Russell 3000 indices.
We also implemented a special adaptive learning form of Genetic Programming (GP) called Strongly Typed Genetic Programming (STGP), in order to investigate the relationship between market efficiency and adaptability (the computational nature of GP and STGP is described in Appendices A and B).
The reason for using STGP is because Lo (2004) and Lo (2005) regarded the market as an evolutionary process where the principles of evolution such as competition, adaptation, and natural selection are applicable to financial markets.
In this sense, the artificial traders in our experiment can be considered to be agents that adapt, learn, evolve, and try to survive.
The random nature of the initial trading rules of the agents allowed us to observe how they learn, adapt, and survive (the worst performing traders were replaced).
The scientific advantage of the STGP over conventional GP used in most studies so far is that STGP evaluates the fitness of agents through a dynamic fitness function which processes the most recent data (quotes of the three indices in our experiment), rather than a re-execution of the same trading rules.
We then empirically evaluated the price series of these three indices to investigate the relationship between markets populated by different numbers of heterogeneous agents with different dynamics and the validity of the EMH and the AMH.
We also explored the dynamic behaviour of the models when testing for the presence of nonlinearity.
Despite the voluminous literature on the topic, no other study has implemented the STGP technique with a large number of artificial agents, which has enabled us to develop of a wider variety of trading rules.
Our financial markets can, therefore, be viewed as co-evolving ecologies of different trading strategies.
These strategies are analogous to a biological species, and the amount of funds deployed by traders following a given strategy is analogous to the population of that species (Farmer & Lo, 1999).
The presence of 10,000 heterogeneous and interacting adaptive traders, rich in dynamics, provides the opportunity to study the stock market as a complex adaptive system.
Artificial traders are, by definition, capable of adapting, learning, and evolving, which makes them extremely suitable for the analysis of market efficiency and adaptability, because adaptation and learning in heterogeneous structures are known as important tools for analysing financial market behaviour (Hommes, 2001).
Hommes (2011) argued that heterogeneity is a critical aspect of the theory of expectations, because a model of heterogeneous expectations can explain different aggregate outcomes across different market settings.
To summarise, the contributions of this study are as follows.
Firstly, we are the first to apply the STGP technique in the analysis of market efficiency and adaptability, whilst taking into account different market structures and individual trader cognitive abilities and heterogeneity.
Recent studies, such as Urquhart and Hudson (2013), Soufian, Forbes, and Hudson (2013), Zhou and Lee (2013), Zhou, Gu, Jiang, Zhang, and Zhou (2014), Ghazani and Aragli (2014), Verheyden, Van den Bossche, and De Moor (2014) and Hull and McGroarty (2014), suggest that the AMH better describes the behaviour of stock returns than the EMH.
However, these authors based their conclusions entirely on econometric tests or theoretical hypotheses only, and failed to observe the processes of adaptation, learning, competition, and evolutionary selection pressures that govern the AMH.
Our study aimed to fill this gap by providing significant empirical findings combined with evidence gained from evolutionary dynamic processes.
Secondly, the conditions under which the EMH or the AMH are appropriate have not been appropriately studied.
We hope that the solid empirical evidence that we present will shed light on the formation of stock market dynamics and the formalisation of both hypotheses within artificial laboratory stock market settings.
Thirdly, we have found that different trader populations behave as an efficient adaptive system.
We observed that market efficiency is not necessarily associated with rational assumptions and that nonlinear dependence in index returns evolve over time.
Hence, we think that market efficiency is not a static characteristic as assumed in most of the studies published so far.
Our findings are consistent with the perception of financial markets as adaptive systems subject to evolutionary selection pressures.
Emerson, Hall, and Zalewska-Mitura (1997) and Zalewska-Mitura and Hall (1999) see markets as a continuous process of evolving efficiency and when market participants become more experienced the level of efficiency will gradually improve.
The remainder of this paper is organised as follows: Section 2 presents the background and a review of the literature in the field together with some discussion on the relevant contributions of this study; Section 3 discusses the experimental design; Section 4 discusses the simulation results and the paper concludes in Section 5.
Origins and supporting empirical evidence on the EMH In this subsection we discuss the history of the EMH and the empirical evidence supporting the hypothesis.
More than a century ago, Bachelier (1900) analysed the mathematical theory of random processes and expressed the view that stock price movements follow a Brownian motion and that, therefore, stock prices are unpredictable.
Many years later, Samuelson (1965) generated a series of non-linear programming solutions to spatial pricing models with no uncertainty, and proposed that the price changes in informationally efficient markets are unpredictable if market prices fully incorporate the information disseminated from all market participants.
In four different seminal papers, Fama (1963), Fama (1965a), Fama (1965b) and Fama (1970) measured the statistical properties of market prices and thus operationalised the EMH in empirical terms.
Fama (1970) reviewed the empirical evidence gained in the 1960s and proposed three major versions of the EMH: (a) the weak form, (b) the semi-strong form, and (c) the strong form relating to efficiency in relation to historic information, public information and all information respectively.
During the early years of development, the EMH gained massive academic attention and approval.
Jensen (1978) stated that ‘there is no other proposition in economics which has more solid empirical evidence supporting it than the EMH’.
Thompson (1978), Galai (1978), Charest (1978) and Davidson and Froyen (1982) provided empirical evidence in support of the EMH.
Studies by Nichols (1993), Conrad (1995), Shanken and Smith (1996), Bernanke and Frank (2006), Schuster (2006) and Chen and Diaz (2013) generated further support for market efficiency.
Malkiel (2003) acknowledged that market participants are not fully rational and that predictable patterns in stock returns can appear for short periods of time, but that stock markets are more efficient and less predictable than many recent research papers demonstrate.
Mobarek and Fiorante (2014) apply a bias – free statistical technique to daily data of the equity markets in Brazil, Russia, India and China (BRIC) and detect the existence of weak – from efficiency.
Chen and Yeh (1997) approached the EMH through the application of Genetic Programming (GP).
The authors tested a short-term sample of TAIEX (Taiwan index) and the S&P 500 and concluded that the EMH is sustained, although they also confirmed the existence of short-term nonlinear regularities.
However, these nonlinear regularities could not be exploited profitably due to the high search costs involved in the process of discovering them.
Although Chen and Yeh’s study was the first to use intelligence systems (GP) to demonstrate market efficiency, we apply more sophisticated expert systems (STGP) to provide evidence of adaptive rather than efficient markets.
The differences between GP and STGP are explained in Appendix B.
Challenging empirical evidence on the EMH In this subsection we outline the general theoretical and empirical evidence that has been put forward against the EMH.
From a theoretical viewpoint Grossman and Stiglitz (1980) argued that if financial markets were efficient, there would not be any profit generated through information gathering, therefore, there would be no reason to trade and the markets would eventually collapse.
Thus, there must be profit-making- opportunities to compensate investors for the cost of acquiring information and trading.
The ‘behavioural finance’ group of researchers equipped with anomalous empirical evidence against the validity of the EMH, challenged the advocates of market efficiency (Haugen, 1999; Koonce, 2001; Schleifer, 2000; Shiller, 2003; Thaler, 1993).
Chan et al.
(1996) investigated whether the predictability of future returns from past returns is based on the market’s under reaction to information associated with past earning news.
The authors found insignificant evidence of subsequent reversals in the returns of stocks with high price and earning momentum, suggesting that financial markets respond gradually to new information.
Studies by De Bondt and Thaler (1985), De Bondt and Thaler (1987), Kahneman and Tversky (1982), Arrow (1982) and Dissanaike (1997) demonstrated that investors do not behave in a rational way but tend to overreact, building the foundations of the stock market Overreaction Hypothesis (ORH).
The ORH, postulates that if stock prices systematically overshoot as a result of excessive investor optimism or pessimism, price reversals should be predictable from past price performances (Dissanaike, 1997).
Brock et al.
(1992) implemented two simple trading rules using historic data the – moving average and trading break rules – to analyse the Dow Jones movements from 1897 to 1986, and found strong support for such technical trading strategies being able to predict the market.
Jegadeesh and Titman (1993) and Jegadeesh and Titman (2001) provided evidence of substantial momentum trading profits that were not a product of data snooping, confirming the assumptions behind the behavioural models.
More recent behavioural finance studies also provide evidence contradicting the EMH (Bollen, Mao, & Zeng, 2011; Casarin & Squazzoni, 2012; Joarder, Ahmed, Haque, & Hasanuzzaman, 2014; Kleinnijenhuis, Schultz, Oegema, & Atteveldt, 2013; Soares, Herling, Lima, & Moritz, 2014).
Ongoing debate about the EMH and the emergence of the AMH In this subsection we discuss prior research on the bounded rationality of agents and the interactions between them and the market and the closely related Adaptive Market Hypothesis (AMH).
As this area is one of the most related to our work we also point out some specific contributions we make to the literature.
Sargent (1993) used the notion of bounded rationality as opposed to perfect rationality, to describe how traders with limited information about fundamental values develop expectation price models.
In this view of the world traders are not irrational, but adopt optimal beliefs – considering the limited amount of information they possess.
The endogenous nature of uncertainty of the state of the world does not allow traders to develop life-time optimisation strategies in favour of more simple reasoning and rules of thumb (Shefrin, 2000).
Brock and Hommes (1997a), Brock and Hommes (1997b), Brock and Hommes (1998a) and Brock and Hommes (1998b) demonstrated that evolutionary adaptive systems with many heterogeneous agents, which implemented various trading strategies, represent nonlinear systems capable of generating a wide variety of stylised facts about the stock markets.
The authors observed an evolutionary competition between trading strategies where traders implemented their strategies according to an evolutionary fitness measure, such as accumulated past profits.
They proposed that financial markets can be modelled as Adaptive Belief Systems populated by boundedly rational traders.
However, artificial traders within the Adaptive Belief Systems were allowed to choose from a finite set of different beliefs with predictor selection based on past realised profits.
We evaluate the adaptation of traders through a dynamic fitness function which enables the return estimation period to move forward rather than looking at high past profits.
Hommes (2000) studied financial markets using evolutionary systems with different competing trading strategies.
All traders involved in the experiment were boundedly rational, in the sense that they were capable of following strategies that perform well according to wealth accumulated in the past.
The author showed how simple technical trading rules exist and survive evolutionary competition in an entirely heterogeneous environment, where prices and beliefs co-evolve over time.
The evolutionary model successfully replicated and described the formulation of various stylised facts.
Chen and Yeh (2002) investigated the emergent properties of artificial stock markets in the light of the EMH and the Rational Expectations Hypothesis (REH).
The authors used Genetic Programming and inquired whether the macro-behaviour determined by the two hypotheses was consistent with the behaviour at the micro-level.
A conjecture based on a sunspot-like signal indicated that macro-behaviour can be very different from micro-behaviour and the aggregate results cannot be regarded as a simple scaling-up of individual behaviour.
Kaizoji, Bornholdt, and Fujiwara (2002) implemented a spin model to investigate stock market dynamics in the context of a stock market with fundamentalists and interacting heterogeneous traders.
The authors demonstrated that the magnetisation in the spin model can be associated with trading volume in the stock market, and, most importantly, that the market price is determined by magnetisation under natural assumptions.
AMH was proposed by Lo (2004) and can be regarded as a new version of the EMH, based on revolutionary principles.
Lo argued that market prices reflect information determined by a mixture of environmental factors and the number of distinct groups of market participants.
The AMH postulates that market efficiency is a very dynamic and context-dependent process where market participants adapt to a changing environment and the processes of learning and competition, as well as evolutionary selection pressures.
The agents are not perfectly rational, but rather they are boundedly rational satisfiers that operate in ecological systems competing for scarce resources.
The ecological systems exhibit cycles in which competition for resources depletes trading opportunities, but completely new opportunities can appear later.
Lo (2005) extended his work further and highlighted that traders act in their own-self-interest, but also make mistakes.
However, they tend to learn from their mistakes and adapt to changing market conditions.
Competition drives adaptation and innovation and evolution determinates market dynamics.
Lim (2007) analysed eleven emerging and two developed markets through the portmanteau bicorrelation test and concluded that market efficiency evolves over time in a way consistent with the AMH.
Their rolling sample framework was able to capture periods of efficiency and inefficiency by comparing the time windows that these markets generate significant nonlinear serial dependence.
Potters, Cont, and Bouchaud (2008) studied the market prices of options in liquid markets to investigate two statistical features of asset fluctuations: volatility clustering and correlations in the scale of fluctuations.
These two features were not in the pricing models for individual options but appeared in the prices fixed by the market as a whole.
Hence, the authors concluded that financial markets behave as rather efficient adaptive systems.
Neely, Weller, and Ulrich (2009) used daily exchange rate data from the Federal Reserve H.10 Statistical Release and concluded that financial markets deviate substantially from the EMH and are adaptive systems based on evolutionary selection pressures.
Kim (2009) developed a monetary model governed by adaptive learning rules based on market participants with incomplete knowledge who could learn about the economic environment.
Simulation results demonstrated that the model under adaptive learning dominates the market and explained why fundamentals predict exchange-rate returns over long horizons but not over short horizons.
Benink, Gordillo, Pardo, and Stephens (2010) used artificial financial markets to study market efficiency and learning in the context of the Neo-Austrian economic paradigm.
The authors demonstrated that markets are more efficient when informational advantages are small and the learning of traders leads to a more informationally efficient market but also a less efficient market.
However, the major limitation of this particular study compared to our experiment is the introduction of ‘copycat’ artificial traders that learn the relative values of different strategies in the market and copy the most successful one.
The performance of artificial traders in our experiment is evaluated by the Breeding Fitness Return (a trailing return of a wealth-moving average) rather than copying the trading strategies of other market participants.
Breeding is a process of creating new artificial traders to replace poor-performing ones.
Also, breeding involve the creation of completely new genomes (traders) by recombination of the parent genomes by crossover and mutation operations.
Hence, agents’ trading rules in our study are improved by a natural selection process as the survival-of-the-fittest principle is in place.
Urquhart and Hudson (2013) implemented linear and non-linear econometric techniques to investigate the US, the UK and the Japanese markets using long run historical data and concluded that the AMH provides a better description of the behaviour of stock returns than the EMH.
Their empirical results suggested that each of the three markets showed evidence of being an adaptive market, with returns going through periods of independence and dependence.
Zhou and Lee (2013) applied the automatic variance ratio test and the automatic portmanteau test to Real Estate Investment Trust (REIT) data and reported that market efficiency varies continuously over time depending on market conditions.
In a later study Zhou et al.
(2014) provided further supportive evidence of the AMH.
In a similar econometric fashion Ghazani and Aragli (2014) investigated the daily returns of the TEPIX index and concluded that the AMH gives an appropriate evolutionary perspective on market efficiency.
Verheyden et al.
(2014) used six state-of-the-art rolling efficiency tests and data of the three leading stock market indices from around the developed world and confirmed the validity of the AMH.
Hull and McGroarty (2014) examined 16 years of data across 22 emerging markets and reported findings consistent with the AMH.
Our study extends Urquhart and Hudson (2013), Zhou and Lee (2013), Zhou et al.
(2014), Ghazani and Aragli (2014), Verheyden et al.
(2014) and Hull and McGroarty (2014) econometric findings by using intelligence systems to demonstrate how financial markets learn, adapt and evolve over time.
Soufian et al.
(2013) proposed three testable hypotheses to establish the degree to which observable trading behaviour is consistent with the principles of bounded rationality.
The three hypotheses were related to the evolutionary concept of bounded rationality that according to the authors seems more plausible than the notion of efficient markets.
They found that the AMH gives a theoretical basis for a new financial paradigm which better describes the financial crises.
In our experiment we went beyond the theoretical framework by Soufian et al.
(2013) to show how 10,000 boundedly rational traders survive, compete and adapt to constantly changing market conditions.
Creation of initial trading rules Table 1 represents the main technical settings of our artificial stock market.
Every agent in our experiment has only one initial trading rule.
We use a special adaptive form of Strongly Typed Genetic Programming (STGP), where the genomes are the actual trading rules of agents.
The first generation of trading rules is created randomly.
The random nature of the initial rules is to ensure that a large variety of possible trading rules is fully investigated enabling us to observe the processes of learning, adaptation and evolution of traders.
The agents’ trading rules evolve and adapt through a breeding process, which create new artificial traders to replace the poorly performing ones.
It involves the selection of well performing traders and the production of new trading rules by a recombination of the parent genomes trough crossover and mutation operations (Witkam, 2013).
The crossover recombination technique (randomly chosen parts of two trading rules are exchanged in order to create two new trading rules) and mutation operation that randomly change a small part of the trading rule are applied to create later generations.
To avoid the creation of trading rules that cannot be properly evaluated and to reduce the creation of meaningless trading rules, the modelling software uses a form of STGP.
STGP (Montana, 1995) involves the definition of a specific set of types that fit the problem domain.
Every function and terminal is defined to return a specific type and every function argument is defined to be of a specific type.
This in turn defines which functions and terminals can be used as arguments for other functions.
Table 1.
Artificial stock market parameter settings.
Artificial stock market parameters Total population size (agents) 100,000 in Market J; 9000 in Market I; 8000 in Market H; 7000 in Market G; 6000 in Market F; 5000 in Market E; 4000 in Market D, 3000 in Market C; 2000 in Market B; 1000 in Market A Initial wealth (equal for all agents) 100,000 Significant forecasting range 0–10% Number of decimal places to round quotes on importing 2 Minimum price increment for prices generated by model 0.01 Minimum position unit 20% Maximum genome size 4096⁎ Maximum genome depth 20⁎⁎ Minimum initial genome depth 2 Maximum initial genome depth 5 Breeding cycle frequency (bars) 1 Minimum breeding age (bars) 80⁎⁎⁎ Initial selection type Random Parent selection (percentage of initial selection that will breed) 5%⁎⁎⁎⁎ Mutation probability (per offspring) 10% Total number of quotes processed-FTSE 100 7262 Total number of quotes processed-S&P 500 7262 Total number of quotes processed-Russell 3000 7262 Seed generation from clock Yes Creation of unique genomes Yes Offspring will replace the worst performing agents of the initial selection Yes ⁎ Maximum genome size measure the total number of nodes in an agent’s trading rule.
A node is a gene in the genome such as a function or a value.
⁎⁎ Maximum genome depth measures the highest number of hierarchical levels that occurs in an agent’s genome (trading rule).
The depth of a trading rule can be an indicator of its complexity.
⁎⁎⁎ This is the minimum age required for agents to qualify for potential participation in the initial selection.
The age of an agent is represented by the number of quotes that have been processed since the agent was created.
This measure also specifies the period over which agent performance will be compared.
Our minimum breeding age is set to 80, which means that the agent’s performance over the last 80 quotes will be compared.
⁎⁎⁎⁎ 5% Of the best performing agents of the initial selection that will act as parents in crossover operations for creating new agents.
The trading rules use historical price and volume data as input, and according to their internal logic, generate advice which consists of a desired position in the security as a percentage of an agent’s wealth and an order limit price for purchasing or selling the security.
The internal logic of the trading rules consists of the following operators: • Price and volume data access functions.
• Average, min, max functions on historical price and volume data.
• Various logical and comparison operators.
• Some basic Technical Indicators (Witkam, 2013).
Artificial stock market structure We study the relationship between market efficiency and adaptability within the context of the artificial stock market populated by 10,000 boundedly rational traders.
All of the traders are characterised by adaptive learning behaviour represented by the Genetic Programming algorithm (Altreva Adaptive Modeler).
The artificial traders all have different trading rules to ensure greater heterogeneity.
Hence, the traders in the model are not orientated towards predetermined formation of strategies, and therefore are free to develop and continually evolve new trading rules.
We developed ten different markets (denoted A–J) populated by different number of traders for each of the three financial instruments.
Market A is populated by 1000 traders; Market B has 2000 traders; Market C has 3000 traders; Market D has 4000 traders; Market E has 5000 traders; Market F has 6000 traders; Market G has 7000 traders; Market H has 8000 traders; Market I has 9000 traders and Market J populated by 10,000 artificial traders.
The continuous Breeding Fitness Return (a trailing return of a wealth moving average) is an important component of the market structure.
This particular type of return is used to measure the fitness criterion for the selection of agents to survive.
Hence, the heterogeneity of traders will improve by a natural selection process because the survival-of-the-fittest principle is in place.
Hommes (2011) argue that general heterogeneous expectations hypothesis could describe individual expectations and the aggregate behaviour their interactions co-creates in different financial market settings.
Traders generate wealth by investing in two financial instruments that are available on the artificial market – the risky financial instrument represented by the FTSE100, S&P 500, and Russell 3000 indices and the risk free asset represented by cash.
In each period, an artificial trader’s wealth is defined by: (1) where Wi,t is the wealth accumulated by trader i up to and including period t, Mi,t and hi,t represents the money stock and the amount of risky asset (index) held by an artificial trader i in period t, respectively; and Pt is the real-world market price of the index observed by each trader in period t. 3.3.
Artificial stock market clearing mechanism and order generation process Our experimental stock market is a simulated double auction market where all buy and sell orders from artificial traders are collected.
The traders receive real-life historical quotes from the FTSE 100, S&P 500 and Russell 3000 and then evaluate their trading rule and place their order (if any), and then the artificial stock market calculates the clearing price.
The clearing price on the other hand is the price at which the highest trading volume from limit orders can be matched.
If the same highest trading volume can be matched at multiple price levels, then the clearing price will be the average of the lowest and the highest of those prices.
The market orders have no influence on the clearing price.
After the clearing price has been successfully estimated, all of the executable orders are executed according to the established clearing price.
Hence, the buyers and sellers automatically receive “price improvement”.
There is no market maker.
The number of shares bought by agents is always equal to the number of shares sold by agents.
In other words, prices in the artificial market are determined by the traders’ orders.
In case when the total number of assets offered (at or below clearing price) exceeds the total number of assets asked (at or above clearing price) or vice versa, the remaining orders will not be (fully) executed.
In this case, the orders at the clearing price will be selected for execution with priority for market orders over the limit orders and then on a first-in-first-out (FIFO) basis.
There is also a possibility of partial order execution.
If there are no matching limit orders at all, no market orders will be executed either.
In that case, the published Virtual Market price will be the Virtual Market price of the previous quote (Witkam, 2013).
The order generation process comprises the following components.
The STGP technique transforms the output of an agent’s trading rule into a buy or sell order through comparison of the desired position and the agent’s current position.
Then the number of assets that need to be purchased or sold has been estimated.
In case that assets need to be purchased or sold, an order is generated to buy or sell the required amount of assets determined by the specified limit price or market order indication (Witkam, 2013).
For greater clarity we would like to illustrate the above theory with a particular example.
If a trader, for example, holds 1000 assets of FTSE 100, priced at $38.50 and $80,000 in cash his wealth will be $118,500 and its position is 32.5%.
If the trading rule generates an advice of a position of 50% and a limit price of $3850.
Then a limit order will be produced to purchase 539 (=50% * 118, 500/3850–1000) additional FTSE 100 assets priced at $3850.
Is a financial market populated by more heterogeneous adaptive traders efficient?
Throughout the years, the EMH was mainly formalised based on the concept of probabilistic independence in probability theory.
Malkiel (1987) quantified the notion of efficient markets by considering the rate of return Rt, a random function defined in the L2 probabilistic Hilbert space, as well as Ωt−1, the σ-algebra produced by the history of rate of return .
Hence, the EMH states that Rt is independent of any random variables in Ωt−1.
Moreover, when taking into account the conditional expectation , the EMH then implies the following: (2) Because Eq (2) is a result of the random walk in a discrete-time stochastic process, the EMH is associated with the random walk process.
Recent research based on nonlinear tests indicates the existence of nonlinear dependence in stock market data.
For instance, the studies of Brock, Dechert, and Scheinkman (1987), Frank, Gencay, and Stengos (1988), Savit (1988), Savit (1989), Hsieh (1989), Scheinkman and LeBaron (1989), Peters (1991) and Willey (1992) demonstrate the presence of nonlinear dependence between Rt and Ωt−1, or that may represent a chaotic time series which seems to be characterised with random behaviour, but it is in fact deterministic.
Moreover, Farmer and Lo (1999) argue that to make the EMH operational, researchers have to specify trader cognitive abilities, information structure, risk preferences, etc.
But then the EMH will consist of a test of several auxiliary hypotheses and any potential rejection of such a joint hypothesis could provide misleading information which part of the joint hypothesis is inconsistent with the dataset.
Is there any other hypothesis that better represent market efficiency taking into account market complexity and trader behaviour?
To analyse the implications of market size on market efficiency and adaptability, we consider experiments associated with ten different market sizes (number of traders).
Lo (2004) argue that measuring the level of market efficiency of a particular market should be seen as relative to other markets.
In view of this claim, we investigated the outcome of various tests for nonlinearity and compare market efficiency between the ten different markets.
The STGP modelling approach represents an appropriate tool for examining the stock market mechanism in isolation from the traders who populate the artificial stock market.
An important addition is that we are able to investigate the relationship between the market dynamics and trading activities and therefore to analyse the efficiency of the stock market in terms of the EMH and the AMH.
The environment where stock prices and heterogeneous traders’ beliefs co-evolve, adapt and try to survive over time provides an appropriate laboratory platform to investigate market efficiency and the emergent behaviour of stylised facts of financial return.
Also the heterogeneity of expectations among artificial traders provides important nonlinear conditions for the market.
First, we investigate whether our artificial stock markets for the three financial instruments are efficient in the sense that the stock returns are statistically independent.
In order to test for statistical independence, we adopted the procedure of Chen et al.
(2000) which consists of the Rissanen’s predictive stochastic complexity (PSC) filtering (Rissanen, 1989), followed by the celebrated BDS testing proposed by Brock et al.
(1996).
We begin our econometric analysis by applying the Augmented Dickey-Fuller (ADF) test to detect the presence of a unit root.
Our ADF testing procedure includes running a regression of the first difference of the log price series against the series that have been lagged once and then combined with a drift and time trend.
The process can be quantified by: (3) The null hypothesis of the test postulates that pt(ln (pt)) contains a unit root (β1 = 0).
The alternative hypothesis of no presence of unit root is rejected when β1 ≠ 0.
The null hypothesis of the presence of a unit root was rejected in all three index price series in all different market levels (Tables 2–4).
Hence, the return series generated by all artificial traders in all markets are stationary at the 95% significance level.
This finding is consistent with Lee et al.
(2010) who reported stationary price series in 32 developed and 26 developing countries.
The formation of stationarity could be a result of the serious microstructure biases associated with low-priced stocks (Conrad & Kaul, 1993; Ball, Kothari, & Shanken 1995), the role of leverage (Chan, 1988; Ball and Kothari, 1989) and the importance of stock market size and associated risk factors (Zarowin, 1990; Richards, 1997).
Table 2.
Econometric statistics for FTSE 100 price series generated by a different number of traders in various artificial stock markets.
FTSE 100 Market Standard deviation SK KU JB ADF⁎ PSC BDS GARCH Kaplan A 1656.6 −0.1717 1.6546 575.35 −37.99 (1, 0) 0.13 (0, 1) R B 1656.3 −0.1717 1.6545 575.30 −37.12 (1, 0) 0.43 (0, 1) R C 1637.1 −0.1706 1.6221 570.05 −37.89 (0, 1) 0.29 (0, 1) R D 1600.8 −0.1701 1.5651 569.23 −37.16 (1, 0) 0.71 (1, 2) R E 1489.4 −0.1687 1.4769 568.11 −38.23 (1, 0) 1.48 (1, 0) R F 1204.7 −0.1611 1.4476 567.58 −35.68 (0, 1) 1.72 (0, 1) R G 1201.2 −0.1565 1.4290 566.47 −35.54 (0, 0) 0.38⁎⁎ ⁎⁎⁎ R H 1011.3 −0.1522 1.3618 565.78 −35.46 (0, 0) 0.30⁎⁎ ⁎⁎⁎ R I 800.87 −0.1489 1.2767 560.03 −35.17 (0, 0) 0.27⁎⁎ ⁎⁎⁎ A J 781.89 −0.1119 1.2001 559.32 −34.94 (0, 0) 0.20⁎⁎ ⁎⁎⁎ A SK-skewness; KU-kurtosis; JB-the Jarque–Bera test; the numbers in brackets (p, q) in the column PSC are the orders of the ARMA (p, q) selected by the PSC criterion.
⁎ The MacKinnon (1996) one-sided critical value for rejection of the Null hypothesis of a unit root at 5% level is −3.410060.
⁎⁎ Failed to reject the Null hypothesis that series are identically and independently distributed (IID).
⁎⁎⁎ No presence of ARCH effect.
Table 3.
Econometric statistics for S&P 500 price series generated by a different number of traders in various artificial stock markets.
S&P 500 Market Standard deviation SK KU JB ADF⁎ PSC BDS GARCH Kaplan A 436.61 −0.0732 1.4571 715.57 −64.42 (0, 1) 0.42 (0, 1) R B 436.57 −0.0730 1.4569 715.50 −64.88 (0, 1) 0.85 (0, 1) R C 436.20 −0.0727 1.4560 714.23 −64.35 (1, 0) 1.21 (1, 0) R D 411.01 −0.0699 1.4555 714.11 −64.10 (1, 0) 1.38 (1, 2) R E 390.77 −0.0690 1.4490 713.78 −63.37 (1, 0) 0.90 (1, 2) R F 387.32 −0.0687 1.4474 713.32 −63.19 (1, 0) 0.36 (0, 1) R G 380.93 −0.0680 1.4470 713.27 −63.67 (0, 0) 1.37 ⁎⁎⁎ R H 373.56 −0.0674 1.3329 713.01 −62.46 (0, 0) 0.35⁎⁎ (0, 1) R I 370.31 −0.0645 1.3012 712.23 −62.33 (0, 0) 0.31⁎⁎ ⁎⁎⁎ A J 368.89 −0.0631 1.2991 712.01 −62.01 (0, 0) 0.24⁎⁎ ⁎⁎⁎ A SK-skewness; KU-kurtosis; JB-the Jarque–Bera test; the numbers in brackets (p, q) in the column PSC are the orders of the ARMA (p, q) selected by the PSC criterion.
⁎ The MacKinnon (1996) one-sided critical value for rejection of the Null hypothesis of a unit root at 5% level is −3.410060.
⁎⁎ Failed to reject the Null hypothesis that series are identically and independently distributed (IID).
⁎⁎⁎ No presence of ARCH effect.
Table 4.
Econometric statistics for Russell 3000 price series generated by a different number of traders in various artificial stock markets.
Russell 3000 Market Standard deviation SK KU JB ADF⁎ PSC BDS GARCH Kaplan A 458.59 −0.0453 1.4742 720.21 −90.77 (1, 0) 0.20 (0, 1) R B 458.58 −0.0452 1.4740 720.20 −90.73 (1, 0) 0.30 (1, 0) R C 458.00 −0.0443 1.4738 719.98 −90.12 (1, 0) 1.38 (1, 0) R D 446.21 −0.0411 1.4732 719.34 −90.56 (0, 1) 1.36 (0, 1) R E 433.78 −0.0410 1.4727 719.21 −89.63 (1, 0) 0.34 (1, 1) R F 430.25 −0.0390 1.4001 719.10 −89.39 (1, 0) 0.99 (0, 1) R G 428.11 −0.0378 1.3998 718.88 −89.12 (0, 0) 0.12 (1, 1) R H 420.20 −0.0372 1.3991 718.24 −88.85 (0, 0) 0.32⁎⁎ ⁎⁎⁎ R I 415.11 −0.0321 1.3983 718.11 −88.50 (0, 0) 1.43⁎⁎ ⁎⁎⁎ A J 404.80 −0.0299 1.3930 718.01 −88.11 (0, 0) 0.56⁎⁎ ⁎⁎⁎ A SK-skewness; KU-kurtosis; JB-the Jarque–Bera test; the numbers in brackets (p, q) in the column PSC are the orders of the ARMA (p, q) selected by the PSC criterion.
⁎ The MacKinnon (1996) one-sided critical value for rejection of the Null hypothesis of a unit root at 5% level is −3.410060.
⁎⁎ Failed to reject the Null hypothesis that series are identically and independently distributed (IID).
⁎⁎⁎ No presence of ARCH effect.
We then applied the Rissanen’s PSC criterion to the return series generated by the STGP mechanism to identify an linear ARMA model by selecting the model with minimum PSC.
If some of the three financial instruments at any market level satisfies the EMH, both p and q of ARMA should equal zero.
Hence, there will not be any linear dependence and the return series are not linearly predictable.
The seventh column of Tables 2–4 shows the ARMA process extracted from the return series.
All FTSE 100, S&P 500 and Russell 3000 return series in Markets A, B, C, D, E and F are linearly dependent and therefore inefficient.
The return series generated by artificial traders in Markets G, H, I and J are linearly independent (p = 0, q = 0).
Lack of linearity in these four markets suggest an important initial finding that artificial stock markets populated by 7000, 8000, 9000 and 10,000 artificial traders are so efficient that there are no linear signals found.
We estimated the most appropriate ARMA (p, q) model and fitted it to the data set in order to discard all linearity from the sample.
Any signal left in the residual series must be non-linear.
We implemented the BDS test to investigate for nonlinearity.
This particular test is based on the assumption that if the return series are identically and independently distributed (IID), then the BDS test statistic has a limiting standard normal distribution.
The null hypothesis incorporates the notion of an IID return series.
The BDS test detects significant deviations in the correlation of integral behaviour from that anticipated under the IID of the dataset.
The correlation integral is quantified by: (4) where an ‘m-history’ computed from the underlying uni-variate dataset and Iɛ( ) an indicator function: and zero otherwise.
The correlation integral establishes the frequency and connectivity with which different points are within radius ɛ of each other.
Here m represents the embedding dimension within which lag τ has been implemented in the computing of ‘m-history’ to prevent the formation of a very high correlation between the elements of an m-tuple (Chen, Lux, & Marchesi 2000).
If the returns generated by the artificial traders in our experiments are identically and independently distributed, then the correlation function (Eq (4)) suggests that Cɛ,m = (Ce,1)m for sure for all ɛ > 0 and m = 2, 3, … .
The test statistic, with limiting standard normal distribution under the null hypothesis was proposed by Brock, Dechert, LeBaron, and Scheiknman (1996): (5) Brock et al.
(1996) offered an estimation process for the standard deviation σɛ,m.
We applied the BDS test directly to the data generated by the artificial traders in Markets G, H, I and J because there are no linear signals detected in these series.
Importantly, BDS test parameters are the distance parameter ɛ and the embedding dimension (DIM).
Our results proved not to be sensitive to the choice of ɛ and DIM parameters.
Hence, we only report results with ɛ = 0.7 and DIM = 6.
The empirical results of the test are reported in the eighth column of Tables 2–4.
The null hypothesis of IID is significantly rejected in Markets A, B, C, D, E and F indicating nonlinear dependence in the return series.
In Markets G, H, I and J the null hypothesis has not been rejected, suggesting that series generated by the artificial agents are identically and independently distributed.
In terms of the BDS test, Markets G, H, I and J are nonlinearly dependent, more random and therefore more efficient than markets populated with fewer traders.
This finding can be considered to support the classical version of the EMH.
According to the econometric literature, however, a large part of the nonlinearity in data is in their second moment.
We performed the Lagrange multiplier (LM) test with up to 14 lags to detect the presence of an ARCH effect of the residual.
In case the null hypothesis is rejected, we further identified the GARCH order of the series according to the Schwartz Information Criterion.
The ninth column of Tables 2–4 represents the results.
All markets with small numbers of artificial traders reported the presence of an ARCH effect.
We proceeded to further identify the GARCH order based on the Swartz Information Criterion.
This process can be expressed in quantitative terms by: (6) where ɛt is the IID normal innovations and the restrictions are α0 > 0, αi, and αi + βi < 1, (Chen et al., 2000).
While Market H of the S&P 500 and Market G of the Russell 3000 showed ARCH effects, the most populated markets-I and J- in all experiments of the three indices did not show any ARCH effect.
This result is consistent with the BDS test.
Barnett, Gallant, Hinich, and Jungeilges (1998) highlighted that the BDS and the Kaplan tests are the best performing ones in nonlinear terms.
Moreover, there is a possibility that the process in the data might be chaotic, rather than stochastic.
To investigate this assumption we adopted the Kaplan test (Kaplan, 1994).
Kaplan argues that in deterministic processes, unlike stochastic processes, nearby points are also nearby under their image in the phase space environment.
In technical terms, if Xi and Yj are relatively close to each other, then Xi+1 and Yj+1 are also close to each other.
When Xi = (ri, ri−τ, ri−2τ, … , ri−(m−1)τ) is embedded in m dimensional phase space, we observe the presence of a recursive function given by: (7) where τ is the fixed positive integer time decay.
Hence, we can calculate: (8) For all time subscripts (i, j) for a specified choice of embedding dimension m. We assume that E(ζ) = ∑Aζ ∈ i,j/#{Aζ}, where Aζ ≡ {(i, j):δi,j < ζ}.
In a case of a completely deterministic system with continuous f we achieved and therefore K (the actual value of Kaplan) is the limitation of E(ζ) as ζ → 0.
One of the most important moments in this particular test is to estimate a piecewise regression line for (δi,j, εi,j) and apply the intercept to calculate the value of K. We based the actual statistical procedure for K on simulated series which have the same histogram and similar autocorrelation functions as the original series.
Acceptance of the null of IID is when K is smaller than the test statistic.
The opposite is valid for the rejection of the null hypothesis.
In terms of the Kaplan test (the last column of Tables 2–4), only Markets I and J of the three financial instruments demonstrate consistency with the test.
The two markets populated by the highest number of artificial traders do not reject the null hypothesis of IID based on the BDS, ARCH and Kaplan tests.
de Lima (1998) investigated nonlinearity and nonstationarity in the S&P daily returns from January 2, 1980 to December 31, 1990.
The author was unable to reject the null hypothesis of IID series in all subsamples prior to the 1987 crash.
Interestingly, when he expanded the sample and included the crash, the outcome was a strong rejection.
Chen et al.
(2000) reported similar experimental findings.
Our results are consistent with Chen et al.
(2000) based on rejection and non-rejection of the null for the entire markets of the three indices, rather than based on subsample tests as performed by de Lima (1998).
The EMH postulates that prices should always be consistent with their fundamental values because asset prices reflect all available information.
This is sometimes referred to as allocative efficiency which means that stock prices reflect the true fundamental value of the underlying asset.
Artificial stock markets provide an appropriate environment for testing the EMH because the researcher knows the fundamental values with great certainty (Smith, 2011).
We implemented the Gordon model explained by LeRoy (2004), where market prices equal their fundamentals: (9) where pt is the asset price at time t and ft is the fundamental value of the asset at time t. As illustrated in Figs.
1, 3 and 5 the FTSE 100, the S&P 500 and the Russell 3000 price series generated by only 1000 traders (all markets denoted by A) exhibit market inefficiency.
The red curve which represents the price series generated by the artificial traders, deviates substantially from the fundamental values of the three indices represented by the yellow curve.
Figs.
2, 4 and 6 illustrate the price series for the three financial instruments generated by 10,000 artificial traders (all markets denoted by J).
Time series plot of FTSE 100 generated by 1000 traders Fig 1.
Time series plot of FTSE 100 generated by 1000 traders.
Note: the yellow curve consists of historical FTSE 100 quotes, the red curve represent price series generated by 1000 traders.
(For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)
Time series plot of FTSE 100 generated by 10,000 traders Fig 2.
Time series plot of FTSE 100 generated by 10,000 traders.
Time series plot of S&P 500 generated by 1000 traders Fig 3.
Time series plot of S&P 500 generated by 1000 traders.
Time series plot of S&P 500 generated by 10,000 traders Fig 4.
Time series plot of S&P 500 generated by 10,000 traders.
Time series plot of Russell 3000 generated by 1000 traders Fig 5.
Time series plot of Russell 3000 generated by 1000 traders.
Time series plot of Russell 3000 generated by 10,000 traders Fig 6.
Time series plot of Russell 3000 generated by 10,000 traders.
It is clearly evident that the price series and their fundamental values significantly overlap, suggesting market efficiency.
This claim is confirmed by the empirical tests we performed.
Table 5 describes the absolute deviations from real prices as a proportion of the real FTSE 100, S&P 500 and Russell 3000 in Market A and Market J.
The mean, maximum and standard deviation reported by the markets with reduced numbers of artificial traders are significantly higher than the equivalent statistics for the prices series generated by traders in the most populated markets.
Table 5.
Descriptive statistics of FTSE 100, S&P 500 and Russell 3000 price series-absolute deviations from real prices as a proportion of real prices for markets populated by 1000 and 10,000 traders.
Market N Minimum Maximum Mean Standard deviation FTSE 100 J-10,000 traders 7262 0.0000 0.8456 0.3283⁎ 0.0272 A-1000 traders 7262 0.0000 0.9041 0.9918⁎ 0.1018 Mean paired difference between Market J and Market A −0.6635 S&P 500 J-10,000 traders 7262 0.0000 1.0141 0.0236⁎ 0.2387 A-1000 traders 7262 0.0000 1.0293 0.0890⁎ 0.9310 Mean paired difference between Market J and Market A −0.0654 Russell 3000 7262 0.0000 0.7223 0.1736⁎ 0.0113 7262 0.0000 0.8327 0.7632⁎ 0.1487 Mean paired difference between Market J and Market A −0.5896 ⁎ Significantly different from 0 at the 1% level.
The significance of the differences in the mean values has been estimated by t-tests on the paired differences between the deviations from intrinsic (fundamental) values for the two markets.
All differences recorded in Table 5 are significant at the 1% level.
It is important to note, however, that the market prices in inefficient markets periodically and temporarily deviate from their fundamental values.
As the figures illustrate, market prices moves back to their intrinsic levels in the long-run.
Hence, inefficiency does not cause long-run mispricing of assets, leading to persistent arbitrage opportunities.
Our empirical results are consistent with the findings of Decamps and Lovo (2002) who pointed out that asset prices converge to their fundamental values in the long-run: (10) where V is the fundamental value of the asset at time t, E is the expected value of the asset at time t, and represents the history of traders actions up until time t. Blanchard and Watson (1983) proposed that the mispricing of assets relative to their fundamental values will persist with each period with probability π, and the mispricing will disappear with probability (1 − π).
In case of risk averse traders, the price acceleration is higher but the probability of a reversion to fundamental values is also higher.
The Hurst exponent proposed by Hurst (1951) provides a measure for long-term memory and fractality of a time series.
Hurst (1951) demonstrated that range series scaled by power-law as time increases: (11) (R/S)t represents the rescaled range series at time t, c is a constant and H is called the Hurst exponent.
H = 0.5 indicates a random series, 0 < H < 0.5 indicates an anti-persistent series and 0.5 < H < 1 signals a persistent series.
A persistent series is trend reinforcing (the direction of the next value is more likely the same as the current value).
Fig 7 illustrates the Hurst exponent for FTSE 100 generated by 10,000 artificial traders.
The Hurst exponent ranges from 0.390 to 0.620.
Fig 8 shows that the FTSE 100 Hurst exponent peaks around the value H = 0.5 which indicates a random series and therefore efficient markets.
Fig 9 illustrates that the Hurst exponent ranges between 0.375 and 0.625 for S&P 500 return series generated by 10,000 traders.
Fig 10 clearly indicate that the Hurst exponent has peaked at the value of H = 0.5 suggesting randomness and efficient markets.
Fig 11 shows the Hurst exponent for the Russell 3000 ranging between 0.400 and 0.610 and Fig 12 shows a peaked value of 0.5.
Hence, the return series generated by 10,000 artificial traders for the Russell 3000 are random and the market is efficient.
Hurst exponent for FTSE 100 price series generated by 10,000 traders Fig 7.
Hurst exponent for FTSE 100 price series generated by 10,000 traders.
Histogram of Hurst exponent for FTSE 100 price series generated by 10,000… Fig 8.
Histogram of Hurst exponent for FTSE 100 price series generated by 10,000 traders.
Hurst exponent for S&P 500 price series generated by 10,000 traders Fig 9.
Hurst exponent for S&P 500 price series generated by 10,000 traders.
Histogram of Hurst exponent for S&P 500 price series generated by 10,000 traders Fig 10.
Histogram of Hurst exponent for S&P 500 price series generated by 10,000 traders.
Hurst exponent for Russell 3000 price series generated by 10,000 traders Fig 11.
Hurst exponent for Russell 3000 price series generated by 10,000 traders.
Histogram of Hurst exponent for Russell 3000 price series generated by 10,000… Fig 12.
Histogram of Hurst exponent for Russell 3000 price series generated by 10,000 traders.
Furthermore, we investigated whether the efficient markets populated by the largest number of traders possess long-memory for the FTSE 100, S&P 500 or Russell 3000.
A random process is characterised by long memory when the autocorrelation function decays asymptotically as a power-law of the form τ−α with α < 1 (Lillo & Farmer, 2004).
In other words, values from the past could have significant implications on the present, implying anomalous diffusion under stochastic conditions which emphasises the presence of long-memory.
When the value of the exponent α is smaller than 1, the process might have long-memory (the smaller the value of α, the longer the memory).
In order to define the long-memory process we follow Lillo and Farmer (2004): (12) where 0 < α < 1 and L(x) is a slowly varying function at infinity if (Embrechts, Kluppelberg, & Mikosch, 1997).
In our experiment, we are considering only positively correlated long-memory processes, with a Hurst exponent in the interval (0.5, 1).
In terms of the Hurst exponent (H), the long-memory process is characterised by: (13) On the other hand, short-memory processes can be quantified by: (14) With an autocorrelation function that decays faster than k−1.
The relationship between the diffusion properties of the integrated process explains the rationale behind the use of the Hurst exponent.
In cases of normal diffusion the increments do not possess long-memory and the standard deviation increases at the rate of t1/2.
In cases of long-memory increments, the standard deviation increases at the rate of tHL(t), with 1/2 < H < 1 and a slow-varying L(t) function.
As the peak value of the Hurst exponent is 0.5 in all experiments with markets denoted by J for the three financial instruments, we estimate the corresponding (α) exponent from Eq (13).
In all three markets populated by 10,000 artificial traders, we calculated that α = 1, suggesting a lack of long-memory processes.
Consistent with our previous empirical results we can conclude that all markets denoted by J for the FTSE 100, S&P 500 and Russell 3000 are characterised by strong market efficiency i.e., the process possesses short-memory with H = 0.5 and the autocorrelation function decay faster than k−1 .
Overall our experimental results show that artificial stock markets populated by a smaller number of traders behave differently from markets with greater genetic diversity.
The price series generated by artificial agents in Markets I and J conform to the EMH.
This is clear evidence that enhanced genetic diversity has a beneficial effect on the market.
The presence of more artificial traders in Markets I and J corresponds to an enhanced heterogeneity, and most importantly, greater market efficiency.
Also, with the presence of a greater number of traders, the market is more competitive and more information is reflected in the order flow.
It seems that markets populated by a greater number of traders react to price changes in a timely manner, making the entire market more efficient by enhancing the process of adjustment of prices to their fundamental values.
Moreover, the BDS, ARCH and Kaplan tests revealed that richer dynamic structures, such as stock markets populated by a greater number of traders, helps to describe the findings of complex nonlinear dependence in stock market data.
Despite the EMH view, our empirical results provide evidence that patterns observed in financial markets seems to indicate that markets are characterised by internal dynamics of their own.
Financial markets dynamics seems influenced by the heterogeneity as well as the microstructure of the market.
We think that the key to analyse and understand the rich market dynamics is a mechanism which allows populations of traders to learn and adapt over time.
Our empirical results better explain market efficiency in terms of the AMH.
We observe that market efficiency is not a static process, but it is a dynamic and context-dependent process where market participants adapt to their changing environment and where structured interaction influences the evolution of competition dynamics.
Enhanced genetic diversity provides an appropriate environment where different numbers of artificial traders involved in the evolutionary process adapt to a changing environment.
Markets composed of more traders seem to adapt better to the changing environmental conditions leading to increased level of market efficiency.
Hence market efficiency can emerge from diverse individual behaviour (Bao, Hommes, Sonnenmans, & Tuinstra, 2012), where efficiency naturally occur from a market that evolves as a complex adaptive system (Lo, 2004; Mauboussin, 2002).
We observe that different levels of heterogeneity can create a complex and co-evolving expectations ecology where markets can ‘display phenomena that are regarded as anomalies in standard theory but observed in real markets’ (Arthur, 1996).
Moreover, the evolutionary nature of the artificial traders is based on the survival of the fittest principle, that is, the need to better cope with changing circumstances, market dynamics and opportunities.
In other words, natural selection operates to select the fittest within an evolutionary framework in which markets and traders interact and evolve dynamically according to the law of economic selection.
Under these circumstances, traders compete, learn and evolve.
Hence, market efficiency involves reasons and beliefs which have adaptive value because they are changeable in response to changing market circumstances.
Our findings are consistent with Blume and Easley (1992) who claim that the ‘market selection hypothesis’ based on the natural selection and survival of economic actors better represent the relationship between market efficiency and market adaptability.
The price of FTSE 100, S&P 500 and Russell 3000 in Markets I and J reflect as much information as required by the mixture of environmental factors and the number of distinct groups of artificial traders ranging from 1,000 in Markets A to 10,000 in Markets J.
Hence, the AMH seems to model market efficiency better due to its less theoretically restrictive nature than the EMH.
The AMH does not require market participants to uniformly follow the rationality axioms of neo-classical economics indicating that market efficiency can be achieved through bounded rationality.
Todd and Gigerenzer (2003) pointed out three major interpretations of bounded rationality – optimisation under constraints, cognitive illusions, and ecological rationality.
The concept adopted in our study is ecological rationality – making good decisions by exploiting the structure of the environment.
Ecological rationality can explain when and why artificial traders equipped with simple decision making mechanism perform well.
This is due to the fact that the structure of the mechanism is adapted to the structure of the information in the stock market environment.
In our case heuristics allow artificial traders to be ecologically rational, making adaptive decisions that combine accuracy with speed and frugality.
The concept of ecological rationality in our experiment examines the structure of the environments, the structure of the heuristics, and the match between them.
Our artificial traders use relatively simple heuristics for decision making purposes in an ecologically rational manner, using very limited information (historical quotes of the three financial instruments) and tailoring their information to different market structures.
The notion of ecological rationality highlights the non – existence of a necessary relationship between market efficiency and the rational ‘economic man’ postulated by the traditional economic theory.
Overall, the experimental results presented above can be summarised as having two implications for financial market efficiency and adaptability, namely, the population size effect and the learning effect.
On the one hand, the population size effect in isolation suggests that the market is more efficient when the population size increases.
On the other hand, the learning effect indicates that the market is more efficient when traders’ adaptive behaviour become more independent.
We can conclude that enhanced market size, and more heterogeneous learning styles are leading to improvements in the diversity of traders’ expectations resulting in more efficient and adaptable financial market structures.
Market efficiency and policy implications The vast majority of the debate over market efficiency has a policy undercurrent.
The EMH is associated with the ideas developed by the members of the free market school of thought (largely associated with the University of Chicago and the University of Rochester).
Scientists from the free market school of thought proposed that markets should be allowed to operate freely (laissez-faire).
In contrast, inefficient market concepts, often associated to the East Coast school of thought, tend to encourage governmental intervention.
However, cognitive barriers make it hard to entirely support either school of thought.
Our stock markets developed within laboratory settings provide an opportunity to study directly the potential rationale for governmental intervention.
Krusell, Kuruscu, and Smith (2001) examined the need of governmental intervention in markets in settings where an imperfectly rational government reduces welfare relative to a competitive equilibrium among irrational private individuals.
The authors strongly supported the need of laissez-faire.
Our results indicate support of the laissez-faire doctrine in Markets G, H and I and J.
The results from markets A, B, C, D, E and F populated by a reduced number of boundedly rational traders do not support the laissez-faire view but instead indicate the potential need for governmental intervention.
We have found this by applying the Rissanen’s criterion to the return series to identify linear ARMA models.
Markets A, B, C, D, E and F does not satisfy the EMH because both p and q of ARMA does not equal zero (Tables 2–4).
Hence we conclude that the return series of the three financial instruments in markets populated by fewer numbers of artificial traders are linearly dependent and therefore inefficient.
Davidson and Froyen (1982) investigated the relationship between stock returns and money growth rates, using monthly data, and found support for the notion of efficient markets.
However, their estimates of the relationship between stock returns and monetary policy based on Federal Reserve funds rate indicated violation of the EMH, and the need for government intervention.
In line with the propositions of Daniel, Hirshleifer, and Teoh (2002), we think that incentives such as investor education, disclosure and reporting rules towards consistent financial reporting standards might be helpful.
The real challenge for regulators is to ensure that all investors possess the same information.
Thus tighter accounting standards, timely publication of company information such as news and data, disclosure of fees and better description of complex financial products are needed.
For instance, during the recent financial crisis some central banks were slow to cut interest rates leading to a longer and deeper crisis.
The main reason for delaying interest rates cut was the significant increase in commodity prices.
Policymakers were reliant on the notion of EMH and used longer-dated futures prices for these commodities in their inflation estimations resulting in overestimated inflationary pressures.
Mispricing of the type observed in our experiments on three indices in the markets with reduced numbers of traders could potentially lead to substantial misallocation of resources in the economy.
There might be a need for governmental policies designed to correct alleged market mispricing ex post.
Another set of possible governmental rules could include limits on how assets are marketed and stricter rules against market manipulation through rumour-spreading.
Appropriate governmental measures can help investors make better informed decisions and improve the efficiency of market prices.
The controversy surrounding the EMH has stimulated several new directions of research.
Many of the well established financial models rely on two building blocks: market efficiency and homogeneous traders.
In this paper, we propose the idea that financial markets should be viewed from a Darwinian ‘survival of the fittest’ perspective and, specially, within an evolutionary framework in which markets, financial instruments, and traders interact, compete, learn, adapt, and evolve dynamically.
Our experiment provided such a dynamic environment.
This study investigates the relation between market diversity and market efficiency.
Our empirical findings demonstrate that greater market heterogeneity and adaptability play key roles in market efficiency.
An increase in market size significantly and positively contributed to the market efficiency by means of enhancing market diversity.
Moreover, individual trader learning, adaptation, and evolution reinforced the motion of efficient markets.
Our research contributions are twofold.
On the one hand, the presence of different market sizes suggests that the market is more efficient when the population size increases.
This is evident in that the BDS null hypothesis of identically and independently distributed price series was not rejected in Markets I and J which are populated by the highest number of traders.
Furthermore, only Markets I and J of the FSE 100, S&P 500 and Russell 3000 demonstrate consistency with the Kaplan test based on acceptance of the hull hypothesis where K of these two most populated markets is smaller than the test statistic.
In terms of the Hurst exponent, all three markets composed of 10,000 traders are characterised by α = 1 and H = 0.5 indicating a short-memory process with autocorrelation function which decay faster than k−1.
On the other hand, the learning effect indicates that the market is more efficient when traders’ adaptive behaviour becomes more independent.
This is evident from the greater heterogeneous learning style of Markets I and J populated by 9000 and 10,000 traders.
We demonstrate how learning at the individual level can interact with a market environment that continuously evolves as a complex adaptive system.
Our study also emphasise on the importance of complex systems in the study of market efficiency.
Hence, larger market size and a more heterogeneous learning style are leading to an improvement in the diversity of traders’ expectations, resulting in more efficient and adaptable financial market structures.
We observe that different levels of heterogeneity can create a complex and co-evolving expectations ecology where markets can ‘display phenomena that are regarded as anomalies in standard theory but observed in real markets’ (Arthur, 1996).
We generated rich varieties of market dynamics which provided a promising direction to contribute to the current studies on micro-structure and anomalies.
Our empirical findings suggest that stock markets composed of a reduced number of traders-represented by Markets A, B, C, D, E, and F, behave differently compared with markets with a greater genetic diversity-represented by Markets G, H, I, and J.
We have found that the presence of more artificial traders in the last two markets (I and J) is associated with an enhanced variety of trading rules, leading to greater market efficiency in terms of the EMH.
However, we have also found that the EMH in isolation cannot explain the internal market dynamics, market micro-structure, and the heterogeneity of market participants.
Our experimental results suggest that market efficiency is a dynamic and context-dependent process, where traders adapt to constantly changing market conditions.
The various empirical tests we performed suggest that markets populated by a greater number of traders help to discover the findings of complex nonlinear dependence in stock market data and explain the emergent nature of the EMH and the AMH.
We think that the AMH does not require market participants to follow the specific rationality axioms of neo-classical economics and, therefore, market efficiency exists simultaneously with the need for adaptive flexibility.
The evolutionary nature of the STGP technique enabled us to empirically test the idea that different trader populations behave as an efficient adaptive system.
Our empirical findings revealed that the presence of increased heterogeneity in markets provides ideal conditions for artificial agents to adapt to the changing environmental conditions, leading to higher market efficiency.
We found support for the free market school of thought in three of the stock markets for each of the three indices, and strong support for the laissez-faire doctrine in the most efficient market.
However, in the other six markets for each of FTSE 100, S&P 500 and Russell 3000 indices-all composed of reduced numbers of traders we found the need for governmental intervention.
Overall, we think that scientists and governmental agencies should study how regulatory and legal policies can limit or even eliminate the damage caused by imperfect rationality and lack of heterogeneity.
A number of interesting directions for future research can therefore be suggested.
In another computational experiment we would like to analyse the cumulative effect of socioeconomic evolution of available information and the role of this information in artificial trader’s performance.
The validity of this particular experiment can be empirically tested with real humans, by observing less-informed and more-informed traders in artificial markets designed to simulate evolution and change in the fundamental state.
It would be interesting to investigate the effects of adaptive learning such as reinforcement learning and adaptive expectations on the operative intelligence between market participants.
Another area of future research we would like to explore is the effect of the time horizon which traders look back at while they make investment decisions.
According to LeBaron (2004), traders use different amounts of past information to evaluate trading strategies, and therefore they possess various memory lengths when evaluating forecasting rules.
Additionally, markets composed of agents with different intelligence levels offer the opportunity to analyse market efficiency in depth, rather than examining whether intelligence improves market properties.
In technical terms this can be achieved by creating agents with different parse tree depth and investigate the implications of their cognitive abilities on market efficiency and adaptability (parse tree depth is explained on page 49 of Appendix B).
We also would like to include social learning in our future experiments and analyse the adaptive switch between social and individual learning.
Harrald (2000) highlighted the difference between phenotype and genotype in biology and doubted whether the adaptation can be governed by the genotype via the phenotype in social processes.
Individual and social learning can be represented by chromosomes or trees in the process of Genetic Programming and the whole population of chromosomes and trees can be treated as a society of market participants.
Genetic operators such as crossover and mutation (described in Appendix A) can be implemented within a framework of individual and social learning, so that the population dynamics generated by these genetic operators can be used to investigate the level of market efficiency and adaptability.
Appendix A Genetic Programming Genetic Programming consists of two parts: the first one is the initialization part which generate an initial population of programs in a random fashion, and the other one is the dynamics part which works in a similar way to that of Genetic Algorithms (GA).
GA and GP both apply the operation of Darwinian selection, crossover and mutation.
However, there is a major difference between GA and GP which makes GP a generalisation of GA.
While GA population is composed of fixed-length binary strings, the population of GP is composed of programs.
Each program in GP is written in LISP S-Expression and can be seen as a parse tree.
By defining the terminal set and the functional set, every single program can be written in LISP S-Expression.
For instance, consider the following logistic map: The terminal set is represented by (R, xt), where R is a constant.
The functional set is represented by (∗4 (∗xt (−1 xt))).
Hence the parse tree for this particular S-expression can be represented as follows: The crossover operation for GP begins with the random selection of two parental parse trees.
Two offsprings are produced by exchanging parts of the two selected parents.
The process of this exchange starts with random and independent selection of one point in each parental parse tree using the following uniform distribution.
By the syntax of LISP, each point of the parse tree could be either a terminal (leaf) or a function (root).
Hence, the selected point could either be a terminal or a function.
The actual probability that the crossover point is a terminal or a function is the same, i.e., one-half.
Given that a terminal or a function is to be the selected point for crossover, the probability that any terminal or function is chosen as the crossover point is uniformly distributed.
For instance, if the terminal is to be the crossover point, and then there are three terminals located in the parse tree, the probability that any one of the three terminals is chosen for the crossover operation is one-third.
Mutation also enables researchers to create new individuals.
Mutation starts with the selection of a parse tree from the whole population.
Then each point can randomly change value within the population.
Each point has a very small probability of being altered by mutation, which is independent of other points (Chen & Yeh, 1997).
Appendix B Strongly Typed Genetic Programming Strongly Typed Genetic Programming (STGP) is an enhanced version of Genetic Programming (GP) which enforces data type constraints and whose use of generic functions and data types makes it a lot more powerful than GP.
GP itself is a machine-learning method to automatic development of computer programs by means of evolution (Banzhaf et al., 1998).
If there are inputs X and outputs Y a program p is sought which satisfy Y = p(X).
In most GP models, the programs are represented as tree genomes.
For instance, Fig 1 illustrate a tree which describe a mathematical expression that uses the input variables x = (a, b, c) where x ∈ X.
The leaf nodes of the tree in Fig 1 are known as terminals whereas the non-leaf nodes are called non-terminals.
While terminals could be inputs to the program with no argument, the non-terminals are functions with at least one argument.
The fitness of a candidate solution or trading rule is based on its ability to satisfy Y = p(X).
Assume that Yexp is the expected known output and YP the actual output generated by a program p with Yp = p(X).
The fitness f(p) of p is estimated by applying the following equation: (15) In most occasions the nodes of the GP tree are not typed.
Montana (2002) suggested that a lot of GP procedures can be formulated more effectively and efficiently by using a typing mechanism for GP nodes.
In this way each node can have a particular return type and the process is known as Strongly Typed Genetic Programming.
In order to construct a parse tree researchers need to bear in mind important additional criteria beyond those required for GP.
For instance, the root node of the tree returns a value of the type required by the problem and each non-root node returns a value of the type required by the parent node as an argument (Montana, 2002).
While GP can be developed in any programming language, the STGP is normally written in a new programming language, which is a combination of Ada (Barnes, 1982) and Lisp (Steele, 1984) programming languages.
The important element taken from Ada programming language is the concept of generics as a method of developing strongly typed data.
Lisp provided the concept of having programs represented by their actual parse trees (Montana, 1995).
In conventional GP, the user must specify all the programs and variables that can be used as nodes in a parse tree and deal with search space of the order of 1030–1040 (Montana, 1994).
STGP effectively reduces the state-space size which must be searched for large and problem domains or trading rules.
The STGP search space is represented by the set all legal parse trees, which means that all functions have the correct number of parameters of the correct type.
The STGP parse tree is usually limited to some maximum depth (in our experiments this is set to 20).
The maximum depth limit of 20 keeps the search space finite and manageable and prevent trees from growing to an extremely large size.
In the STGP, the key concept are generic functions, which is a mechanism for specifying a class of functions, and assigning generic data types for these functions (Haynes, Wainwright, Sen, & Schoenefeld, 1995).
All variables, constraints, arguments, and returned values in STGP can be of any type.
The only requirement is that the type of data for each element to be specified in advance.
As a result the initialization process and the different genetic operators are enabled to construct syntactically correct trees, which is beneficial to the whole process because the search space is reduced (Haynes et al., 1996).
The STGP process produces syntactically correct programs or trading rules by the crossover and mutation operators.
During the process of crossover, the return value type of the two selected for exchange subtrees need to get tested in order to verify whether they are from the same type and if the resulting trees violate depth restrictions.
If either check fails, then the two new subtrees are selected.
In case when after performing a finite number of selections, no valid crossover points are determined, then the two parent trees are copied into the pool for the next generation (Koza, 1992).
In terms of developing forecasting trading rules the crossover process can be described in the following way.
Randomly chosen parts of two trading rules are exchanged in order to create two new trading rules (Fig 2).
In the figure above, trading strategies Si and Sj are the two parents.
The breaking point has been chosen randomly and then one-point crossover has been used to create children (new trading rules) Sk and SI.
The very first generation of trading rules is created randomly.
The random nature of the initial rules is to ensure that a large variety of possible trading rules is fully investigated.
The best performing trading rules from the initial selection are selected based on the Breeding Fitness Return to be parents in the crossover process.
The Breeding Fitness Return process represents a trailing return of a wealth moving average.
This is the return over the last n quotes of an exponential moving average of trader’s wealth, where n could have the maximum breeding value of 250.
Every pair of parents creates two offspring trading rules, so the number of parents and the number of offspring is equal.
The newly created trading rules replaces poorly performing ones in the initial selection based on the replacement fitness return.
The replacement fitness return represents the average return of a wealth moving average per quote since the initial trading rule creation.
In quantitative terms, this is the cumulative return of an exponential moving average of trader’s wealth, divided by trader’s breeding value.