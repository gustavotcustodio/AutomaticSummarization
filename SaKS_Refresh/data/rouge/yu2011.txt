Traditional ABC analysis should be replaced with multi-criteria classification approaches in order to manage inventory more efficiently. AI-based techniques are more accurate than traditional multiple discriminant analysis. SVM enable more accurate classification than other AI-based classification techniques.

0.079867 - ABC analysis is a popular and effective method used to classify inventory items into specific categories that can be managed and controlled separately.
0.078070 - Conventional ABC analysis classifies inventory items three categories: A, B, or C based on annual dollar usage of an inventory item.
0.137123 - Multi-criteria inventory classification has been proposed by a number of researchers in order to take other important criteria into consideration.
0.269032 - These researchers have compared artificial-intelligence (AI)-based classification techniques with traditional multiple discriminant analysis (MDA).
0.101761 - Examples of these AI-based techniques include support vector machines (SVMs), backpropagation networks (BPNs), and the k-nearest neighbor (k-NN) algorithm.
0.121766 - To test the effectiveness of these techniques, classification results based on four benchmark techniques are compared.
0.151378 - The results show that AI-based techniques demonstrate superior accuracy to MDA.
0.381044 - Statistical analysis reveals that SVM enables more accurate classification than other AI-based techniques.
0.176759 - This finding suggests the possibility of implementing AI-based techniques for multi-criteria ABC analysis in enterprise resource planning (ERP) systems.
0.039216 - Effective inventory management has played an important role in the success of supply chain management.
0.021277 - For organizations that maintain thousands of inventory items, it is unrealistic to provide equal consideration to each item.
0.087518 - Managers are required to classify these items in order to appropriately control each inventory class according to its importance rating.
0.141855 - ABC analysis is one of the most commonly employed inventory classification techniques.
0.050633 - Conventional ABC classification was developed for use by General Electric during the 1950s.
0.059058 - The classification scheme is based on the Pareto principle, or the 80/20 rule, that employs the following rule of thumb: “vital few and trivial many.” The process of ABC analysis classifies inventory items into A, B, or C categories based on so-called annual dollar usage.
0.000000 - Annual dollar usage is calculated by multiplying the dollar value per unit by the annual usage rate (Cohen & Ernst, 1988; Partovi & Anandarajan, 2002).
0.047059 - Inventory items are then arranged according to the descending order of their annual dollar usage.
0.028369 - Class A items are relatively small in number, but account for the greatest amount of annual dollar usage.
0.025890 - In contrast, class C items are relatively large in number, but make up a rather small amount of annual dollar usage.
0.018265 - Items between classes A and C are categorized as class B.
0.058653 - Although ABC analysis is famed for its ease of use, it has been criticized for its exclusive focus on dollar usage.
0.056338 - Other criteria such as lead-time, commonality, obsolescence, durability, inventory cost, and order size requirements have also been recognized as critical for inventory classification (Flores & Whybark, 1987; Jamshidi & Jain, 2008; Ng, 2007; Ramanathan, 2006).
0.137123 - In order to accommodate multi-criteria inventory classification, many researchers have proposed methods that consider factors other than annual dollar usage.
0.065728 - Flores and Whybark (1987) developed a cross-tabulation matrix method for use in bi-criteria inventory classification; they found that the method becomes increasingly complicated when three or more criteria are involved in evaluations.
0.057743 - Cohen and Ernst (1988) implemented a statistical clustering technique to classify inventory items with multiple attributes; however, a substantial amount of inventory data is required to execute this technique.
0.047619 - Sophisticated statistical procedures such as factor analysis are also necessary.
0.029412 - Every time a new inventory item is stored in a warehouse, the clustering process must be repeated, and there is a possibility that previously classified items may end up with different classes.
0.086339 - Partovi and Burton (1993) applied the analytic hierarchy process (AHP) to inventory classification in order to include both quantitative and qualitative evaluation criteria.
0.010499 - AHP has been praised for its ease of use and its inclusion of group opinions; however, the subjectivity resulting from the pair-wise comparison process of AHP poses problems.
0.107800 - Bhattacharya, Sarkar, and Mukherjee (2007) developed a distance-based multiple-criteria consensus framework utilizing the technique for order preference by similarity to ideal solution (TOPSIS) for ABC analysis.
0.000000 - TOPSIS (Hwang & Yoon, 1981) evaluates the distance of each alternative from both the most ideal and least ideal solutions.
0.033333 - Alternatives that are closest to the most ideal situation, while being furthest from the least ideal situation, are considered optimal.
0.034364 - To offset the impact of subjectivity, Ramanathan (2006) and Ng (2007) proposed methods similar to data envelopment analysis (DEA).
0.031373 - These methods maximize the artificial inventory score that is used to classify each inventory item.
0.036630 - Unlike AHP, the weights given to classified criteria are solved automatically when the DEA model is optimized.
0.054983 - Like the statistical clustering technique, this model must be reprogrammed and solved whenever a new inventory item is introduced.
0.211891 - AI-based techniques for inventory classification are gaining popularity.
0.067363 - Guvenir and Erel (1998) proposed the genetic algorithm for multi-criteria inventory classification (GAMIC) to calculate the weight of criteria, along with the AB and BC cut-off points of classified inventory items.
0.045455 - Similar to the AHP, criteria hierarchy is utilized to compute weighted scores of the inventory items.
0.041199 - The items with scores greater than the AB cut-off point are classified as A; similarly those between AB and BC are classified as B, and those below BC as C. A chromosome encodes the weight vector, along with the two cut-off points for classification.
0.032520 - Standard genetic operators such as reproduction, crossover, and mutation are applied to the chromosomes.
0.015152 - GAMIC improves the quality of criteria weights previously obtained through pair-wise comparisons between two criteria.
0.051780 - Artificial neural networks have been widely applied for classification purposes, as well as for forecasting problems in a variety of applications.
0.017544 - They are useful for finding nonlinear surfaces and separating the underlying patterns.
0.000000 - Paliwal and Kumar (2009) delivered a comprehensive survey of neural network articles, categorizing the application of networks into categories: accounting and finance, health and medicine, engineering and manufacturing, and marketing.
0.022599 - Accounting and finance is the category with the greatest number of applications, especially with regard to bankruptcy prediction, credit evaluation, fraud detection, and property evaluation finance.
0.067278 - Partovi and Anandarajan (2002) utilized backpropagation (BP) and genetic algorithm (GA)-based learning methods to develop an artificial neural network for inventory classification.
0.113424 - Real-world inventory data from a large pharmaceutical company were used to compare the accuracy of the proposed neural network methods with that of multiple discriminant analysis (MDA), a statistical classification technique.
0.061856 - Multiple attributes including unit price, ordering cost, demand range, and lead-time were used to classify the inventory items.
0.103546 - The results showed that neural network–based classification models have a higher predictive accuracy than the conventional MDA technique.
0.114191 - Between the two neural network–based techniques, the GA demonstrated slightly better classification accuracy than BP.
0.015686 - The support vector machine (SVM) is a powerful novel learning algorithm introduced by Vapnik (1995).
0.057143 - A SVM is based on the structural risk minimization principle.
0.033755 - SVM utilizes a hypothesis space of linear functions in a high dimension space.
0.027491 - In the high dimension space, an optimal separating hyperplane is constructed to give the maximum separation between decision classes.
0.052632 - SVMs have recently proved popular machine learning tools for classification and regression.
0.050847 - Application of SVMs has enabled significant progress in a variety of fields, including image detection, text categorization, bioinformatics, fault diagnosis, and financial analysis (Hu & Zhang, 2008).
0.020050 - k-Nearest neighbors (k-NN) is another popular method for classification and pattern recognition; it was first introduced by Fix and Hodges (1951), and later adapted by Cover and Hart (1967).
0.018868 - In this method, a newly introduced item is classified into the class with the most members present among the k-nearest neighbors.
0.056911 - Applications of k-NN can be found in various pattern recognition and classification problems.
0.000000 - The rest of this paper is organized as follows.
0.145536 - Section 2 reviews the concepts of several AI-based techniques.
0.106615 - Benchmark classification techniques that were found in the literature are discussed and demonstrated in Section 3.
0.190022 - In Section 4, the AI-based inventory classification techniques used in this research are described.
0.079027 - An illustration is provided in Section 5 that compares the accuracy of various classification techniques.
0.166206 - The paper concludes in Section 6 with a discussion of the application of the AI-based techniques to multiple-criteria inventory classification problems.
0.073333 - Inventory classification problems deal with the assignment of inventory items to a group so that they can be appropriately managed.
0.117706 - Artificial-intelligence (AI)-based techniques take advantage of symbolic logic and advanced computer technology when developing various learning algorithms for classification.
0.160447 - In this paper, three AI-based classification techniques will be utilized for inventory classification: BP networks (BPNs), SVMs, and the k-NN algorithm.
0.082192 - The accuracy of each technique will be compared with the others.
0.076312 - Backpropagation networks BPNs are the most widely used classification technique for training an artificial neural network.
0.032362 - A BPN utilizes supervised learning methods and feed-forward architecture to perform complex functions such as pattern recognition, classification, and prediction.
0.000000 - A typical BPN (Fig 1) is composed of three layers of neurons: the input layer, the hidden layer, and the output layer.
0.000000 - The input layer is considered the model stimuli, while the output layer is the associated outcome of the stimuli.
0.000000 - The hidden layer establishes the relationship between the input and output layers by constructing interconnecting weights.
0.000000 - Back propagation network architecture Fig 1.
0.000000 - Back propagation network architecture.
0.027491 - Input layer neurons are linear, while neurons in the hidden and output layers have sigmoidal signal functions (Kumar, 2005).
0.019048 - The input signals are modified by the interconnected weights Wih.
0.016260 - A sigmoidal signal function is used to activate the sum of the modified signals.
0.000000 - It also converts the output of the hidden layer into the input signals of the output layer.
0.015686 - Similarly, the input signals of the output layer are modified by the interconnected weights Whj.
0.000000 - The sum of modified signals is again activated by a sigmoid signal function and the output is collected.
0.038567 - The weights of input-hidden and hidden-output layers are modified by a specific learning function, such as gradient descent based algorithms, as shown in Fig 1.
0.030303 - Support vector machines SVMs were originally developed by Vapnik and co-workers (1995) at Bell Laboratories.
0.043956 - A SVM employs structural risk minimization rather than the empirical risk minimization used by conventional neural networks.
0.024465 - SVMs use a linear model to implement nonlinear class boundaries via the nonlinear mapping of input vectors into a high-dimensional feature space.
0.019417 - In this high-dimensional space, the maximum margin hyperplane is found so that the separation between decision classes can be maximized.
0.032520 - Support vectors are defined as the training examples closest to the maximum margin hyperplane.
0.007491 - Given a training set of instance-label pairs (xi, yi), i = 1, 2, …, n, where the input is labeled xi ∈ Rn and the output is labeled yi ∈ {−1, +1}, the SVM classifier satisfies the following conditions: where w denotes the weight vector and b the bias.
0.015686 - φ(·) is a nonlinear function that maps the input space to a high-dimensional feature space.
0.025890 - wTφ(x) + b = 0 is represented as the linear separating hyperplane that separates two hyperplanes with the margin width equal to .
0.070175 - For classification problems that are linearly non-separable, incorrect classification is unavoidable.
0.024096 - To allow for incorrect classification, a slack variable ξi is introduced to the prime optimization model and is defined as: where C is a penalty parameter, which is a regularized constant that determines the trade-off between training error and model flatness.
0.068898 - In order to solve this quadratic optimization problem, the Lagrangian method is used.
0.034629 - Lagrangian multipliers αi (i.e., support vectors) are introduced to construct the Lagrangian function used to find the saddle point: By applying Karush Kuhn–Tucker (KKT) conditions for the optimum constrained function, Lp can be transformed to the dual Lagrangian LD(α): The dual form of the primal optimization model can be transformed as The inner products in the objective function of the dual Lagrangian are replaced by the kernel function in order to map the instance data into a high-dimensional feature space: The selection of kernels is important in order to obtain robust classification results.
0.016260 - The most popular kernel functions are linear, polynomial, radial basis functions (RBFs), and sigmoid.
0.019048 - Let α∗ be the optimal solution for the dual optimization problem.
0.049587 - The decision function for classification can be defined as: 2.3. k-Nearest neighbors k-NN is a non-parametric technique for classifying observations (Cover & Hart, 1967).
0.054902 - Computations of the measure of distance or similarity between observations are conducted prior to classification.
0.014652 - A newly introduced item is then classified to the group where the majority of k-NNs belong.
0.018391 - Use of k-NN requires an appropriate value of k. Loftgaarden and Queensbrerry (1965) proposed that a reasonable k could be obtained by calculating the square root of the number of observations in a group.
0.075362 - Hand (1981) suggests that a trial and error approach might be more effective in identifying the value of k that incurs the lowest classification error.
0.058824 - Malhotra, Sharma, and Nair (1999) conducted a sensitivity analysis to compare the classification accuracy among various values of k to conclude that a value of 3 gives that highest correct classification rate.
0.141702 - In order to examine the effectiveness of the selected classification techniques, the classification results of several benchmark techniques are compared.
0.062147 - The benchmark techniques and their associated classification criteria have been proposed in studies by Reid (1987), Flores, Olson, and Dorai (1992), Ramanathan (2006), and Ng (2007).
0.043956 - As discussed previously, Reid (1987) used annual dollar usage as the only criterion to classify inventory items.
0.047059 - Forty-seven stock keeping units (SKU) were ordered by rank according to annual dollar usage.
0.013333 - The first 10 SKU items accounted for 73.5% of the total usage value, and were designated as Class A.
0.032520 - The next 13 SKUs accounted for 18.2%, and were assigned to Class B.
0.028369 - The remaining 24 items were grouped as Class C, with a share of 8.3% of total expenditure.
0.000000 - Flores et al.
0.041237 - (1992) applied AHP based on four criteria, namely, average unit cost, annual dollar usage, critical factor, and lead-time.
0.030303 - The weights for the criteria are 0.079, 0.091, 0.42, and 0.41, respectively.
0.026667 - Ramanathan (2006) implemented a DEA-like weighted linear optimization model to compute an optimal inventory score for each inventory item.
0.021231 - The mathematical model is shown below: where ymj is the performance of mth inventory item in terms of criteria j, while vmj is the decision variable that determines the appropriate weight of the mth inventory item for criterion j.
0.020619 - The optimal score for each of the 47 inventory items can be obtained iteratively by changing the objective function.
0.066667 - Inventory items are classified based on their relative inventory scores.
0.052632 - Ng (2007) proposed a classification scheme similar to that of Ramanathan (2006).
0.041379 - Prior to the construction of the linear model, transformation of the performance measures is carried out to scale all measurements into a 0–1 range; the classification criteria are then ranked according to their importance.
0.040900 - When applying the same notations, the weighted linear optimization model modified by Ng can be shown as: The first constraint of the model is a normalization constraint, while the second constraint ensures that the criteria are ranked in a descending order.
0.000000 - The use of normalized weights limits the weight score within a scale of 0–1 while maintaining the sum of all weights as 1.
0.145786 - In this study, the classification results from the four benchmark techniques were used to test the effectiveness of the AI-based techniques.
0.187380 - In our research, the AI-based classification techniques BPN, SVM, and k-NN were implemented to classify inventory items.
0.139497 - In order to study the effectiveness of these classification techniques, the classification results were compared with the results obtaining using traditional MDA.
0.049751 - Four classification criteria, initially utilized by Flores et al.
0.015152 - (1992), were selected as inputs: average unit cost, annual dollar usage, critical factor, and lead-time.
0.068182 - The classification results from the four benchmark techniques were grouped into A, B, or C classes.
0.106867 - The input criteria and the output classification results of the four benchmark techniques are shown in Table 1.
0.000000 - Table 1.
0.126078 - Classification under multi-criteria by various benchmark techniques.
0.007499 - SKUs Criteria Benchmark results Average unit cost Annual dollar usage Critical factor Lead-time Traditional ABC AHP Optimal score Scaled score 1 49.92 5840.64 1 2 A A A A 2 210.00 5670.00 1 5 A A A A 3 23.76 5037.12 1 4 A C A A 4 27.73 4769.56 0.01 1 A C B A 5 57.98 3478.80 0.5 3 A B B A 6 31.24 2936.67 0.5 3 C C A A 7 28.20 2820.00 0.5 3 A C C B 8 55.00 2640.00 0.01 4 A C B B 9 73.44 2423.52 1 6 A A B A 10 160.50 2407.50 0.5 4 A B C A 11 5.12 1075.20 1 2 C B C C 12 20.87 1043.50 0.5 5 C B A B 13 86.50 1038.00 1 7 B A A A 14 110.40 883.20 0.5 5 B B B B 15 71.20 854.40 1 3 B A C C 16 45.00 810.00 0.5 3 C C C C 17 14.66 703.68 0.5 4 B B C C 18 49.50 594.00 0.5 6 B A B C 19 47.50 570.00 0.5 5 C B A B 20 58.45 467.60 0.5 4 B B C C 21 24.40 463.60 1 4 B A C C 22 65.00 455.00 0.5 4 B B C C 23 86.50 432.50 1 4 C A B B 24 33.20 398.40 5 3 C A C C 25 37.05 370.50 0.01 1 C C C C 26 33.84 338.40 0.01 3 C C C C 27 84.03 336.12 0.01 1 C C C C 28 78.40 313.60 0.01 6 B C B B 29 134.34 268.68 0.01 7 A B A A 30 56.00 224.00 0.01 1 C C B C 31 72.00 216.00 0.5 5 C B B B 32 53.02 212.08 1 2 C B C C 33 49.48 197.92 0.01 5 C C A B 34 7.07 190.89 0.01 7 C C A B 35 60.60 181.80 0.01 3 C C C C 36 40.82 163.28 1 3 C B B C 37 30.00 150.00 0.01 5 B C C C 38 67.40 134.80 0.5 3 C C C C 39 59.60 119.20 0.01 5 C C B B 40 51.68 103.36 0.01 6 B C C B 41 19.80 79.20 0.01 2 C C B C 42 37.70 75.40 0.01 2 C C C C 43 29.89 59.78 0.01 5 B C C C 44 48.30 48.30 0.01 3 C C C C 45 34.40 34.40 0.01 7 B B B B 46 28.80 28.80 0.01 3 C C C C 47 8.46 25.38 0.01 5 B C C C The BPNs for this study were developed using the Neural Network Toolbox of MATLAB.
0.000000 - Each network consisted of four input neurons and one output neuron.
0.067511 - These neurons represent four criteria for inventory classification and the three inventory classes.
0.014184 - A hidden layer composed of eight hidden neurons was also utilized to connect the input and output layers.
0.016260 - As regards the SVM, the radial basis function (RBF) served as the kernel function.
0.012945 - The upper bounds of C and the kernel parameter γ can be found by a grid search within a predetermined grid space.
0.017544 - The SVM for this study was implemented using LIBSVM (Chang & Lin, 2004).
0.014184 - The selection of the optimal value of the neighborhood parameter k is critical when classifying with k-NN.
0.013746 - In this study, the neighborhood parameter k was assigned a value of 3, as suggested by Malhotra et al.
0.000000 - (1999).
0.000000 - The k-NN classifier was then implemented using MATLAB.
0.032520 - The dataset was analyzed with BPN, SVM, and k-NN using threefold cross validation.
0.020000 - The 47 inventory items were first divided into three subsets of nearly equal size (i.e., 16, 16, and 15).
0.000000 - One subset was sequentially tested using the classifier trained by the remaining two subsets.
0.079699 - The average accuracy for all threefolds was computed for each classification technique.
0.000000 - This data was used for subsequent comparison.
0.209811 - The classification results of the three AI-based classification techniques were compared with the MDA results in order to study their effectiveness.
0.068854 - The forecasting accuracy of the classification results was compared for four benchmark techniques, namely, traditional ABC (Reid, 1987), AHP (Flores et al., 1992), optimal score (Ramanathan, 2006), and scaled score (Ng, 2007).
0.071184 - The accuracy was measured by the percentage of observations correctly classified for a classification technique.
0.084543 - Table 2 shows the accuracy of the four classification techniques used to predict the classification results of the four benchmark techniques.
0.098572 - In order to evaluate the differences between these techniques, analysis of variance (ANOVA) was conducted, and the results are reported in Table 3.
0.083010 - The prediction performance of the four classification techniques was significantly different.
0.016260 - Duncan’s multiple range test (MRT), a post-hoc testing of ANOVA, was performed.
0.153274 - SVM outperformed the other classification techniques in every benchmark classification.
0.016878 - BPN ranked second in every prediction performance except the prediction of scaled score.
0.054795 - The lowest ranked were k-NN and MDA, in that order.
0.215040 - The results show that AI-based classification techniques perform better than traditional MDA except in one instance.
0.000000 - Table 2.
0.162418 - Classification accuracy of AI-based techniques vs. MDA: the results of benchmark techniques.
0.041732 - Classification techniques Benchmark techniques Traditional ABC (%) AHP (%) Optimal score (%) Scaled score (%) Average (%) SVM 89.58 (1) 76.67 (1) 72.36 (1) 78.89 (1) 79.38 BPN 85.28 (2) 51.25 (2) 66.11 (2) 71.11 (4) 68.44 k-NN 80.97 (3) 48.89 (3) 61.94 (3) 74.31 (2) 66.53 MDA 76.39 (4) 48.75 (4) 51.85 (4) 71.57 (3) 62.14 Average 83.06 56.39 63.07 73.97 The number in parenthesis signifies the ranking order among the four techniques used to predict the results of a specified benchmark technique.
0.000000 - Table 3.
0.094828 - Results of ANOVA for the four classification techniques.
0.000000 - Source DF Mean square F value p value Significance Between groups 3 0.160 8.01 0.0000 ∗ Within groups 140 0.020 Total 143 ⁎ Significant at 5% level.
0.155398 - Although AI-based techniques demonstrated superiority in inventory classification, they performed differently when predicting different benchmark results.
0.204066 - For traditional ABC analysis, the classification accuracies for all AI-based techniques were above 80%, since only a single criterion was applied for classification.
0.117706 - The performance of the AI-based techniques was not as good when forecasting the classification results for AHP and optimal score.
0.107143 - This could be attributed to the benchmark techniques’ use of multiple criteria for multi-class classifications, especially when only 47 inventory records are available.
0.051282 - Nevertheless, SVM was the most accurate among all four benchmark predictions due to its high generalization capability, as well as its use of kernel functions to increase the learning efficiency.
0.440845 - Traditional ABC analysis should be replaced with multi-criteria classification approaches in order to manage inventory more efficiently.
0.129850 - Multi-class classification utilizing multiple criteria requires techniques capable of providing accurate classification and processing a large number of inventory items.
0.180507 - AI-based classification techniques such as BPN and GA have proven to be efficient methods for classifying inventory items.
0.083333 - In this study, SVM and k-NN were utilized to compare their classification performance with MDA.
0.000000 - A data set formerly used by Reid (1987) that contains 47 disposable SKUs.
0.065111 - The same data set had been used by a number of other researchers (Flores et al., 1992; Ng, 2007; Ramanathan, 2006) to test various classification techniques, so these classification results were utilized as the benchmark for this study.
0.062745 - A threefold cross validation approach was applied to evaluate the effectiveness of the four techniques.
0.295489 - The results showed that AI-based techniques are more accurate than MDA.
0.087719 - In particular, SVM outperformed the other techniques on all four benchmark results.
0.079954 - Traditional ABC analysis has been implemented in many enterprise resource planning (ERP) systems for effective inventory management; however, traditional ABC analysis is hindered by the fact that it uses a single criterion and ignores other important factors such as production or purchase lead-time, costing information, criticality, etc.
0.210018 - The findings of our study suggest that ERP systems can be used to implement AI-based classification techniques with multiple criteria in ABC analysis.
0.065041 - The use of these techniques will improve the effectiveness and efficiency of inventory management.

[Frase 3] Multi-criteria inventory classification has been proposed by a number of researchers in order to take other important criteria into consideration.
[Frase 4] These researchers have compared artificial-intelligence (AI)-based classification techniques with traditional multiple discriminant analysis (MDA).
[Frase 180] The findings of our study suggest that ERP systems can be used to implement AI-based classification techniques with multiple criteria in ABC analysis.
[Frase 177] The results showed that AI-based techniques are more accurate than MDA.
