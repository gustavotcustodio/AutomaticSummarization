Prediction of high performance concrete using geometric semantic operators. Results that outperform state of the art machine learning techniques. Good generalization ability on unseen instances.

0.020833 - Concrete is a composite construction material made primarily with aggregate, cement, and water.
0.062752 - In addition to the basic ingredients used in conventional concrete, high-performance concrete incorporates supplementary cementitious materials, such as fly ash and blast furnace slag, and chemical admixture, such as superplasticizer.
0.071679 - Hence, high-performance concrete is a highly complex material and modeling its behavior represents a difficult task.
0.151402 - In this paper, we propose an intelligent system based on Genetic Programming for the prediction of high-performance concrete strength.
0.076055 - The system we propose is called Geometric Semantic Genetic Programming, and it is based on recently defined geometric semantic genetic operators for Genetic Programming.
0.135119 - Experimental results show the suitability of the proposed system for the prediction of concrete strength.
0.079399 - In particular, the new method provides significantly better results than the ones produced by standard Genetic Programming and other machine learning methods, both on training and on out-of-sample data.
0.020833 - Concrete is a composite construction material made primarily with aggregate, cement, and water.
0.068729 - There are many formulations of concrete, which provide varied properties, and concrete is the most-used man-made product in the world (Lomborg, 2001).
0.028986 - Modern concrete mix designs can be complex.
0.065813 - The choice of a concrete mix depends on the need of the project both in terms of strength and appearance and in relation to local legislation and building codes.
0.110575 - The design begins by determining the requirements of the concrete.
0.043956 - These requirements take into consideration the weather conditions that the concrete will be exposed to in service, and the required design strength.
0.049874 - Many factors need to be taken into account, from the cost of the various additives and aggregates, to the trade offs between, the “slump” for easy mixing and placement and ultimate performance.
0.019048 - A mix is then designed using cement, coarse and fine aggregates, water and chemical admixtures.
0.067511 - The method of mixing will also be specified, as well as conditions that it may be used in.
0.101274 - This allows a user of the concrete to be confident that the structure will perform properly.
0.098845 - As reported in Yeh (1998), high-performance concrete (HPC) is a new terminology used in the concrete construction industry.
0.029412 - In addition to the basic ingredients in conventional concrete the making of HPC needs to incorporate supplementary cementitious materials, such as fly ash and blast furnace slag, and chemical admixture, such as superplasticizer (Kumar, Singh, & Singh, 2012).
0.089223 - High-performance concrete is such a highly complex material that modeling its behavior is a difficult task.
0.046377 - The Abrams’ water-to-cement ratio (w/c) law (Abrams, 1927; Nagaraj & Banu, 1996) has been described as the most useful and significant advancement in the history of concrete technology.
0.070175 - According to Abrams’s law, the compressive strength of concrete varies inversely with the W/C ratio.
0.029304 - Hence, an increase in the w/c decreases the concrete strength, whereas a decrease in the w/c ratio increases the strength.
0.062450 - The implication, therefore, is that the strengths of various but comparable concrete are identical as long as their w/c ratios remain the same, regardless of the details of the compositions.
0.076643 - The Abrams rule implies that only the quality of the cement paste controls the strength of comparable cement.
0.031008 - The paste quantity does not matter.
0.045662 - Analysis of a variety of experimental data shows that this is not quite true (Popovics, 1990).
0.069560 - For instance, if two comparable concrete mixtures have the same w/c ratio, the strength of the concrete with the higher cement content is lower (Popovics, 1990).
0.038369 - As reported in Yeh (1998), several studies independently have shown that concrete strength development is determined not only by the w/c ratio, but that it is also influenced by the content of other ingredients (Bhanja & Sengupta, 2005).
0.037879 - Therefore, although experimental data have shown the practical acceptability of this rule within wide limits, a few deviations have been reported.
0.075949 - The current empirical equations for estimating compressive strength are based on tests of concrete without supplementary cementitious materials.
0.061404 - The validity of these relationships for concrete with supplementary cementitious materials (fly ash, blast furnace slag, etc.)
0.000000 - should be investigated (Bhanja & Sengupta, 2005).
0.050314 - The more we know about the concrete composition versus strength relationship, the better we can understand the nature of concrete and how to optimize the concrete mixture.
0.081301 - All these aspects highlight the need of reliable and accurate techniques that allow modeling the behavior of concrete materials.
0.100258 - In this paper, for the first time, we propose an intelligent system based on Genetic Programming for the prediction of the concrete strength.
0.062155 - More in detail, in this work we use recently defined geometric semantic genetic operators for Genetic Programming.
0.069182 - These operators allow to include the concept of semantics in the search process and have several advantages with respect to standard genetic operators used in Genetic Programming.
0.044422 - The paper is organized as follows: Section 2 introduces basic concepts about Genetic Programming with a particular focus on the standard operators used in the evolutionary process; Section 3 presents the geometric semantic operators used in this work and outlines some of their properties that have a direct effect on the search process; Section 4 presents a literature review on using computational intelligence methods in simulating the behavior of concrete materials; Section 5 describes the data used in this work, the experimental settings and proposes an accurate discussion and analysis of the results; Section 6 concludes the paper summarizing the results that have been achieved.
0.039216 - Models lie in the core of any technology in any industry, be it finance, manufacturing, services, mining, or information technology.
0.049645 - The task of data-driven modeling lies in using a limited number of observations of system variables for inferring relationships among these variables.
0.062147 - The design of reliable learning machines for data-driven modeling tasks is of strategic importance, as there are many systems that cannot be accurately modeled by classical mathematical or statistical techniques.
0.049606 - Reliable learning in the field of Machine Learning (ML) revolves around the notion of generalization, which is the ability of a learned model to correctly explain data that are drawn from the same distribution as training data, but have not been presented during the training process, and it is this very important property that ML algorithms aim to optimize.
0.039918 - Genetic Programming (GP) (Koza, 1992; Poli, Langdon, & Mcphee, 2008) is one of the youngest paradigms inside the computational intelligence research area called Evolutionary Computation (EC) and consists in the automated learning of computer programs by means of a process mimicking Darwinian evolution.
0.042776 - GP tackles learning problems by means of searching a computer program space for the program that better respects some given functional specifications.GP is an evolutionary computation technique that automatically solves problems without requiring the user to know or specify the form or structure of the solution in advance.
0.033898 - At the most abstract level GP is a systematic, domain-independent method for getting computers to solve problems automatically starting from a high-level statement of what needs to be done.
0.051282 - In GP a population of computer programs is evolved.
0.067511 - That is, generation by generation, GP stochastically transforms populations of programs into new, hopefully better, populations of programs.
0.086957 - The search is performed using an EA.
0.014652 - The recipe for solving a problem with GP is as follows: • Chose a representation space in which candidate solutions can be specified.
0.101959 - This consists of deciding on the primitives of the programming language that will be used to construct programs.
0.030303 - A program is built up from a terminal set (the variables in the problem) and a function set (the basic operators).
0.057471 - • Design the fitness criteria for evaluating the quality of a solution.
0.066451 - This involves the execution of a candidate solution on a suite of test cases, reminiscent of the process of black-box testing.
0.048110 - In case of supervised learning, a distance-based function is employed to quantify the divergence of a candidate’s behavior from the desired one.
0.000000 - • Design a parent selection and replacement policy.
0.047092 - Central to every EA is the concept of fitness-driven selection in order to exert an evolutionary pressure towards promising areas of the program space.
0.016878 - The replacement policy determines the way in which newly created offspring programs replace their parents in the population.
0.057143 - • Design a variation mechanism for generating offspring from a parent or a set of parents.
0.048485 - Standard GP uses two main variation operators: crossover and mutation.
0.053585 - Crossover recombines parts of the structure of two individuals, whereas mutation stochastically alters a portion of the structure of an individual.
0.035714 - • After a random initialization of a population of computer programs, an iterative application of selection-variation-replacement is employed to improve the programs quality in a stepwise refinement way.
0.073171 - In GP learning, both terms of program and model refer to the same entity and will be used interchangeably.
0.062893 - Supervised learning of regression models in GP relies on the extraction of implicit relationships that may exist in the input–output mappings specified by the training examples.
0.017544 - The discovered relationships are expressed in symbolic form, which is traditionally represented by an expression-tree structure.
0.020833 - For a complete introduction to GP the reader is referred to Koza (1992).
0.085627 - In this work we used genetic operators that, diversely from the canonical ones, are based on the concept of semantics that will be introduced in the next section.
0.040580 - To understand the differences between the genetic operators used in this work (described in Section 3) and the ones used in the standard GP algorithm, the latter are briefly described.
0.072072 - The “Standard” Crossover Operator.
0.056981 - The crossover operator is traditionally used to combine the genetic material of two parents by swapping a part of one parent with a part of the other.
0.047504 - More specifically, tree-based crossover proceeds by the following steps: • Choose two individuals as parents, based on mating selection policy (the better the fitness of the individual the higher its probability of being selected).
0.000000 - • Select a random subtree in each parent.
0.047059 - The selection of subtrees can be biased so that subtrees constituting terminals are selected with lower probability than other subtrees.
0.027211 - • Swap the selected subtrees between the two parents.
0.062016 - The resulting individuals are the children.
0.019048 - This process is shown, using two arbitrary simple Boolean expressions as parents, in Fig 1.
0.048485 - An example of standard GP (subtree swapping) crossover Fig 1.
0.054422 - An example of standard GP (subtree swapping) crossover.
0.078431 - The Mutation Operator.
0.077083 - The mutation operation introduces random changes in the structures of the individuals in the population.
0.044025 - While there are many different mutation operators, here subtree mutation (that is the most well-known one and also the one used in this work) is described.
0.020833 - Subtree mutation begins by selecting a point at random within the “parent” tree.
0.032362 - Then, it removes whatever is currently at the selected point and whatever is below the selected point and inserts a randomly generated subtree at that point.
0.053571 - This operation is controlled by a parameter that specifies the maximum size (usually measured in terms of tree depth) for the newly created subtree that is to be inserted.
0.047059 - This parameter typically has the same value as the parameter for the maximum size of trees in the initial population.
0.026667 - This process is shown in Fig 2, where an arbitrary simple Boolean expression is mutated by replacing one of its subtrees with a random tree.
0.051282 - An example of standard GP (subtree) mutation Fig 2.
0.057971 - An example of standard GP (subtree) mutation.
0.036354 - In the last few years, GP has been extensively used to address problems in different domains (Aslam, Zhu, & Nandi, 2013; Gusel & Brezocnik, 2011; Guo, Rivero, Dorado, Munteanu, & Pazos, 2011; Kayadelen, 2011; Tsakonas, Dounias, Doumpos, & Zopounidis, 2006) and it has produced a wide set of results that have been defined human-competitive (Koza, 2010).
0.062147 - While these results have demonstrated the suitability of GP in tackling real-life problems, research has recently focused on developing new variants of GP in order to further improve its performances.
0.061240 - In particular, efforts have been dedicated to an aspect that was only marginally considered up to some years ago: the definition of methods based on the semantics of the solutions (Beadle & Johnson, 2008; Beadle & Johnson, 2009; Castelli et al.
0.000000 - ; Krawiec & Lichocki, 2009; Jackson, 2010).
0.057971 - Although there is no universally accepted definition of semantics in GP, this term often refers to the behavior of a program, once it is executed on a set of data.
0.053571 - For this reason, in many references, including here, the term semantics is intended as the vector of outputs a program produces on the training data (Moraglio et al., 2012).
0.035714 - Thought semantics determines what a program actually does, the traditional GP operators, like crossover and mutation described so far ignore this knowledge and manipulate programs only considering their syntax.
0.076503 - Abstraction from semantics allows them to rely on simple, generic search operators.
0.083333 - The main consequence of this choice is that it is difficult to predict how modifications of programs will affect their semantics.
0.062155 - Very recently, new genetic operators, called geometric semantic genetic operators have been proposed in Moraglio et al.
0.000000 - (2012).
0.038462 - These operators, that manipulate programs considering directly their semantic information, have a number of theoretical advantages, compared to the ones of standard GP, the most important one being the fact that they induce a unimodal fitness landscape (Stadler & Institute, 1995) on any problem consisting in finding the match between a set of input data and a set of expected target ones.
0.040650 - According to the theory of fitness landscapes (Vanneschi, 2004) this should relevantly improve GP evolvability (Altenberg, 1994) (i.e.
0.101266 - the ability of genetic operators to produce offspring that are fitter than their parents) on all these problems.
0.088805 - In this section we report the definition of geometric semantic operators for real functions domains, since these are the operators we will use in the experimental phase.
0.033071 - Definition 1 Geometric Semantic Crossover Given two parent functions , the geometric semantic crossover returns the real function TXO = (T1 · TR) + ((1 − TR) · T2), where TR is a random real function whose output values range in the interval [0,1].
0.025641 - The interested reader is referred to Moraglio et al.
0.075425 - (2012) for a formal proof of the fact that this operator corresponds to a geometric crossover on the semantic space, in the sense that it produces an offspring that stands between its parents in this space.
0.048008 - Nevertheless, even without a formal proof, we can have an intuition of it by considering that the (unique) offspring generated by this crossover has a semantic vector that is a linear combination of the semantics of the parents with random coefficients included in [0,1] and whose sum is equal to 1.
0.077519 - The authors of Moraglio et al.
0.045708 - (2012) also prove an interesting consequence of this fact: the fitness of the offspring cannot be worse than the fitness of the worst of its parents.
0.020378 - Also in this case we do not replicate the proof here, but we limit ourselves to give a visual intuition of this property: in Fig 3 we report a simple two-dimensional semantic space in which we draw a target function T (training points are represented by “X” symbols), two parents P1 and P2 and one of their offspring O (which by construction is included between its parents).
0.028369 - In Fig 3, it is immediately clear that O is closer to T than P2 (which is the worst parent in this case).
0.080460 - The generality of this property is proven in Moraglio et al.
0.000000 - (2012).
0.079699 - The toy functions that we use to give a visual intuition of the fact that… Fig 3.
0.075816 - The toy functions that we use to give a visual intuition of the fact that geometric semantic crossover produces an offspring that is at least not worse than the worst of its parents.
0.000000 - In this simple case, offspring O (which stands between parents P1 and P2 by construction) is clearly closer to target T (training points represented by “X” symbols) than parent P2.
0.025157 - To constrain TR in producing values in [0,1] we use the sigmoid function: where Trand is a random tree with no constraints on the output values.
0.037006 - Definition 2 Geometric Semantic Mutation Given a parent function , the geometric semantic mutation with mutation step ms returns the real function TM = T + ms · (TR1 − TR2), where TR1 and TR2 are random real functions.
0.060000 - Reference (Moraglio et al., 2012) formally proves that this operator corresponds to a box mutation on the semantic space, and induces a unimodal fitness landscape.
0.049740 - However, even though without a formal proof, it is not difficult to have an intuition of it, considering that each element of the semantic vector of the offspring is a “weak” perturbation of the corresponding element in the parent’s semantics.
0.013746 - We informally define this perturbation as “weak” because it is given by a random expression centered in zero (the difference between two random trees).
0.051799 - Nevertheless, by changing parameter ms, we are able to tune the “step” of the mutation, and thus the importance of this perturbation.
0.040642 - It is important to point out that at every step of one of these operators, offspring contain the complete structure of the parents, plus one or more random trees as its subtrees and some arithmetic operators: the size of each offspring is thus clearly much larger than the one of their parents.
0.070630 - The exponential growth of the individuals in the population (demonstrated in Moraglio et al.
0.042553 - (2012)) makes these operators unusable in practice: after a few generations the population becomes unmanageable because the fitness evaluation process becomes unbearably slow.
0.038462 - The solution that is suggested in Moraglio et al.
0.070922 - (2012) consists in performing an automatic simplification step after every generation in which the programs are replaced by (hopefully smaller) semantically equivalent ones.
0.042553 - However, this additional step adds to the computational cost of GP and is only a partial solution to the progressive program size growth.
0.026667 - Last but not least, according to the particular language used to code individuals and the used primitives, automatic simplification can be a very hard task.
0.038855 - The work proposed in Vanneschi, Castelli, Manzoni, and Silva (2013) explained how the exponential growth of the individuals has been addressed, with a very simple and effective implementation of the GP algorithm.
0.054422 - This is the implementation used in this paper.
0.081103 - Prediction of concrete strength is an important problem in the engineering field and several works have been proposed to address this problem.
0.054902 - In this section a brief literature review about the use of computational intelligence techniques for facing this problem is proposed.
0.052974 - In Saridemir, Topcu, Ozcan, and Severcan (2009), artificial neural networks and fuzzy logic models for prediction of long-term effects of ground granulated blast furnace slag on compressive strength of concrete under wet curing conditions have been developed.
0.050633 - For purpose of constructing these models, 44 different mixes with 284 experimental data were gathered from the literature.
0.085048 - Results have shown that artificial neural networks and fuzzy logic systems have strong potential for prediction of long-term effects of ground granulated blast furnace slag on compressive strength of concrete.
0.130451 - In Yeh (1998), author demonstrates the possibilities of adapting artificial neural networks to predict the compressive strength of high-performance concrete.
0.061404 - A set of trial batches of HPC was produced in the laboratory and demonstrated satisfactory experimental results.
0.038107 - The study led to two main conclusions: (1) a strength model based on artificial neural networks is more accurate than a model based on regression analysis; and (2) it is convenient and easy to use artificial neural networks models for numerical experiments to review the effects of the proportions of each variable on the concrete mix.
0.092827 - The work proposed in Lai and Serra (1997) also predicted the strength of concrete by using neural networks.
0.070588 - In particular, a model is developed, based on neurocomputing, for predicting, with sufficient approximation, the compressive strength of cement conglomerates.
0.039216 - Some neural networks are constructed in which the different mix-design parameters of a variety of cement conglomerates i.e.
0.033333 - the compressive strength, are associated.
0.093750 - Satisfactory results were obtained for evaluating the mechanical properties of different concrete mixes.
0.052746 - The work proposed in Lee (2003) used single and multiple artificial neural networks architecture for predicting of concrete strength, while authors of Dias and Pooliyadda (2001) utilized back propagation neural networks to predict the strength and slump of ready mixed concrete and high strength concrete in which chemical admixtures and/or mineral additives were used.
0.030303 - Authors of Akkurt, Ozdemir, Tayfur, and Akyol (2003) utilized genetic algorithms and ANN for modeling of compressive strength of cement mortar.
0.063927 - In the study, the common three layer feed-forward type of artificial neural networks is used.
0.028986 - Other works that used artificial neural networks to model concrete strength are reported in Yeh (1998a), Sebastiá, Olmo, and Irabien (2003), Kim and Kim (2002) and Ren and Zhao (2002).
0.121277 - While artificial neural network represents the most studied technique in order to address the prediction of concrete strength other machine learning techniques have been investigated.
0.071197 - For instance, in Demir (2005) the theory of fuzzy sets, especially fuzzy modeling, is discussed to determine elastic modulus of both normal and high-strength concrete.
0.054795 - A fuzzy logic algorithm has been devised for estimating elastic modulus from compressive strength of concrete.
0.061162 - The main advantage of fuzzy models is their ability to describe knowledge in a descriptive human-like manner in the form of simple rules using linguistic variables only.
0.057143 - The use of fuzzy logic to predict cement strength is also proposed in Gao (1997).
0.047059 - While the use of Genetic Programming to predict material strength has not been widely investigated, some works have been proposed.
0.042254 - The work in Gandomi, Alavi, and Sahab (2010) proposes a new approach for the formulation of compressive strength of carbon fiber reinforced plastic (CFRP) confined concrete cylinders using a promising variant of Genetic Programming namely, Linear Genetic Programming (LGP).
0.062500 - The LGP-based models are constructed using two different sets of input data.
0.046667 - The first set of inputs comprises diameter of concrete cylinder, unconfined concrete strength, tensile strength of CFRP laminate and total thickness of utilized CFRP layers.
0.041237 - The second set includes unconfined concrete strength and ultimate confinement pressure which are the most widely used parameters in the CFRP confinement existing models.
0.086667 - The results demonstrate that the LGP-based formulas are able to predict the ultimate compressive strength of concrete cylinders with an acceptable level of accuracy.
0.033898 - The work proposed in Chen (2003) is aimed at addressing a mixture-proportioning problem, which uses the macroevolutionary algorithm (MA) combined with Genetic Programming to estimate the compressive strength of HPC.
0.033898 - Genetic Programming provides system identification in a transparent and structured way, while MA could improve the capability of searching global optima and avoid premature convergence during the selection process of GP.
0.047059 - Experimental results show that the proposed model is better than the traditional proportional selection Genetic Programming for HPC strength estimation.
0.050692 - Data set information Following the same procedure described in Yeh (1998), experimental data from 17 different sources was used to check the reliability of the strength model.
0.019048 - Data were assembled for concrete containing cement plus fly ash, blast furnace slag, and superplasticizer.
0.045350 - A determination was made to ensure that these mixtures were a fairly representative group governing all of the major parameters that influence the strength of HPC and present the complete information required for such an evaluation.
0.065041 - The dataset consist in 1028 instances, each of them described by 8 variables that are reported in Table 1.
0.000000 - Table 1.
0.062016 - Variables used to describe each instance.
0.000000 - For each variable minimum, maximum, average, median and standard deviation values have been reported.
0.007036 - Minimum Maximum Average Median Standard Deviation Cement (kg/m3) 102.0 540.0 281.2 272.9 104.5 Fly ash (kg/m3) 0.0 359.4 73.9 22.0 86.3 Blast furnace slag (kg/m3) 0.0 200.1 54.2 0.0 64.0 Water (kg/m3) 121.8 247.0 181.6 185.0 21.4 Superplasticizer (kg/m3) 0.0 32.2 6.2 6.4 6.0 Coarse aggregate (kg/m3) 801.0 1145.0 972.9 968.0 77.8 Fine aggregate (kg/m3) 594.0 992.6 773.6 779.5 80.2 Age of testing (days) 1.0 365.0 45.7 28.0 63.2 5.2.
0.100851 - Experimental settings We tested the proposed implementation of GP with geometric semantic operators (GS-GP from now on) against a standard GP system (ST-GP).
0.071406 - A total of 50 runs were performed with each technique: this is a fundamental aspect given the stochastic nature of the considered systems.
0.000000 - In each run a different partition between training and test data has been considered.
0.083888 - In particular 70% of the instances have been used as training data, while the remaining have been used as test data.
0.082192 - All the runs used a population of 200 individuals and the evolution stopped after 2000 generations.
0.030303 - Trees initialization was performed with the Ramped Half-and-Half method (Koza, 1992) with a maximum initial depth equal to 6.
0.038095 - The function set contained the four binary arithmetic operators + , − , ∗, and/ protected as in Koza (1992).
0.038095 - Fitness was calculated as the root mean square error (RMSE) between predicted and target values.
0.018265 - The terminal set contained 8 variables, each one corresponding to a different feature in the dataset.
0.012232 - To create new individuals, ST-GP used standard (subtree swapping) crossover (Koza, 1992) and (subtree) mutation (Koza, 1992) with probabilities equal to 0.9 and 0.1 respectively.
0.031373 - For GS-GP, crossover rate is 0.7, while mutation rate is 0.3 (with a mutation step of 1).
0.032864 - The motivation for this different mutation (and crossover) rate for the two GP systems is that a preliminary experimental study has been performed (independently for the two systems) for finding the parameter setting able to return the best results.
0.076643 - Survival from one generation to the other was always guaranteed to the best individual of the population (elitism).
0.022989 - No maximum tree depth limit has been imposed during the evolution.
0.092827 - In the next section, experimental results are reported using curves of RMSE on the training and test set.
0.041667 - In particular, at each generation the best individual in the population (i.e.
0.064725 - the one that has the smaller training error) has been chosen and the value of its error on the training and test data has been stored.
0.066667 - The reported curves finally contain the median of all these values collected at each generation.
0.042194 - The median was preferred over the mean in the reported plots because of its higher robustness to outliers.
0.047962 - Results Plots in Fig 4(a) and (b) report training and test error at each generation, for ST-GP and GS-GP respectively and clearly show that GS-GP outperforms ST-GP on both training and test sets.
0.054795 - Evolution of (a) training and (b) test errors for each technique, median of 50… Fig 4.
0.057143 - Evolution of (a) training and (b) test errors for each technique, median of 50 runs.
0.054422 - Boxplots of (c) training and (d) test fitness.
0.065813 - Plots in Fig 4(c) and (d) report a statistical study of the results achieved by ST-GP and GS-GP, considering the 50 runs that have been performed.
0.034540 - Denoting by IQR the interquartile range, the ends of the whiskers represent the lowest datum still within 1.5· IQR of the lower quartile, and the highest datum still within 1.5· IQR of the upper quartile.
0.065041 - As it is possible to see, GS-GP outperforms ST-GP both on training and out of samples data.
0.097561 - To analyze the statistical significance of these results, a set of tests has been performed on the median errors.
0.038835 - As a first step, the Kolmogorov–Smirnov test has shown that the data are not normally distributed and hence a rank-based statistic has been used.
0.040000 - Successively, the Wilcoxon rank-sum test for pairwise data comparison has been used under the alternative hypothesis that the samples do not have equal medians.
0.046154 - The p-value obtained was 7.07 · 10 − 18 when results on training data have been considered, while we obtained a p-value of 3.79 · 10 − 16 when we considered results on test data.
0.043165 - Therefore, when using the usual significance level α = 0.01 we can clearly state that GS-GP produces solutions that are significantly lower (i.e., better) than the ones produced by ST-GP both on training and test data.
0.084799 - Comparison with other machine learning techniques Besides comparing GS-GP with ST-GP, we are also interested in comparing GS-GP with other well-known state of the art ML methods to have an idea of the competitiveness of the obtained results.
0.076055 - To perform the comparison with other ML methods we used the implementations provided by the Weka public domain software (Weka Machine Learning Project, 2013).
0.063270 - As done for the previous experimental phase, a preliminary study has been performed in order to find the best tuning of the parameters for the considered techniques.
0.121411 - The results of the comparison we performed are reported in Table 2.
0.000000 - Table 2.
0.120290 - Experimental comparison between different Machine Learning techniques.
0.036530 - Median error on training and test data over 50 runs has been reported for each technique.
0.015255 - Method Train Test Linear regression (Weisberg, 2005) 10.567 10.007 Square Regression (Seber & Wild, 2003) 17.245 15.913 Isotonic Regression (Hoffmann, 2009) 13.497 13.387 Radial Basis Function Network (Haykin, 1999) 16.778 16.094 SVM Polynomial Kernel (1 degree) (lkopf & Smola, 2002) 10.853 10.260 SVM Polynomial Kernel (2 degree) 7.830 7.614 SVM Polynomial Kernel (3 degree) 6.323 6.796 SVM Polynomial Kernel (4 degree) 5.567 6.664 SVM Polynomial Kernel (5 degree) 4.938 6.792 Artificial Neural Networks (Haykin, 1999) 7.396 7.512 Standard GP 7.792 8.67 Geometric Semantic GP 3.897 5.926 From this table it is possible to state that GS-GP performs better than all the considered ML methods on both training and test instances.
0.107092 - Beside GS-GP, SVM with polynomial kernel of fourth degree is the technique that produces the best performance on unseen data among the considered ones.
0.088471 - Increasing the degree of the polynomial used by SVM’s kernel, results in overfitting the training data.
0.045198 - To assess if the differences between GS-GP and the consider ML techniques are statistical significant, we performed a Wilcoxon rank-sum test with a Bonferroni correction for the value of α.
0.027211 - The p-values are summarized in Table 3.
0.082932 - From the results of the statistical test, we can state that GS-GP produces significantly better results on both training and test data with respect to all the other considered ML techniques.
0.025641 - Table 3. p-values given by the statistical test.
0.000000 - LIN SQ ISO RBF SVM-1 GS-GP TRAIN 6.88 · 10−18 6.88 · 10−18 6.88 · 10−18 6.88 · 10−18 6.88 · 10−18 TEST 6.88 · 10−18 6.88 · 10−18 6.88 · 10−18 6.88 · 10−18 6.88 · 10−18 SVM-2 SVM-3 SVM-4 SVM-4 ANNs GS-GP TRAIN 6.88 · 10−18 6.88 · 10−18 6.88 · 10−18 6.88 · 10−18 6.88 · 10−18 TEST 2.21 · 10−16 3.04 · 10−11 1.75 · 10−7 1.40 · 10−7 2.12 · 10−15
0.092960 - High-performance concrete is a highly complex material that makes modeling its behavior a difficult task.
0.059072 - In this study an intelligent GP-based system to predict the compressive strength of HPC has been proposed.
0.098203 - The system uses recently defined geometric semantic genetic operators that present several advantages with respect to standard GP operators.
0.051282 - In particular, they have the extremely interesting property of inducing a unimodal fitness landscape for any problem consisting in matching input data into known output ones (regression and classifications are instances of this general problem).
0.102653 - Experimental results show the suitability of the proposed system for the prediction of the compressive strength of HPC.
0.114340 - In particular, the proposed method outperforms standard Genetic Programming and returns results that are significantly better to the ones produced by other well-known machine learning techniques.

[Frase 4] In this paper, we propose an intelligent system based on Genetic Programming for the prediction of high-performance concrete strength.
[Frase 149] While artificial neural network represents the most studied technique in order to address the prediction of concrete strength other machine learning techniques have been investigated.
[Frase 222] In particular, the proposed method outperforms standard Genetic Programming and returns results that are significantly better to the ones produced by other well-known machine learning techniques.
[Frase 67] In this work we used genetic operators that, diversely from the canonical ones, are based on the concept of semantics that will be introduced in the next section.
