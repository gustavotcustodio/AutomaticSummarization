A proposal for online feature selection is proposed. The proposed pipeline covers discretization, feature selection and classification. Classical algorithms were modified to make them work online. K-means discretizer, Chi-Square filter and Artificial Neural Networks were used. Results show that classification error is decreasing, adapting to the arrival of new data.

0.086629 - With the advent of Big Data, data is being collected at an unprecedented fast pace, and it needs to be processed in a short time.
0.099800 - To deal with data streams that flow continuously, classical batch learning algorithms cannot be applied and it is necessary to employ online approaches.
0.063348 - Online learning consists of continuously revising and refining a model by incorporating new data as they arrive, and it allows important problems such as concept drift or management of extremely high-dimensional datasets to be solved.
0.174322 - In this paper, we present a unified pipeline for online learning which covers online discretization, feature selection and classification.
0.138578 - Three classical methods—the k-means discretizer, the χ2 filter and a one-layer artificial neural network—have been reimplemented to be able to tackle online data, showing promising results on both synthetic and real datasets.
0.042042 - During the last years and with increasing frequency, real-time production systems generated tremendous amount of data at unprecedented rates, such as network event logs, telephone call records or sensoring and surveillance video streams.
0.099800 - To deal with data streams that flow continuously, classical batch learning algorithms cannot be applied and it is necessary to employ online approaches.
0.068119 - Online data mining consists of continuously revise and refine a model by incorporating new data as they arrive (Wang, Fan, Yu, & Han, 2003).
0.072464 - Note that any online method is inherently incremental.
0.037037 - This type of learning has been applied in fields such as classification of textual data streams, financial data analysis, credit card fraud protection, traffic monitoring and predicting customer behavior (Elwell & Polikar, 2009; Katakis, Tsoumakas, & Vlahavas, 2006; Wang et al., 2003).
0.074870 - Most of these applications present a great challenge for machine learning researches due to the high amount of data available.
0.067796 - Theoretically, it could seem logical that having more features could lead to better results, but this is not always the case due to the so-called curse of dimensionality (Bellman, 1966).
0.065844 - This phenomenon happens when the dimensionality increases and the time required by the machine learning algorithm to train the data increases exponentially.
0.109860 - To overcome these problems, feature selection is a well-known dimensionality reduction technique.
0.073692 - Feature selection consists of selecting the relevant features and discarding the irrelevant ones to obtain a subset of features that describes properly the problem with a minimum degradation of performance (Guyon, Gunn, Nikravesh, & Zadeh, 2006).
0.067653 - A special case of feature selection is known as online feature selection (Glocer, Eads, & Theiler, 2005; Nguyen, Wu, & Mukherjee, 2015; Perkins & Theiler, 2003; Wang, Zhao, Hoi, & Jin, 2014; Wu, Yu, Wang, & Ding, 2010), which can be very useful, being one of the most interesting when a concept drift appears.
0.059524 - This phenomenon is present in situations where the underlying data distribution changes.
0.073883 - These changes make the model built on old data inconsistent with the new data, and regular updating of the model is necessary (Tsymbal, 2004).
0.095900 - Applied to feature selection, a concept drift may cause that the subset of relevant features changes over the time.
0.058608 - In other words, as time goes by, different sets of features become important for classification and some totally new features with high predictive power may appear.
0.085575 - Online feature selection has been faced mostly individually, i.e., by selecting features previously in a single step independent of the online machine learning step, or performing online feature selection without performing online classification afterwards.
0.079697 - Notice that after an online feature selection process, where the set of relevant features changes across the time, the classification algorithm has to be capable of updating its model according not only to new samples but also to new features, limiting the alternatives available capable of coping with both requirements.
0.152384 - Therefore, in this work we propose a method that covers both online feature selection and online learning.
0.120558 - Our proposal includes an algorithm that performs online feature selection and classification at the same time, by modifying a classical feature selection algorithm and introducing a novel implementation for a classification training algorithm.
0.064004 - Among the different feature selection methods available, we chose a representative of so-called filter methods (Guyon et al., 2006) since they are known for being fast, simple, classifier-independent and having a low computational cost (Bolón-Canedo, Sánchez-Maroño, & Alonso-Betanzos, 2013).
0.046346 - Specifically, we reimplemented the χ2 metric (Liu & Setiono, 1995), chosen because of its simplicity and effectiveness, as well as having some characteristics that make it inherently incremental.
0.086615 - However, this filter requires data to be discrete, and thus, well-known k-means discretizer (MacQueen et al., 1967; Tou & González, 1977; Ventura & Martinez, 1995) was also adapted to make it incremental.
0.066007 - The last step of our proposed online pipeline requires an incremental classifier, however, those available in the literature are incremental in the instance space, but not in the feature space.
0.075473 - Up to the authors’ knowledge, a complete pipeline as the one introduced here has not been presented elsewhere.
0.041116 - In fact, the popular machine learning tool Weka (Hall et al., 2009) provides methods able to receive new instances, but they do not support different sets of features, perhaps with different sizes, in each iteration.
0.098701 - Thus, an online training algorithm for one-layer artificial neural networks ANNs is also introduced in this paper, which continuously adapts the input layer to those features, that remind might vary in number, selected at each time.
0.046358 - In order to achieve this, we are presenting a new implementation of our previously proposed algorithm (Fontenla-Romero, Guijarro-Berdiñas, Pérez-Sánchez, & Alonso-Betanzos, 2010), which reaches a minimum error in a few epochs of training and exhibits a higher speed when compared to other classical methods.
0.080679 - Moreover, the structure of this algorithm makes it suitable for a dynamic input space, as happens when selecting features on-line.
0.097378 - In this research, we propose a novel implementation, which continuously adapts the size of the input layer to those features selected at each time.
0.093231 - In summary, the contribution of this paper consists of introducing an approach that uses three components together conforming a pipeline: (a) an online discretizer, (b) an online filter, and (c) an online learning algorithm; that will be applied to either online or large data.
0.042773 - The rest of the paper is organized as follows: Section 2 presents the state of the art in the field of online machine learning, Section 3 describes the method proposed in this research, Section 4 describes the experimental settings, Section 5 shows the experimental results, Section 6 is focused on a case study about the influence of the order of occurrence of the samples (data order) on the performance of the pipeline, finally, Section 7 presents the discussion and conclusions.
0.037225 - Online learning has become a trending area in the last few years since it allows to solve important problems such as concept drift or managing extremely high-dimensional datasets.
0.013072 - For this reason, advances in this field have recently appeared.
0.091036 - However, online feature selection has not evolve in line with online learning.
0.086806 - Zhang, Ruan, and Tan (2011) proposed an incremental computation feature subset selection algorithm which, originated from Boolean matrix technique, selects useful features for the given data objective efficiently.
0.099303 - Nevertheless, the efficiency of the feature selection method has not been tested with an incremental machine learning algorithm.
0.045198 - Keerthika and Priya (2015) examined various feature reduction techniques for intrusion detection, where training data arrive in a sequential manner from a real time application.
0.000000 - Katakis et al.
0.082474 - (2006) mentioned the idea of a dynamic feature space.
0.054146 - The features that are selected based on an initial collection of training documents are the ones that are subsequently considered by the learner during the operation of the system.
0.056338 - However, these features may vary over time and in some applications an initial training set is not available.
0.081364 - In the approach presented inhere, we are interested in flexible feature selection methods able to modify the selected subset of features as new training samples arrive, in both subset size and specific features selected.
0.064698 - It is also desirable that these methods can be executed in a dynamic feature space that would be empty at the beginning and add features when new information arrives (e.g., documents in their text categorization application).
0.000000 - Katakis et al.
0.083314 - (2006) applied incremental feature selection combined with what they called a feature based learning algorithm to deal with online learning in high-dimensional data streams.
0.073473 - This framework is applied to a special case of concept drift inherent to textual data streams, which is the appearance of new predictive words over time.
0.076503 - The problem with this approach is that they assume that features have discrete values.
0.052227 - Perkins, Lacker, and Theiler (2003) presented a novel and flexible approach, called grafting, which treats the selection of suitable features as an integral part of learning a predictor in a regularized learning framework.
0.055746 - To make it suitable for large problems, grafting operates in an incremental iterative fashion, gradually building up a feature set while training a predictor model using gradient descent.
0.073643 - Perkins and Theiler (2003) tackle the problem in which, instead of all features being available from the start, features arrive one at a time.
0.073881 - Online Feature Selection (OFS) assumes that, for any reason, is not affordable to wait until all features have arrived before learning begins, therefore one needs to derive a mapping function f from the inputs to the outputs that is as “good as possible” using a subset of just the features seen so far.
0.000000 - By Wu et al.
0.115227 - (2010), a promising alternative method, Online Streaming Feature Selection (OSFS), to online select strongly relevant and non-redundant features is presented.
0.000000 - Glocer et al.
0.048554 - (2005) demonstrated the power of OFS in the image processing domain by applying it to the problem of edge detection.
0.094020 - Mao, Yuan, Wu, Qu, and Li (2014) proposed a real-time compressive tracking algorithm based on online feature selection to address the problems of drifting and tracking lost caused by changes in the appearance of the tracked object.
0.093224 - The discriminating features selected are then integrated to construct a classifier to carry out the tracking process.
0.037037 - And Nguyen et al.
0.088338 - (2015) presented an online unsupervised feature selection method for background suppression in video sequences, that allows them to prune the feature set avoiding any combinatorial search.
0.158636 - Finally, some other researches have been found in the literature comprising online feature selection and classification.
0.092071 - Kalkan and Çetisli (2011) presented an online learning algorithm for feature extraction and classification, implemented for impact acoustics signals to sort hazelnut kernels.
0.126167 - Levi and Ullman (2010) proposed to classify images by ongoing feature selection.
0.057743 - However, their approach only uses at each stage a small subset of the training data.
0.114372 - Carvalho and Cohen (2006) performed online feature selection based on the weights assigned to each input of the classifiers.
0.062305 - Note, however, that this method is highly dependent on the classifier.
0.052493 - Another method that is strongly dependent on the classifier was presented by Wang et al.
0.000000 - (2014).
0.060803 - They addressed two different tasks of OFS: (1) learning with full input, where the learner is allowed to access all the features to decide the active ones, and (2) learning with partial input, where only a limited number of features is allowed to be accessed for each instance by the learner.
0.069597 - In a recent work, Roy (2015) proposed an interesting algorithm for streaming big data and for highly parallel implementation on Apache Spark based on Kohonen networks.
0.091816 - It examines some streaming data to select the features with a high discriminative power and then uses those features to learn pattern classifiers.
0.079772 - Kohonen networks trained in the first phase are discarded once features are selected.
0.092579 - So online feature selection has been dealt with mostly on an individual basis, or by performing online feature selection without subsequent online classification.
0.150667 - In the few researches that comprise online feature selection and classification, the methods proposed are highly dependent on the classifier.
0.035088 - Therefore, achieving real-time analysis and prediction for high-dimensional datasets remains a challenge for computational intelligence on portable platforms.
0.080174 - The question now is to find flexible feature selection methods capable of modifying the selected subset of features as new training samples arrive (Bolón-Canedo, Sánchez-Maroño, & Alonso-Betanzos, 2015).
0.057851 - It is also desirable for these methods to be executed in a dynamic feature space that would initially be empty but would add or remove features as new information arrived (e.g., documents in their text categorization application).
0.075117 - In this paper, we propose a unified pipeline that tries to fill the gap detected in the literature.
0.084910 - On the one hand, our proposal is able to modify the selected subset of features as new samples arrive (in both the number and the specific features selected) and, on the other hand, the classifier we include can be updated according not only to new samples but also to new features.
0.102990 - The proposed method consists of three independent stages that can be used alone or in a pipeline, bearing in mind that the filter requires discrete data.
0.085859 - Besides, we propose a general method that could be applied to a wide range of problems.
0.098947 - As explained before, the method introduced in this research consists of three online stages: discretizer, filter and classifier.
0.084507 - Fig 1 shows the flowchart of this method (parameters k and λ will be explained in the corresponding subsections).
0.077859 - Each step of the methodology and the reimplementation of the algorithms will be following described in depth.
0.069886 - Notice that not all the existing algorithms can be reimplemented to tackle online data, as they need to have some properties that make them inherently incremental (remember that the classifier must be incremental not only in the samples space but also in the feature space).
0.140259 - For this reason, the methods we have chosen to be reimplemented are the k-means discretizer, the χ2 filter, and a one-layer artificial neural network.
0.000000 - Fig 1.
0.087872 - Flowchart of the proposed method.
0.113185 - Discretizer Many feature selection algorithms are shown to work on discrete data, as it is the case of the filter selected in this work (χ2), therefore the first stage of our proposed method is devoted to online discretization.
0.090806 - However, due to the incremental nature of online learning, we cannot assume a range of input values for each feature a priori.
0.045198 - This fact prevents the use of well-known discretization algorithms such as entropy minimization discretization (EMD), equal width discretization (EWD) or equal frequency discretization (EFD).
0.088358 - For this reason, the k-means discretization algorithm (Tou & González, 1977; Ventura & Martinez, 1995) was chosen.
0.090137 - K-means has been selected by Wu et al.
0.056075 - (2008) as one of the most influential algorithms in data mining.
0.052632 - This algorithm moves the representative weights of each cluster along an unrestrained input space, making it suitable for our purposes.
0.099688 - It is important to note that each feature is discretized independently.
0.103792 - This clustering algorithm operates on a set of data points and assumes that the number of clusters to be determined (k) is given.
0.048110 - The partition is done based on certain objective function.
0.072988 - The most frequently used criterion function in k-means is minimizing the squared error ϵ criterion between the centroids μi of clusters and the samples x in those clusters Let C be the set of clusters and |C| its cardinality.
0.058072 - For each new sample x, the discretizer works as follows: • If |C| < k and x ∉ C then i.e., if the maximum number of cluster was not already reached and the new sample is not in C, then create a new cluster with its centroid in x.
0.000000 - • (else) 1.
0.056911 - Find the closest cluster to x.
0.041667 - Update its centroid μ as the average of all values in that cluster.
0.045977 - The method assigns at most k clusters.
0.082305 - Notice that the number of clusters is the minimum between the parameter k and the number of different values in the feature.
0.071038 - It is important to remark that in online methods there is no convergence criterion.
0.086957 - The system is continuously adapted while data arrives.
0.078740 - In Section 5 a discussion on the impact of k on the algorithm is presented.
0.044053 - Filter The χ2 metric (Liu & Setiono, 1995) was chosen because it evaluates each feature based on cumulative statistics concerning the number of times that it appears for a different class, which render it inherently incremental.
0.067736 - So, in our reimplementation, when a new instance appears, the statistics are updated and the evaluation can be calculated without the need of re-processing past data.
0.088358 - The χ2 method evaluates features individually by measuring their chi-squared statistic with respect to the classes.
0.038835 - The χ2 value of an attribute is defined as: (1) where (2) k being the number of intervals (number of different values in a feature), c the number of classes, Aij the number of samples in the ith interval, jth class, Ri the number of samples in the ith interval, Cj the number of samples in the jth class, S the total number of samples, and Eij the expected frequency of Aij.
0.093384 - Note that the size of the matrices is related to the number of intervals.
0.071247 - In this manner, a very large k in the discretizer will lead to a very large size of the matrices A and E. A very large matrix is computationally expensive to update and should be taken into account for real-time applications.
0.051051 - After calculating the χ2 value of all considered features, these values can be sorted with the largest one at the first position, as the larger the χ2 value, the more important the feature is.
0.043478 - This will provide an ordered ranking of features.
0.101186 - To automatically select the important features in an online manner a threshold needs to be added to the original algorithm.
0.100435 - The problem of selecting a threshold for rankers is still one of the open issues in Feature Selection research.
0.060914 - At present, there is not yet a general and automatic method that allows researchers to establish a threshold for any given data set (Bolón-Canedo et al., 2013).
0.054374 - Some authors have tried some kind of automatic threshold that is related with the means and variance of the weights obtained for the features in the rankers (Molina, Belanche, & Nebot, 2002), others with the largest gap between two consecutive attributes (Mejía-Lavalle, Sucar, & Arroyo, 2006).
0.077135 - However, the most frequent approach is to test the results of a classifier after retaining different percentages of the ranked features (Khoshgoftaar, Golawala, & Hulse, 2007), and thus the threshold should be tailored for the specific problem being studied.
0.056980 - In this paper, we propose a threshold λ which works in the following way.
0.089172 - On each iteration, given the χ2 value for each feature, the mean and the standard deviation of these values is computed.
0.091168 - Note that the initial set of features is the full set of features.
0.099738 - For each feature i and iteration t: • if then the feature i is not selected.
0.107280 - • if then the feature i is selected.
0.045584 - • otherwise, the feature i maintains the same state as in the previous iteration.
0.107204 - When λ is 0, the features selected in each iteration fluctuate significantly.
0.104801 - On the other hand, when λ tends to infinity, there is no feature selection process.
0.058394 - A further discussion about the impact of different values of λ can be found in Section 5.2.
0.059524 - Fig 2 shows an example of the use of λ in the filter.
0.054054 - In the current iteration, features with a χ2 value over are selected (features 1, 2 and 9) while features with a χ2 value under are not selected (features 3, 4, 5, 7 and 8).
0.046948 - Those features with a χ2 value between mean ± λstd maintains the same state as in the previous iteration.
0.074561 - In this case, assuming that feature 6 was selected in the previous iteration then it will be also selected now.
0.000000 - Fig 2.
0.105996 - Example of the use of λ in the feature selection process.
0.122421 - Classifier For the classification step of our online pipeline, a one-layer artificial neural network was chosen.
0.062305 - Notice that in online applications, real-time response is often demanded.
0.049844 - Thus, a light-weight machine learning algorithm is an appropriate election.
0.082863 - Moreover, the algorithm must be incremental in both input and sample space, which is not a characteristic supported by most of the available classification algorithms.
0.057069 - In a previous work, we proposed an incremental training algorithm for one-layer ANNs (Fontenla-Romero et al., 2010), which reaches a minimum error in a few epochs of training and exhibits a higher speed when compared to other popular methods.
0.062016 - Besides these characteristics, the structure of this algorithm makes it suitable for a dynamic input space, as it is the case in this research.
0.136616 - A new implementation is proposed herein, so as to be able to continuously adapt the input layer to the features selected in each iteration.
0.097394 - Our proposal is a one-layer neural network which is fast and has the capability of adapting its number of inputs to the number of features that are selected at a given step, adding or removing neurons as needed.
0.029466 - In a one-layer ANN (see Fig 3), the set of equations relating inputs and outputs is given by (3) where I, J, S are the number of inputs, outputs and training samples, respectively, wji is the weight of the connection between the ith input neuron and the jth output neuron, and fj is the nonlinear activation function of jth output neuron.
0.023810 - The system described by Eq (3) has J × S equations in unknowns.
0.056497 - However, since the number of samples of data is often large , in practice, this set of equations and wji is overdetermined and has no solution.
0.072507 - Thus, as the errors ϵjs between the real (yjs) and the desired output (djs) of the network are defined by (4) djs being the desired output for neuron j, and usually the sum of squared errors is minimized to learn the weights wji.
0.000000 - (5) Fig 3.
0.032520 - Architecture of a one-layer ANN.
0.048780 - However, if it is assumed that the nonlinear activation functions fj are invertible (as it is the case for the most commonly used functions), alternatively, the system of equations in Eq (4) can be rewritten in the following way (Fontenla-Romero et al., 2010): (6) where and .
0.043614 - Eq (6) measures the errors in terms of the inputs (xis).
0.057199 - It is important to note that in Eq (6) the unknowns (weights of the network) are not affected by the nonlinear activation function fj, i.e., the error is linear with respect to weights.
0.039667 - Then, an alternative objective function is obtained to be minimized (Fontenla-Romero et al., 2010): (7) whose global minimum can be computed by solving the system of equations obtained by equalizing its derivative to zero: (8) where (9) For every output j, Eq (9) has linear equations and unknowns and, thereby, there exists only one real solution which corresponds to the global optimum of the objective function.
0.059493 - Several computationally efficient methods can be used to solve this kind of systems with a complexity of where I and J are the number of inputs and outputs of the ANN, respectively (Bojańczyk, 1984; Carayannis, Kalouptsidis, & Manolakis, 1982).
0.079096 - Furthermore, this training algorithm is able to learn incrementally since the coefficients Api and bpj are calculated as a sum of terms (see Eq (9)).
0.070007 - Due to the commutative and associative properties of the sum, the same solution is obtained independently of the order of occurrence of the samples.
0.089202 - The structure of the matrices A and b is also suitable for a dynamic space of input features.
0.070423 - On the one hand, removing a feature only comprises removing the row and column corresponding with that feature.
0.079812 - On the other hand, adding a new feature only comprises adding a row and a column of zeros.
0.063872 - Note that a row and a column of zeros in the matrices A and b corresponds with learning from the scratch that feature.
0.075472 - Thus, this method is able to adapt the architecture of the one-layer ANN and deal with changing environments in which the relevant features may be different at one time or another.
0.068027 - The experiments presented in this section are focused on the evaluation of the online method proposed in this work.
0.091034 - The three methods (discretizer, filter and classifier) will be evaluated both independently and also integrated in the unified pipeline.
0.072606 - Materials Five different classification datasets were used during this research.
0.059448 - Corral, LED, and Friedman are synthetic while Connect and Forest were selected from the UCI Machine Learning Repository (Asuncion & Newman, 2007).
0.055888 - Table 1 depicts the characteristics of these datasets: number of features, number of instances, number of classes, and percentage of the majority class.
0.000000 - Table 1.
0.046296 - Characteristics of the datasets.
0.000000 - Dataset No.
0.043011 - features No.
0.000000 - instances No.
0.000000 - classes % maj. class (%) Corral-100 100 10 000 2 56.02 LED-100 100 10 000 10 10.00 Friedman-100 100 10 000 2 54.57 Connect4 42 67 557 3 65.83 Forest 54 101 241 7 48.69 4.1.1.
0.068358 - Corral-100 In this research, a modified version of the CorrAL dataset (John, Kohavi, & Pfleger, 1994) will be used.
0.052632 - The original dataset has four binary relevant features one irrelevant feature f5 and one feature f6 correlated with the output.
0.028986 - Its class value is (f1∧f2)∨(f3∧f4).
0.039886 - This research is not focused on detecting correlation, but on discarding irrelevant features.
0.119883 - The correct behavior for a given feature selection method is to select the four relevant features and to discard the irrelevant ones.
0.065844 - A new dataset called Corral-100 will be employed, consisting of the 4 relevant features plus 96 irrelevant binary features randomly generated.
0.056497 - The LED-100 problem The LED problem (Breiman, 1984) is a simple classification task that consists of identifying the digit that the display is representing.
0.060264 - Given the active leds described by seven binary attributes (seven segments display), the task to be solved is its classification in ten possible classes available .
0.065789 - A 1 in an attribute indicates that the led is active, and a 0 indicates that it is not active.
0.049844 - The LED-100 problem was constructed by adding 93 irrelevant features.
0.023810 - Friedman-100 This synthetic dataset uses a function suggested by Friedman (1991).
0.048309 - It is defined by the equation , where σ(0, 1) is zero mean unit variance Gaussian noise and the inputs are sampled independently from a uniform distribution in the interval [0, 1].
0.040650 - This dataset is a regression task.
0.061674 - We transformed it into a classification task where the goal was to predict class 1 for the examples of output under 15 (prevalence around 55%) and class 2 for the other examples (prevalence around 45%).
0.066071 - The Friedman-100 dataset was constructed by adding 95 irrelevant features to the previous Friedman dataset.
0.089239 - The data for the added features were generated randomly from a uniform distribution [0, 1].
0.041868 - Connect4 This database contains all legal 8-ply positions in the game of connect-4 in which neither player has won yet, and in which the next move is not forced.
0.047244 - The outcome class is the game theoretical value for the first player (win, loss, draw).
0.105691 - The number of features is 42.
0.028986 - All features are categorical with 3 possible values.
0.049180 - Forest This dataset is a classification task with 7 classes (representing forest cover types).
0.023810 - Predicting forest cover type from cartographic variables only (no remotely sensed data).
0.046346 - The actual forest cover type for a given observation (30 × 30 meter cell) was determined from US Forest Service (USFS) Region 2 Resource Information System (RIS) data.
0.055556 - Independent variables were derived from data originally obtained from US Geological Survey (USGS) and USFS data.
0.048964 - Data is in raw form (not scaled) and contains binary (0 or 1) columns of data for qualitative independent variables (wilderness areas and soil types).
0.105691 - The number of features is 54.
0.077859 - Performance measures Discretization is concerned with the process of translating continuous values of features into discrete values.
0.098750 - As a result, some error is committed during the process.
0.078740 - The discrepancy between the exact value and some approximation to it is called approximation error.
0.077429 - The absolute approximation error is defined as the magnitude of the difference between the exact value and the approximation.
0.056648 - Given some value v and its approximation the absolute error is computed as follows: (10) The relative approximation error is the absolute error divided by the magnitude of the exact value, and it is computed as follows: (11) Respect to the filter method, its efficiency was evaluated in terms of precision, recall, percentage of selected features, and stability.
0.068447 - Precision is the fraction of the selected features that are relevant and it is computed as follows: (12) Recall is the fraction of the relevant features that are selected and it is computed as follows: (13) Note that the relevant features of the dataset have to be known to compute these measures.
0.102777 - The percentage of selected features is simply computed as the fraction of the features selected and the total number of features.
0.103097 - To evaluate the stability of the filter the Jaccard index will be used.
0.090535 - The Jaccard similarity coefficient (Paul, 1901) is a measure used for comparing the similarity of two sets of features A and B.
0.060366 - It is defined as the fraction of the cardinality of the intersection and the cardinality of the union of the features (14) In an online environment, the stability of the filter will be related with the set of features selected in the current step in comparison with the set of features selected in the previous step.
0.062878 - Finally, the performance of the classifier is computed in terms of standard classification error (Weiss & Kulikowski, 1990) which is defined as the fraction of samples incorrectly classified over the data.
0.091365 - The experimental results obtained for each of the three methods and the proposed pipeline are presented in this section.
0.074074 - Discretizer For this experiment, we chose the first feature from the Friedman dataset.
0.063492 - Note that every feature in this set is sampled independently from a uniform distribution in the interval [0, 1].
0.084967 - Fig 4 shows the absolute approximation error of the discretizer.
0.077381 - The variable k indicates the number of clusters used in the discretizer.
0.060606 - Notice that the values of the feature appear in random order along the interval [0, 1].
0.076580 - If the values of the features lay approximately along all their possible values the discretizer shows a very fast adjustment to the data, as seen in the graph.
0.035714 - As expected, the larger the number of clusters, the better the adjustment.
0.054945 - Note however that a very large number of clusters increases computations at the expense of a decrement in terms of approximation error that should be evaluated.
0.054945 - For example, using instead of increases computation by 60% but only decreases the approximation error by 20% (from 0.06 to 0.04, see Fig 4).
0.116990 - Note also that a larger number of k implies larger matrices in the feature selection filter.
0.000000 - Fig 4.
0.084926 - Absolute approximation error of the online discretizer in a feature sampled independently from a uniform distribution in the interval [0, 1].
0.132229 - Filter To check the efficiency of the feature selection method proposed, the two synthetic datasets Corral-100 and LED-100 were employed.
0.114485 - We have chosen to use artificially generated data because the desired output is known, therefore a feature selection algorithm can be evaluated with independence of the classifier.
0.080460 - The main advantage of artificial datasets is the knowledge of the set of optimal features that must be selected; thus, the degree of closeness to any of these solutions can be assessed in a confident way.
0.000000 - Figs.
0.057240 - 5 and 6 show the performance of the feature selection filter for the Corral-100 and LED-100 datasets, respectively, in terms of precision, recall, percentage of selected features, and stability for different values of the parameter λ (threshold for the filter, see Section 3.2 for further details).
0.000000 - Fig 5.
0.126954 - Performance measures of the online feature selection filter in the Corral-100 dataset.
0.000000 - Fig 6.
0.126954 - Performance measures of the online feature selection filter in the LED-100 dataset.
0.030651 - As can be seen in both Figs.
0.043796 - 5 and 6, the lower the value of the parameter λ the faster the curve of precision converges.
0.082725 - Moreover, the percentage of selected features is lower but at the expense of a more unstable behavior.
0.030303 - On the other hand, the higher the value of λ the slower the curve of precision converges.
0.095238 - However, the filter shows a more stable behavior, because the number of selected features is larger in this case.
0.089286 - Note that the classifier needs to learn from scratch each new feature.
0.070246 - If the subset of features selected by the filter changes frequently then the classifier will lose partial knowledge many times during the learning process.
0.064171 - Thus an appropriate selection of this parameter plays a crucial role in the different measures, for example in this case appears to be the most sensible solution.
0.058140 - Notice that a large value of the parameter λ will not find the optimum subset of features (the curve of precision does not converge for ).
0.063492 - Classifier The efficiency of the classifier is also shown on the two synthetic datasets Corral-100 and LED-100.
0.101493 - The objective is to train the classifier when the input space is changeable by adding or removing features as happens in online feature selection.
0.088028 - Synthetic datasets are useful here to check the impact in terms of classification error when adding or removing relevant features.
0.080515 - Data were divided using holdout validation, i.e., a subset of samples is chosen at random to form the test set and the remaining observations are retained as the training set.
0.080168 - In this research, the 10% of data were used for testing while the 90% were used for training.
0.062271 - In other words, after a new training sample arrives and the model is updated, its performance is tested on the 10% data left as test set.
0.084675 - Regarding the set of features selected in each step, three different situations were considered: • The set of selected features contains all relevant features and none irrelevant.
0.095238 - • The set of selected features contains all relevant features and some irrelevant.
0.095238 - • The set of selected features contains some relevant features and some irrelevant.
0.072893 - Fig 7 shows the classification error in the Corral-100 and LED-100 datasets when the input space changes.
0.068966 - Fig 7(a) shows the performance of the classifier on the Corral-100 dataset in the following situations, • From 1 to 3000 samples, the set of selected features contains all (4) relevant features and 96 irrelevant.
0.097324 - • From 3001 to 5000 samples, the set of selected features contains all relevant features and none irrelevant.
0.097324 - • From 5001 to 8000 samples, the set of selected features contains 2 relevant features and some irrelevant.
0.097324 - • From 8001 to 10000 samples, the set of selected features contains 1 relevant feature and some irrelevant.
0.000000 - Fig 7.
0.096530 - Classification error of the classifier.
0.075473 - As can be seen in Fig 7(a), the classifier reaches its minimum classification error in few epochs.
0.069597 - At this point, if the set of selected features is reduced to contain only relevant features (from 3001 to 5000 samples), the classifier maintains its performance.
0.073260 - However, if the set of selected features only contains some of the relevant features (from 5001 to 10000 samples), the classification error of the classifier increases because it is only able to converge to a local minimum.
0.084887 - As expected, the lesser the relevant features selected the worse the performance of the classifier.
0.089239 - It is also interesting to check the performance of the classifier when adding relevant features.
0.061069 - In a similar manner to above, Fig 7(b) shows the performance of the classifier on the LED-100 dataset in the following situations, • From 1 to 1000 samples, the set of selected features contains all (7) relevant features and 94 irrelevant.
0.101010 - • From 1001 to 3000, the set of selected features contains 3 relevant features and 44 irrelevant.
0.097324 - • From 3001 to 5000 samples, the set of selected features contains 5 relevant features and 94 irrelevant.
0.097324 - • From 5001 to 10000 samples, the set of selected features contains all relevant features and 44 irrelevant.
0.062016 - As can be seen in Fig 7(b), the classifier improves its performance according as the set of selected features contains more relevant features.
0.078431 - Notice that if only some (or none) relevant features are selected as the set of current features, the classifier is only able to reach a local minimum.
0.095238 - Finally, as can be inferred from both Fig 7(a) and (b), the online classifier proposed in this research is able to efficiently adapt its structure to changes in the input feature space.
0.152120 - Pipeline For testing the unified pipeline (discretized plus filter plus classifier), the classification error will be used along with the number of features selected.
0.086420 - Note that a slight degradation in the performance of the classifier may be acceptable if the number of features is significantly reduced.
0.052288 - The following procedure was followed for each new sample: 1.
0.110463 - Discretize the sample using the k-means discretizer.
0.089347 - Select the most relevant features using the χ2 filter.
0.072464 - Update the one-layer ANN using those features.
0.131483 - Compute the test classification error and the percentage of selected features.
0.132141 - The goal here is to compare the performance of the system with and without feature selection.
0.024922 - The experimental setup was the same as in the previous section.
0.070484 - Data were divided using holdout validation, 10% of data were used for testing while the 90% were used for training.
0.071856 - After a new training sample arrives, the model is updated and its accuracy is measured on the 10% data left as test set.
0.054645 - This type of validation is appropriate because the size of the datasets is large.
0.028490 - Moreover, every experiment was repeated 10 times in order to ensure unbiased results.
0.048611 - Finally, a Kruskal–Wallis test (Wolfe & Hollander, 1973) was applied to check if there are significant differences among the medians of the methods for a level of significance .
0.045045 - If there are differences among the medians, we then applied a multiple comparison procedure (Hsu, 1996) to find the method whose performance is not significantly different from the method with the best mean performance.
0.029197 - In this work, a Tukey’s honestly significant criterion (Hsu, 1996) was used as multiple comparison test.
0.000000 - Figs.
0.121816 - 8–10 show the classification error of the online classifier and the percentage of features selected by the online filter in Friedman, Connect4 and Forest datasets, respectively.
0.112353 - Note that the percentage of selected features when no features selection is applied ( ) is always 1.
0.047619 - For purposes of simplicity, only the first 10, 000 samples are shown.
0.065476 - The classifier and the filter maintain their behavior for the remainder samples.
0.000000 - Fig 8.
0.109048 - Performance measures of the proposed method in Friedman dataset in which means that no feature selection was applied.
0.000000 - Fig 9.
0.109048 - Performance measures of the proposed method in Connect dataset in which means that no feature selection was applied.
0.000000 - Fig 10.
0.109048 - Performance measures of the proposed method in Forest dataset in which means that no feature selection was applied.
0.081807 - In any case, the statistical tests indicate that the classification error obtained when no feature selection is applied is similar to that obtained when the less aggressive λ is used, with the additional advantage, in this last case, that only 37%, 44% and 56% of the relevant features are used for Friedman, Connect4 and Forest, respectively.
0.093296 - If a more aggressive feature selection is performed, the percentage of features used for learning is significantly reduced but at the cost of a significant higher error.
0.071038 - Finally, the issue of selecting the parameter λ in real datasets is an open question.
0.061674 - It will depend, on the first term, on the tradeoff between accuracy and speed (lower or higher number of features) and, on the last term, it will need to be determined by trial and error.
0.054598 - As a rule of thumb, we suggest not using very low lambda values (next to 0.0) because it will tend to select smaller sets of features, consequently the filter will have a more unstable behavior.
0.034483 - In this section we introduce an exhaustive analysis of how the order of appearance of the samples affects the performance of the machine learning algorithms chosen in this work, an important aspect for on-line algorithms.
0.039216 - In the previous experiments, samples appeared in a random order.
0.055202 - However, notice that in an online environment data may arrive in a certain order, i.e., following biased distributions of data.
0.050314 - It is possible that either all the data keep the same distribution, or that a so-called concept drift appears, leading to data following different distributions at different times of the process.
0.050314 - Discretization In Section 5.1 we have shown discretization experiments when the values of the feature (the first feature of Friedman dataset) appeared in a random order along the interval [0, 1].
0.075848 - However, as the discretizer is not order independent, it is also important to check its efficiency when data arrives in a certain order.
0.045455 - The curve of error depicted in Fig 11 assumes a certain order of appearance of samples.
0.053498 - In particular, the values of the feature are as follows: • From 1 to 250 samples its values lay in [0, 0.33].
0.022792 - • From 250 to 500 samples its values lay in [0.34, 0.66].
0.023810 - • From 500 to 750 samples its values lay in [0.67, 1].
0.024922 - • From 750 to 1000 samples its values lay in [0, 1].
0.000000 - Fig 11.
0.063063 - Absolute approximation error of the online discretizer in a feature, the first one of Friedman dataset, sampled independently from a uniform distribution in the interval [0, 1], with biased order of appearance of samples.
0.100775 - As can be seen, the discretizer is also able to tackle with biased distributions of the values of a feature by storing past knowledge.
0.030303 - At the end, the centroids of the clusters are located in averaged positions along the samples.
0.113095 - Thus, the discretizer is considered to be quite robust to biased distributions.
0.049887 - Finally, Fig 12 displays the curve of error of each execution when the feature values appear in random order.
0.048662 - In this experiment, 1000 samples extracted from a uniform distribution in the interval [0, 1] were employed.
0.056980 - For each value of k (number of clusters), 50 executions were carried out.
0.043860 - As expected, the larger the number of clusters, the larger the number of samples needed to reach the minimum error.
0.052227 - It is interesting to note that this experiment proves that the order of appearance is not critical given a certain number of samples, since the error of each execution converges to similar values.
0.000000 - Fig 12.
0.079840 - Absolute approximation error of the online discretizer of each execution when the feature values appear in random order (first feature of Friedman dataset).
0.074122 - Feature selection In an online scenario, data order can also affect the feature selection process, so an experiment was carried out on Friedman dataset.
0.050388 - For this sake, 10 features are considered, where the first five are relevant and the remaining ones are irrelevant (see Section 4.1.3).
0.032922 - The dataset used consists of 2000 samples where the order of appearance was randomly generated in each one of the 100 executions.
0.097324 - For the discretization stage, different values of k (number of clusters) were employed, from 1 to 100.
0.074074 - After seeing the whole dataset, the χ2 value of each feature is computed.
0.054146 - Fig 13 shows the distance between the minimum and maximum χ2 values of the relevant and irrelevant features, respectively –i.e., the minimum margin between relevant and irrelevant features.
0.033970 - The pale lines represent each one of the 100 executions, whilst the mean ± standard deviation are also displayed in bold lines.
0.047619 - The margin increases until around and from this point on, it stabilizes.
0.067797 - In this case, data order seems to have a low impact on the performance of the filter because the maximum standard deviation is around 10%.
0.000000 - Fig 13.
0.060109 - Minimum distance between relevant and irrelevant features according to χ2 values on Friedman dataset.
0.058140 - Classification Finally, we test the influence of data order when the last step of classification is included in the learning process on Forest dataset.
0.078015 - So far, it has been shown that the data order has little influence on the discretizer (see Section 5.1) and consequently, on the filter (see Section 5.2).
0.092233 - Fig 14 displays the effect of the data order on the proposed pipeline.
0.057018 - The parameter λ was established to 0.8, since it has been proved to obtain a good performance with this dataset.
0.035088 - The training dataset used consists of 5000 balanced samples where the order of appearance was randomly generated in each execution.
0.097953 - For assessing the classification error, a set of 1000 independent samples was used, and a total of 100 executions were accomplished.
0.000000 - Fig 14.
0.045752 - Data order effect on the overall pipeline in Forest dataset.
0.075627 - On the one hand, Fig 14(a), (c) and (e) (first column) depict the classification error trace of each execution for different values of k (number of clusters for the discretization stage).
0.067340 - On the other hand, Fig 14(b), (d) and (f) (second column) display the percentage of times that a feature is selected in average for all the executions, again for different values of k. Remind that the smaller the k, the more information is missed in the discretization step.
0.058394 - For this reason, when it can be seen that there are more differences among the different executions.
0.100963 - This fact may be caused by the information lost in the discretization stage, which also affects to the features selected by the filter, resulting in an irregular classification, where the data order plays an important role.
0.074866 - However, when k increases, the behavior of the pipeline is more stable and independent of the order, as can be seen in Fig 14(c) and (e).
0.137488 - The classification error decreases and the stability of the selected features raises.
0.086273 - Finally, Fig 14(g) visualizes the average classification error for the three values of k tested.
0.085215 - As can be seen, the classification error improves notably from to higher values, whilst no significant differences are found between and .
0.092414 - In light of the above, it seems reasonable to select the lowest value for k without compromising the classification error.
0.070007 - In this case, might be the optimal, since it converges to the same error than and decreases computations, as mentioned in Section 5.1.
0.168002 - In this work, we have presented a complete pipeline (covering discretization, feature selection and classification) which is capable of continuously updating its model to learn from online data.
0.094613 - One of the strengths of the proposed method is that it consists of three independent stages that can be used alone or in a pipeline.
0.069310 - Up to the authors’ knowledge, there is no other work in the literature that covers efficiently these three stages, since some of the existing approaches apply feature selection in an off line fashion (because the online classifiers cannot deal with changing subsets of features) or they apply online feature selection but then this is not connected with an online classification stage.
0.171296 - Therefore, the main advantage of our proposed method is that it allows researchers to perform both online feature selection and classification.
0.120154 - The key contributions of this paper are the following ones: • Adaptation and reimplementation of the χ2 filter to perform online feature selection.
0.136748 - • Since the χ2 filter requires data to be discrete, adapting the k-means discretizer to be used in an online fashion.
0.077926 - • The adaptation of a learning algorithm for one layer neural network to be incremental not only in the instance space, but also in the feature space, allowing for feature subsets that changes, increasing or reducing in number during the learning process.
0.048309 - • Since an important aspect of on-line algorithms is the impact of data order on the performance of the methods, this issue is specially assessed, showing the robustness of the method.
0.020997 - This is crucial in some real life environments in which concept-drift situations might appear.
0.081635 - However, although to the best of our knowledge a complete pipeline as this one has not been presented elsewhere, our proposal has some limitations.
0.089328 - Most of all, it is important to notice that not all the machine learning methods available in the literature can be adapted to deal with online data.
0.078928 - Therefore, although more sophisticated learning methods exist, they cannot be adapted to learn in an online manner, and we had to choose simpler models such as the χ2 filter and an ANN.
0.075330 - Although simple, the chosen methods demonstrated to be adequate for this type of learning, exhibiting promising results, both separately and when combined in a pipeline.
0.197991 - Experimental results showed that the classification error is decreasing over the time, adapting to the appearance of new data.
0.095238 - Plus, the number of features is reduced while maintaining the classification performance.
0.113816 - Another restriction, which is not specific of the pipeline, but general for machine learning methods (including preprocessing methods, such as discretization and feature selection), is the need for parameter estimation, that should be adapted to each problem under consideration.
0.037225 - At this respect, some general recommendations are given through the specific subsections of the paper, although this issue is still an open problem, in which researchers are still working.

[Frase 371] Experimental results showed that the classification error is decreasing over the time, adapting to the appearance of new data.
[Frase 5] Three classical methods—the k-means discretizer, the χ2 filter and a one-layer artificial neural network—have been reimplemented to be able to tackle online data, showing promising results on both synthetic and real datasets.
[Frase 24] Our proposal includes an algorithm that performs online feature selection and classification at the same time, by modifying a classical feature selection algorithm and introducing a novel implementation for a classification training algorithm.
[Frase 4] In this paper, we present a unified pipeline for online learning which covers online discretization, feature selection and classification.
