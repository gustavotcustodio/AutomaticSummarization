A modular neural network is proposed for both predicting and programming problems. The programming is able to represent problems in modular functions mathematically. Parameter impacts and functional influences were addressed for concrete strengths. Good accuracies and programmed formulas were provided for high strength concrete.

0.207937 - This study proposes a modular neural network (MNN) that is designed to accomplish both artificial intelligent prediction and programming.
0.117063 - Each modular element adopts a high-order neural network to create a formula that considers both weights and exponents.
0.154493 - MNN represents practical problems in mathematical terms using modular functions, weight coefficients and exponents.
0.086207 - This paper employed genetic algorithms to optimize MNN parameters and designed a target function to avoid over-fitting.
0.168498 - Input parameters were identified and modular function influences were addressed in manner that significantly improved previous practices.
0.103026 - In order to compare the effectiveness of results, a reference study on high-strength concrete was adopted, which had been previously studied using a genetic programming (GP) approach.
0.101336 - In comparison with GP, MNN calculations were more accurate, used more concise programmed formulas, and allowed the potential to conduct parameter studies.
0.180988 - The proposed MNN is a valid alternative approach to prediction and programming using artificial neural networks.
0.081428 - Soft-computing approaches have been refined into specialized branches that include neural networks (NNs), fuzzy logic, support vector machines, genetic algorithms (GAs) and genetic programming (GP), among others.
0.016667 - Each has its particular merits in comparison with others.
0.060606 - NNs are the most familiar soft-computing approach for learning tasks.
0.077519 - The tools categorized into NNs, including back propagation networks (BPNs), radial basic functions, self-organizing mapping and learning vector quantization, are already applied in many fields for data mining and knowledge learning.
0.053333 - BPNs are the most widely applied, accounting for over 70% of all NN applications.
0.067901 - A neuron in the BPN model calculates net value based on associated inputs and multiplicative weights.
0.077020 - As they lack hidden layers, BPNs are not able to address complicated problems.
0.014493 - To enhance BPN capacity, hidden layers are employed between input-output mapping.
0.084259 - High order neural networks (HONNs) were developed around 1990.
0.055556 - HONN’s nonlinear combination of inputs (Zurada, 1992), allows for easy capture of high order correlations and efficient nonlinear mapping (Abdelbar & Tagliarini, 1996; Tsai, 2009).
0.072150 - HONNs have been applied widely in various research domains, especially to problems related to pattern recognition and function approximation (Artyomov & Yadid-Pecht, 2005; Foresti & Dolso, 2004; Rovithakis, Chalkiadakis, & Zervakis, 2004; Wang & Lin, 1995).
0.031746 - References show the HONN as a powerful soft-computing approach.
0.115072 - HONN designs can further be specially adapted to represent problems in more meaningful ways.
0.029412 - Since it was first proposed by Koza (1992), GP has received much attention with regard to modeling nonlinear relationships in input-output mappings.
0.104798 - GP creates solutions as programs (formulas) to solve problems with an operation tree.
0.095833 - As some HONN models provide such programming functions as well, HONN is considered a branch of NN that, in contrast to GP, programs nonlinear relationships for input-output mappings.
0.217491 - This study proposes a modular neural network (MNN) able to accomplish both artificial intelligent prediction and programming.
0.097767 - The proposed MNN comprises HONN models and modular functions, with each HONN model containing weight and exponent coefficients able to transfer inputs into meaningful polynomials.
0.108262 - The remaining sections of this paper include Section 2: Proposed MNN and GA optimization; Section 3: Programming high-strength concrete parameters to study MNN capacities in comparison with GP results; Section 4: Suggestions for further studies and future work and Section 5: Conclusions.
0.080747 - Modular neural network The output (O) of the modular neural network (MNN) is a combination of several modular elements (M), with relationships wK, where w represents weight coefficients and K denotes the module index from 0 to NK (meaning that there are NK function selections and a bias term).
0.055556 - Hence, O can be mathematically represented as (see Fig 1): (1) where M0 is a bias unit with a value of 1.
0.107143 - Modular neural network Fig 1.
0.123016 - Modular neural network.
0.096154 - Each MK is activated by a weighted summation operator and a setting function type FK.
0.100000 - MK is a derivative of a HONN (Abdelbar & Tagliarini, 1996) designed for programming simplicity.
0.039216 - This HONN has three layers, i.e., an input layer, a hidden layer and an output layer, with neuron numbers NI-NJ-1.
0.043478 - The eventual layer has only one neuron, i.e., modular element MK.
0.050926 - There are NI input neurons XIK and NJ hidden neurons YJK indexed of I and J respectively in each modular element MK (see Fig 2).
0.000000 - (2) (3) MNN module Fig 2.
0.000000 - MNN module.
0.057292 - The three layers are connected through two connections functioned, respectively, by an exponent-product (pIJK-Π) connection and weight-summation (wJK-Σ) connection.
0.075000 - Coefficients wK, wJK, and pIJK activate the proposed MNN.
0.069892 - Besides, in contrast with traditional NNs, MNN has no activation function (using an identity function) for the first two layers.
0.103448 - Activation function FK is associated with modular output MK, and each FK is assigned a user-selected function.
0.062500 - Such function assignments depend on user settings to identify problems within multiple attributes.
0.049383 - (4) While there are NP input parameters P, MNN adopts NI inputs for an MNN module.
0.038462 - Each XI selects a P as: (5) This input XI selection is dominated by optimization.
0.066667 - Therefore, the MNN parameters that require optimized include: wK, wJK, and pIJK and XI.
0.064103 - Parameters that should be input include: NP, NI, NJ, NK, FK, and training / testing patterns.
0.065789 - The final MNN output O can be formed as: (6) For instance, an example O could be: (7) where f may serve as the user defined function.
0.065217 - Eq (7) represents the original concept and primary target of this study.
0.202899 - With the proposed MNN, NN learning will be able to both predict and program.
0.071429 - As mentioned above, optimization remains a challenge to MNN programming.
0.020833 - Genetic algorithm optimization The Genetic Algorithm (GA), which imitates parts of the natural evolution process, was first proposed by (Holland, 1975).
0.061111 - GA is a stochastic search approach inspired by natural evolution that involves crossover, mutation, and evaluation of survival fitness.
0.023810 - Genetic operators work from initial generation to offspring in order to evolve an optimal solution through generations.
0.100000 - Each individual of a generation generates a result for the problem, which is represented as a string-named chromosome.
0.054054 - This relatively straightforward and simple implementation procedure gives GA exceptional flexibility to hybridize with domain-dependent heuristics to create effective implementation solutions tailored to specific problems.
0.053763 - Based on these merits, the potential to use of GA in optimization techniques has been studied intensively (Gen & Cheng, 1997).
0.085859 - However, simple GA is difficult to apply directly and successfully to a large range of difficult-to-solve optimization problems (Michalewicz, 1996).
0.050505 - MATLAB was employed in this study because it is a powerful tool that incorporates various function sets, including genetic algorithm (ga function).
0.078947 - To study how the MATLAB ga function works is essential due to the numerous parameter settings that must be set properly in order to ensure correct results.
0.033333 - The five basic steps to using GA are: 1.
0.000000 - Initialize population
0.066667 - Value boundaries are set as integer 1 ∼ NP for XI selections and float −10 ∼ 10 for all coefficient constants.
0.000000 - Evaluate individuals
0.017544 - A large fitness value indicates a strong individual.
0.091954 - In this study, a target function that is an inverse of the fitness function will be discussed later.
0.000000 - Perform crossover
0.100000 - To create crossover children, a function must be selected.
0.040230 - This study performed a scattered (function of @corssoverscattered in MATLAB) crossover with a crossover rate of 0.8.
0.000000 - Perform mutation
0.053333 - The MATLAB function @mutationuniform with a mutation rate at 0.05 was used herein.
0.000000 - Select individuals
0.053333 - In additional, five elitist individuals were guaranteed to survive to the next generation herein.
0.047619 - The author took four quantities as the target function (TF).
0.058824 - One was the training root mean square error (RMSEtr) and the others were the effects of pIJK, wJK, and wK coefficients (Tsai, 2010).
0.041667 - Individual health (fitness function) correlates inversely to TF quantity.
0.065657 - (8) (9) (10) (11) where C1, C2, and C3, are hyper-parameters leveraged to balance the bilateral impact between errors and coefficients.
0.044444 - VPIJ, VWJ and VWK encourage all coefficients to approach zero and become normalized by their respective numbers of coefficients.
0.000000 - With this designed TF, MNN always identifies optimal results under simple linkages.
0.015873 - Such a criterion prevents an over-fitting of resultant models.
0.089286 - This was proposed previously for a Bayesian neural network (MacKay, 1995), but only for weights that were not exponents.
0.077778 - Settings for the hyper-parameter were determined under conditions in which the effects of RMSEtr on TF exceeded 70%.
0.077778 - Although particular settings for hyper-parameters may be meaningful, this study adopted the same value for all coefficients currently.
0.130303 - Baykasoglu, Oztas, and Ozbay (2009) gathered 104 high strength concrete datasets.
0.048780 - Six parameters were selected as input parameters, including water-binder ratio (W/B), water (W), fine aggregate (s/a), fly ash (FA), air entraining agent (AE), and super-plasticizer (SP).
0.067901 - Output targets focused on concrete compressive strength (Strength), cost (Cost) and slump (Slump) (see Table 1).
0.037037 - Parameter settings are listed in Table 2.
0.085470 - The referenced RMSEs for GP were 2 MPa, 0.2 $/m3, and 20.7 cm for compressive strength, cost, and slump, respectively, with GP (Baykasoglu et al., 2009).
0.115942 - The parameter settings for MNN and GA are listed in Table 2.
0.090278 - NK = 5 indicates that five modular functions have been adopted (five functions in Eq (4)).
0.108696 - Nine sets of NI and NJ combinations were selected for MNN topology.
0.043011 - Each MNN model was executed ten times to obtain optimal results depending on summation of final training and testing RMSEs.
0.000000 - Table 1.
0.105263 - Lower and upper bound for inputs and targets.
0.014706 - Factors Lower bound Upper bound P1: W/B,% 30 45 P2: W, kg/m3 160 180 P3: s/a,% 37 53 P4: FA,% 0 20 P5: AE, kg/m3 0.036 0.078 P6: SP, kg/m3 1.89 8.5 Strength, MPa 38 74 Cost, $/m3 35.02 66.88 Slump, cm 95 260 Table 2.
0.020833 - Summary of MNN parameter settings.
0.038462 - Setting MNN parameter NI 2, 4, 6 NJ 2, 4, 6 NK 5 NP 6 wk, wjk, and pijk ranges −10 ∼ 10 Datasets 80 of 104 for training 24 for testing GA parameter Population size 200 Crossover rate 0.8 Mutation rate 0.05 Elitist 5 Iterations 2,000 C1, C2, and C3 0.08 for strength 0.02 for cost 1.5 for slump 3.1.
0.094248 - Compressive strength The best analyzed results for concrete compressive strength are listed in Table 3, which show training RMSE, training R-square (R2), testing RMSE, and testing R2.
0.087302 - Average computation time for the ten trials is also listed.
0.000000 - Table 3.
0.068627 - Results of concrete compressive strength analysis.
0.000000 - Training RMSE (MPa); Training R2; Testing RMSE (Mpa); Testing R2; Avg.
0.011019 - CPU time (min) NJ 2 4 6 NI 2 1.519 1.579 1.676 0.974 0.972 0.969 1.945 1.555 1.927 0.953 0.970 0.954 1727 1743 1858 4 1.336 1.429 1.259 0.980 0.977 0.982 1.646 1.190 1.215 0.966 0.982 0.982 1721 2004 2142 6 1.354 1.343 1.236 0.980 0.980 0.983 1.426 1.806 1.828 0.975 0.959 0.958 1982 2208 2406 The referenced GP result is listed in Eq (12), with RMSE at 2 MPa (Baykasoglu et al., 2009).
0.057292 - (12) All MNN model results perform better than the GP’s in terms of RMSE and are attached with good R2.
0.043210 - In result sets where the NI equals 2, a large NJ does not improve result accuracy.
0.140000 - Such indicates that an inadequate number of input parameters were available for strength programming.
0.013889 - Results obtained with NI up to 4 improve with increasing NI or NJ.
0.114394 - However, such increases processing time required and raises programmed formula complexity.
0.098958 - While the proposed MNN was originally designed as a tool for artificial intelligence programming, this goal has not yet been achieved.
0.049020 - In Table 3, the result set {NI = 4, NJ = 2, NK = 5} delivers outstanding results in terms of RMSE, R2 and model complexity.
0.072917 - While a complex model topology may lead to accurate prediction results, it will form a complicated formula in terms of programming.
0.108991 - When {NI = 4, NJ = 2, NK = 5} is adopted, MNN programs high-strength concrete Strength as an expression of: (13) The MNN learning process is shown in Fig 3.
0.070513 - The curve of the target function decreases consistently during iterations and RMSEs trend toward minimization.
0.083333 - The coefficient effect is shown in all terms except training RMSE in Eq (8) and encourages MNN to identify values close to zero for all coefficients (see Fig 4).
0.066033 - Furthermore, in order to study the effects of modular functions, the modular function, y = log(x), dominates the final prediction, followed by y = x (see Fig 5).
0.081241 - This paper tried to prune down some of the MNN modules, and tried modular functions {y = x, y = log(x)}, i.e., NK = 2.
0.028986 - Unfortunately, the best trial result provided an RMSE at 2.2 MPa.
0.041667 - This result may be inadequate.
0.126588 - The reason may be attributable to the reflection by other modular functions for some of the sensitive variations under such high accuracy.
0.072264 - Consequently, this paper added another modular function, y = sin(x).
0.023392 - Results obtained by {NI = 4, NJ = 2, NK = 3} with FK = {y = x, y = log(x), y = sin(x)} were, 1.718 MPa, 0.967, 1.679 MPa, 0.965, 1301 min on training RMSE, training R2, testing RMSE, testing R2 and average CPU time, respectively.
0.096285 - The modular function, y = log(x), contributed significantly to the final prediction, while remaining functions accounted for tuning results (see Fig 6).
0.063889 - The final Strength formula was expressed as: (14) where {P1, P5, P6}, {P2, P6}, {P1, P3, P5, P6} contribute to modular outputs of y = x, y = sin(x), y = log(x), respectively, of which, P6 appears in all modular outputs and P4 is absent from the final Strength formula.
0.097222 - No doubt, P6 (super-plasticizer) greatly impacts upon the compressive Strength of high-strength concrete.
0.063492 - Parameter influences reflect input-output mapping or data attribute problems.
0.043860 - Additional datasets may change the influence of parameters.
0.051282 - It is hard to state unequivocally that any parameter does not contribute to final targets.
0.070513 - Compared to StrengthGP, this paper demonstrated that MNN represents an alternative in artificial intelligence programming.
0.037037 - Strength target values during iterations Fig 3.
0.041667 - Strength target values during iterations.
0.098039 - Coefficient values for strength Fig 4.
0.111111 - Coefficient values for strength.
0.058824 - Module impacts on strength Fig 5.
0.066667 - Module impacts on strength.
0.055556 - Final module impacts on strength Fig 6.
0.062500 - Final module impacts on strength.
0.057143 - Cost The best analyzed results of concrete cost are listed in Table 4, along with training RMSE, training R2, testing RMSE and testing R2.
0.063492 - Average computation times for the ten trials are also listed.
0.000000 - Table 4.
0.116071 - Analyzed results for concrete cost.
0.002525 - Training RMSE ($/m3); Training R2; Testing RMSE ($/m3); Testing R2; CPU time (min) NJ 2 4 6 NI 2 0.536 0.409 0.451 0.998 0.999 0.998 0.450 0.402 0.608 0.998 0.999 0.997 1821 1832 1969 4 0.347 0.428 0.296 0.999 0.998 0.999 0.271 0.357 0.274 0.999 0.999 0.999 1808 2156 2210 6 0.445 0.453 0.299 0.998 0.998 0.999 0.481 0.466 0.257 0.998 0.998 0.999 2047 2265 2469 Referenced GP results are listed in Eq (15), with an RMSE of 0.2 $/m3 (Baykasoglu et al., 2009).
0.014493 - No MNN model result performs better than GPs in terms of RMSE.
0.000000 - However MNN models present outstanding R2 values.
0.089286 - Similar to previous statements for Strength, MNN results may be improved by increasing the NI or NJ.
0.013889 - However, any increase in NI or NJ number will require greater processing time.
0.081773 - (15) Under {NI = 4, NJ = 2, NK = 5}, MNN programs high-strength concrete Cost as an expression of: (16) Under the iteration process, parameters shown in Fig 7 all target on minimization.
0.089286 - Two modular functions {y = x, y = log(x)} and the bias term {y = C} all affect Cost predictions significantly.
0.000000 - (see Fig 8).
0.054029 - Furthermore, this paper also tried modular functions of {y = x, y = log(x)}, i.e., NK = 2.
0.037037 - The MNN successfully predicted Cost with training RMSE, training R2, testing RMSE, testing R2, and average CPU time at 0.501 $/m3, 0.998, 0.493 $/m3, 0.998, and 178.1 min, respectively.
0.050505 - Final functional effects are shown in Fig 9, where both {y = x, y = log(x)} clearly contribute to this MNN Cost prediction.
0.083333 - The final Cost formula is: (17) where {P1, P2} and {P3, P6} participate in the modular output of y = x and y = log(x), respectively.
0.053922 - While this paper does not assert that P4 and P5 are not related to Cost, the influences of these two variables are minor.
0.061728 - It may be that current datasets do not adequately reflect the contributions of P4 and P5.
0.000000 - Further testing using other datasets can help clarify this issue.
0.073529 - Compared to CostGP, while the studied Cost presented a somewhat disappointing level of accuracy, it delivered a reliable R2 and a beautiful formula.
0.127714 - Certainly, this case study proves the abilities of MNN in terms of both artificial intelligence prediction and programming.
0.000000 - Cost target values during iterations Fig 7.
0.000000 - Cost target values during iterations.
0.019608 - Module impacts on cost Fig 8.
0.022222 - Module impacts on cost.
0.018519 - Final module impacts on cost Fig 9.
0.020833 - Final module impacts on cost.
0.030303 - Slump The referenced GP Slump prediction has an RMSE of 20.7 cm in training without testing results (Baykasoglu et al., 2009).
0.041667 - MNN Slump results are listed in Table 5 with training RMSE, training R2, testing RMSE, testing R2, and average CPU time.
0.039216 - When an MNN of {NI = 4, NJ = 4, NK = 5} is adopted, result accuracy approximated that of GP’s Slump result in training.
0.091398 - However, as none of the R2 results were adequate, this paper abandoned this case study for the MNN programming process.
0.062500 - Civil engineers appreciate that concrete slump is a valuable index of concrete workability.
0.100000 - Unfortunately, obtaining/learning this index is difficult both in terms of experimentation and prediction.
0.067901 - Further research is needed, and the practical results of this case study reached their potential already.
0.000000 - Table 5.
0.000000 - Analyzed results of slump.
0.000000 - Training RMSE (cm); Training R2; Testing RMSE (cm); Testing R2; CPU time (min) NJ 2 4 6 NI 2 23.797 23.691 23.587 0.350 0.356 0.361 25.618 23.468 24.842 0.536 0.611 0.564 1827 1817 1948 4 23.401 20.881 20.891 0.371 0.500 0.499 24.911 27.595 25.499 0.561 0.462 0.540 1806 2123 2237 6 21.804 20.760 18.206 0.454 0.505 0.620 26.806 25.514 23.741 0.492 0.540 0.602 2074 2298 2489
0.109375 - Significant functions should be studied further in order for results to be applied to particular problems (like f in Eq (6)).
0.111111 - The NI and NJ used in this study were fixed.
0.080741 - This may be improved by using different numbers of MNN inputs to reduce programmed formula length.
0.048611 - Besides, the present MNN is a multi-inputs-single-output (MISO) mapping tool.
0.089744 - To improve MNN for multi-inputs-multi-outputs (MIMO) problems is essential in order to create a set of MNN formulas with modular relationship components (see Fig 10).
0.055556 - Of course, an MIMO problem may be trained via repeated MISO learning executions.
0.058824 - MISO and MIMO MNN Fig 10.
0.066667 - MISO and MIMO MNN.
0.050314 - As defined by (Tsai, 2009), the currently used MNN modules, which are three-layered HONN models, can be represented as a type of PW layer connection of which P indicates a high order layer connection and W denotes a linear layer connection.
0.039717 - Certainly, some HONN derivatives (e.g., WP, PWW, WPW, WWP, polynomial neural networks (Fazel Zarandi, Türksen, Sobhani, & Ramezanianpour, 2008), or other NN derivatives) may be employed.
0.068939 - Such modifications will allow execution of different types of programmed formulas.
0.088137 - In previous sections, each modular function type was used once.
0.081884 - However the same function can be employed several times to program a function (O) such as: (18) Swarm intelligence approaches (e.g., particle swarm intelligence, fish swarm algorithms) may be employed to improve MNN optimization in terms of accuracy and computational efforts.
0.071429 - Such improvements can facilitate the development of different programmed function types, which depend on various MNN designs.
0.062500 - Work in this paper has focused only on developing the MNN programming concept.
0.199176 - This paper proposed a modular neural network (MNN) concept applicable to both artificial intelligent prediction and programming.
0.135150 - The resulting programmed formulas are comprehensive in terms of their mathematic functions, weights and exponents.
0.156593 - MNN greatly improves the abilities of neural networks (even high order neural network) to predict and program.
0.120718 - Case studies focused on parameters specific to high-strength concrete.
0.075758 - The merits of MNN in comparison to genetic programming include: 1.
0.108150 - The modular approach of MNN makes it relatively easy to study the attributes of problems, and users may adopt any modular function type.
0.160110 - After MNN learning, significant modular functions can be identified for particular problems.
0.181739 - High order neural networks provide MNN programming to program problems in meaningful function types.
0.077839 - Sequentially, MNN both improves on previous high order neural networks and creates an NN derivative against GP.
0.128860 - To modify the number of NI, NJ, and NK, a simple formula can be programmed and parameter influence can be studied with good accuracy.
0.180263 - Finally, this paper states that the proposed MNN is a useful branch of artificial intelligent programming which can be used for both problem prediction and programming problems with functions.
0.000000 - 1 Tel.
0.000000 - : +886 2 27301073; fax: +886 2 27301074.

[Frase 1] This study proposes a modular neural network (MNN) that is designed to accomplish both artificial intelligent prediction and programming.
[Frase 196] High order neural networks provide MNN programming to program problems in meaningful function types.
[Frase 199] Finally, this paper states that the proposed MNN is a useful branch of artificial intelligent programming which can be used for both problem prediction and programming problems with functions.
[Frase 8] The proposed MNN is a valid alternative approach to prediction and programming using artificial neural networks.
