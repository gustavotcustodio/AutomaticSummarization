The FCANN and JBOS approaches are used to extract knowledge from neural networks. Applying the JBOS approach to extract reduced knowledge. The reductions resulted in small number of formal concepts and rules for the final user.

0.104233 - Due to its ability to handle nonlinear problems, artificial neural networks are applied in several areas of science.
0.138889 - However, the human elements are unable to assimilate the knowledge kept in those networks, since such knowledge is implicitly represented by their connections and the respective numerical weights.
0.238297 - In recent formal concept analysis, through the FCANN method, it has demonstrated a powerful methodology for extracting knowledge from neural networks.
0.182371 - However, depending on the settings used or the number of the neural network variables, the number of formal concepts and consequently of rules extracted from the network can make the process of knowledge and learning extraction impossible.
0.309738 - Thus, this paper addresses the application of the JBOS approach to extracted reduced knowledge from the formal contexts extracted by FCANN from the neural network.
0.260559 - Thus, providing a small number of formal concepts and rules for the final user, without losing the ability to understand the process learned by the network.
0.071895 - The understanding of a chemical, economic or industrial process can be obtained through the study of the variables involved in the process.
0.084967 - As many real world problems involve a high complexity level in their domains and the understanding of the process becomes very hard.
0.063492 - A more natural alternative to make it understandable is through symbolic or qualitative representation, which expresses the cause-effect relation among the variables.
0.080808 - Artificial intelligence (AI) techniques have been proposed as an alternative to represent knowledge of real world systems without the necessity of a more detailed study about the physical principles of the process.
0.111436 - AI proposes some fields of representation, for example, the connectionist field, where artificial neural networks (ANN) are inserted, dealing with implicit, numerical or sub-symbolic knowledge.
0.070922 - In the last few years, several researches have shown the capacity of the ANN to represent the most different physical systems (Bittern, Cuschieri, Dolgobrodov, Marshall, & Moore, 2004; Zhangand & Trimble, 1996; Zárate et al., 2006).
0.090535 - ANN have a mathematical function that relates the input and output variables of a system.
0.042735 - For this reason they are considered powerful computational tools in machine learning (Haykin, 2008).
0.036176 - However, that characteristic turns ANN into a “black-box” (Benítez, Castro, & Requena, 1997; Towell & Shavlik, 1993) because no explanation can be attributed to them for decision making when in operation.
0.101587 - ANN relate values from the training sets-provided to them-and inform their responses, but information about the generation principles is left aside.
0.072072 - Thus, there is some implicit knowledge hard to be extracted, which, if extracted, can reinforce an intelligent machine in its mechanism to explain its conclusions.
0.205659 - Therefore, methods to extract and represent knowledge from ANN are necessary.
0.143864 - The extracted knowledge can be expressed through symbolic rules, allowing the knowledge of the problem into its real context.
0.141414 - Many researchers have been discussing the knowledge extraction from ANN.
0.077778 - Different methodologies have been presented for rule extraction.
0.133333 - Some methods extract rules through the analysis of the network structure (i.e.
0.000000 - weights, topology, activation function, etc.
0.178551 - ), while others extract rules analyzing the data set used to train the network.
0.155317 - For example the RULEX technique (Andrews & Geva, 2002) extracts prepositional rules from the weights of previously trained local cluster neural networks.
0.090535 - The authors show the feasibility of the technique for a great variety of problem domains.
0.186243 - There are also methods that use both, the network structure and the data set to extract the rules.
0.208309 - There are three strategies used to extract knowledge from a previously trained ANN.
0.134680 - The first one consists in the insertion of knowledge into the network, which does not need to be complete or correct.
0.143791 - Through the training process and considering the generalization capacity of the ANN, it is possible to recover and identify the incomplete knowledge.
0.080773 - This technique is called “Knowledge-based Neural Networks”-KNN (Towell, Shavlik, & Noordewier, 1990).
0.119658 - The second one consists in the training of the KNN using selected training sets.
0.114114 - Through this procedure, the incorrect rules used in the first training of the KNN will be corrected to become consistent with the selected training sets.
0.177778 - The third one consists in the knowledge extraction directly from the network structure.
0.094444 - This is extremely complex mainly due to the different configurations of the networks and the different activation functions that can be considered (Craven & Shavlik, 1995; Towell & Shavlik, 1993).
0.192400 - In recent years, a new approach for extracting knowledge from neural networks, based on formal concept analysis (FCA) (Ganter & Wille, 1999) was proposed.
0.103785 - The FCA is a branch of applied mathematics originally proposed for the analysis of structured data.
0.166131 - Vimieiro, Zárate, Pereira, and Vieira (2005) proposes its use for the extraction and representation of knowledge acquired by neural networks using a method called FCANN (formal concept artificial neural networks).
0.137060 - Recently, Zárate and Dias (2009) applied the FCANN method in the cold rolling process and extracted the process behavior through rules and analysis of concept lattices.
0.127946 - On the other hand, in Zárate, Dias, and Song (2008) the same method was applied to databases for different domains.
0.150837 - Despite having shown great potential to represent in a practical and easy way the knowledge learned by neural networks, the FCANN method has poor scalability.
0.111640 - Depending on the settings applied in the method, even a neural network with few variables can become intractable.
0.129630 - In this case, the limited scalability is caused by the process of rule extraction and construction of concept lattice, which are based on FCA.
0.095431 - However, this computational limitations can be overcome by techniques of reducing the complexity of the concept lattice and through the choice of construction algorithms and paradigms of suitable concept lattices for the application in mind (Dias, 2010).
0.125664 - Regarding the reduction techniques, they seek to reduce the “structural complexity” of the conceptual lattice, obtaining a lattice that exposes the essential and is easier to understand.
0.123132 - On the other hand, the choice of algorithms and paradigms for generating the formal concepts selects the best algorithm and paradigm for an application in mind.
0.123457 - In general, techniques for reducing the complexity of the concept lattices can be applied to the formal context or to the concept lattice itself.
0.074074 - For example, Gajdos, Moravec, and Snásel, 2004 and Cheung and Vogel, 2005 proposes the use of singular value decomposition (SVD), which is a technique capable of reducing the dimensionality of data masses.
0.107744 - Another very interesting technique for reducing the concept lattice based on the formal context was presented by Belohlavek and Vychodil (2009).
0.150538 - The authors propose to use the knowledge of the user to create restrictions on attributes of the formal context.
0.152873 - These restrictions, called AD-formulas (attribute-dependency formulas), are used during the generation of formal concepts and concept lattice construction to retain only the formal concepts conforming to them.
0.105848 - Another option to reduce the complexity of the concept lattice is to associate the concept of frequent items to frequent formal concepts (Stumme, Taouil, Bastide, Pasquier, & Lakhal, 2002).
0.086893 - A formal concept (X, Y) is considered frequent if, only if, sup(Y, G) ⩾ minSup, where sup(Y, G) (the support) is the number of objects in G which contains the attributes in Y and minSup is a minimal support provided as a variable by the user.
0.081481 - Another very interesting technique for reducing complexity is based on the concept of stability proposed by Kuznetsov (1990).
0.097561 - Unlike other methods of reduction, the stability method seeks to create an index for concepts, indicating how much the intent of the concept depends on the set of objects.
0.100853 - Using this index, one can assume a threshold at which all the formal concepts with lower values are removed.
0.115741 - In Dias and Vieira (2010) we propose a new method for reducing the complexity of the concept lattice, called junction based on objects similarity (JBOS), which seeks to replace groups of similar objects by representative one.
0.207937 - Thus, this study aims to train a neural network to learn a cold rolling process and apply the FCANN method to extract qualitative and quantitative knowledge from the neural network.
0.277369 - However, it is intended to apply the JBOS approach to the formal contexts extracted from the neural network through the FCANN method.
0.121212 - Providing this way, greater scalability and quality to the method.
0.113695 - As it will be seen, the knowledge extracted from the network can be summarized and synthesized without causing the loss of the physical behavior of the process learned by the network.
0.076389 - This summarized knowledge can be used for building expert systems or learning processes in a much easier and intuitive way.
0.121647 - In the next section the basic concepts of FCA sufficient for the understanding of the results of this paper are revised.
0.253527 - Next, at Sections 3 and 4 the FCANN and JBOS approaches are explained.
0.091954 - At Section 5 a case study is done and finally the conclusions are drawn on Section 6.
0.074074 - This very short review is mainly based on the notions and terminology exposed on Ganter and Wille (1999).
0.100184 - Formal concept analysis is a field of mathematics which was born in the early eighties (Wille, 1982).
0.082540 - Its main characteristic is knowledge representation by means of a (concept) lattice usually presented in the form of line diagram (or Hasse diagram).
0.106280 - In FCA, the initial data are presented as a formal context.
0.033097 - A formal context is a triplet (G, M, I), where G is a set of elements called objects, M is a set of elements called attributes and I ⊆ G × M is called an incidence relation.
0.068376 - If (g, m) ∈ I, one says that “the object g has the attribute m”.
0.065359 - A formal context is usually presented as a cross table where the objects are rows headers, the attributes are columns headers and there is a cross in row g and column m if and only if (g, m) ∈ I.
0.056790 - Table 1 shows an example of formal context.
0.000000 - Table 1.
0.071429 - Example of formal context.
0.056385 - Obj/Att a b c d e f 1 x x x 2 x 3 x x x x x 4 x x x x 5 x Given a set of objects A ⊆ G from a formal context (G, M, I), the set of attributes which are common to all those objects is termed A′.
0.093190 - Similarly, for a set B ⊆ M, B′ is the set of objects that have all the attributes from B.
0.047138 - That is to say A′ = {m ∈ M∣∀g ∈ A(g, m) ∈ I} and B′ = {g ∈ G∣∀m ∈ B(g, m) ∈ I}.
0.099711 - By using such derivation operators, the notion of formal concept is defined as a pair such that A′ = B and B′ = A, where A is called the extent and B the intent of the concept.
0.108679 - For example, from the formal context of Table 1, it can be seen that the pair is a formal concept with extent {3, 4} and intent {b, d, f}.
0.091089 - The set of formal concepts can be ordered by the partial order ⪯ such that for any two formal concepts (A1, B1) and (A2, B2), (A1, B1) ⪯ (A2, B2) if and only if A1 ⊆ A2 (equivalently, B2 ⊆ B1).
0.088889 - The set of concepts ordered by ⪯ constitutes a complete lattice (Davey & Priestley, 1990), the so called concept lattice.
0.094017 - The concept lattice obtained from a formal context (G, M, I) is denoted by .
0.086099 - Fig 1 presents the line diagram (Ganter & Wille, 1999) of the concept lattice originated from the formal context of Table 1.1 Each node in the line diagram represents a formal concept.
0.095238 - The objects are shown inside white boxes drawn bellow some concept nodes and the attributes inside gray boxes drawn above some concept nodes.
0.097466 - The boxes are distributed in such a way that the extent of a concept is obtainable by collecting all objects from the concept node to the lattice infimum, and its intent is obtainable by visiting all attributes from the concept node to the lattice supremum.
0.135531 - The labeling can also be explained by using the notions of object concept and attribute concept.
0.101587 - Given an object g, γ g is the object concept (g″, g′), and μm, for an attribute m, is the attribute concept (m′, m″).
0.102407 - Then, the labeling of proceeds as follow: for each object g the formal concept γ g is labeled g and for attribute m the formal concept μm is labeled m. Concept lattice originated from the formal context Fig 1.
0.116959 - Concept lattice originated from the formal context.
0.068376 - The first part of the basic theorem(Wille, 1982) on concept lattices says that a concept lattice is a complete lattice in which for any arbitrary set the infimum and supremum are given by ⋀C = (⋂X, (⋃Y)″) and ⋁C = ((⋃X)″, ⋂Y), where X = {A∣(A, B) ∈ C} and Y = {B∣(A, B) ∈ C}.
0.224694 - In this section, the steps to extract knowledge from previously trained neural networks, discussed in Zárate et al.
0.000000 - (2008), will be summary presented.
0.126469 - The structure of the neural network corresponds to a multilayer perceptron, feedforward, totally connected with n inputs and m outputs (in this work m = 1).
0.145848 - As follows, the steps of the FCANN method are summary presented.
0.128205 - Select from the process a representative data set in order to train the ANN.
0.102407 - It is defined as: X = [xij]m × nwhere n is the number of variables; Xij to i = 1, … , m and j = 1, … , n − 1 are the input variables and xin to i = 1, … , m is the output variable.
0.017094 - This output variable should have a known probability distribution, for example a normal distribution .
0.111520 - Define the structure of the multi-layers neural network (with n input variables; h hidden layers and M outputs), and train it.
0.000000 - Build a synthetic database.
0.196296 - To extract knowledge from the network, a synthetic database has been generated considering the domain ranges of the input variables.
0.126744 - This synthetic database will be used to operate and stimulate the trained neural network and to obtain the output variable (explicit relation among variables) trying to reveal the knowledge learnt by it.
0.098765 - The database is defined as: Y = [yij] p×n−1where: Y has only elements generated with the purpose of knowledge extraction; p represents the records number (Eq (1)) and n the variables number.
0.079365 - Each input variable has minimal and maximal values, which defines the domain range of each variable.
0.050778 - The minimum and maximum values are vectors that are respectively defined as: inf = {u1, u2, … , un−1} and sup = {v1, v2, … , vn−1}The vector W defines the number of intervals that will be generated for each variable, between the minimum and maximum values: W = [Wj]; j = 1, … , n − 1 Hence, the variation interval of each variable, which composes the synthetic data set, is represented by the following expression: Int = {I1, I2, … , In−1} where for j = 1, … , n − 1.
0.089259 - The values of each variable used to generate the synthetic database can be represented by: It could be observed that the number of sets p that will be generated depends on the number of data of each variable.
0.000000 - So p can be defined as: (1) 4.
0.081871 - Present the synthetic database Y to the net in order to obtain the output variable Z = [zij]p×1 which should have the same probability distribution.
0.104575 - For example, to a normal distribution , it is possible to verify the net generalization through the comparison of the probability distributions and .
0.057348 - If and eS(x,z) = ∣S(x)1 − S(z)2∣ represent high errors, then, back to step 1.
0.082305 - Classify the variables (columns) of the matrix U = [Y, Z]p ×n into discrete intervals.
0.078431 - As the data considered by this method have continuous values, the better context to represent these data is the many-valued context.
0.070707 - Build a formal context cross table to classify in intervals (discretization) the n objects variables, establishing a binary relationship between objects and attributes (incidence) where an object has or not an attribute.
0.192704 - Obtain the formal concept and build the line diagram.
0.089835 - Apply some algorithm in order to obtain the rules set composed of rules of the type: “If… Then… ” For example, the Next Closure algorithm (Ganter & Wille, 1999) or the Find Implication algorithm (Carpineto & Romano, 2004).
0.074074 - During the study of a physical process, the analysis of a subset of variables involved may be required.
0.082126 - In this case, constant value can be assigned to some variables and vary the other, thus seeking to determine the qualitative behavior of the process as presented in Dias, Nogueira, and Zárate (2008).
0.091954 - This study can be accomplished by the method FCANN altering steps 3 and 6 that comprise it.
0.081197 - Be C the set of all the variable indices that is intended to be affixed, in this case, steps 3 and 6 can be re-defined as: 3 – According to the third step a synthetic database [S] must be built.
0.044444 - To assign a constant value to a subset C of variables do: if j ∈ C then Skj = q; for k = 1, … , wj; where uj ⩽ q ⩽ vj 6 – According to the sixth step a formal context (G, M, I) must be created, in this case make M = M′, where: M′ = {mi∣mi ∈ M; onde j ∈ C e i ≠ j}
0.093892 - In this section, the JBOS method, junction based on objects similarity, discussed in Dias and Vieira (2010) and Dias and Vieira (2013), will be summary presented.
0.170426 - The JBOS method is applied from the formal context.
0.067340 - It is based on the junction of “similar” objects, namely the replacement of such objects by a single representative of them.
0.090909 - Naturally, the key entity here is the notion of similarity.
0.103704 - For its definition, a weight is created for each attribute of the formal context, which seeks to represent the attribute’s relevance, and the similarity is determined from the weights of the attributes.
0.048611 - As the structure of the concept lattice is derived from the arrangements of attributes of objects, it is expected that the junction of similar objects will preserve at a certain degree (which will depend on the specific assigned weights) a portion of the original structure while providing a certain degree of simplification.
0.095238 - In the rest of this section, let (G, M, I) be a formal context from which a reduced formal context must be produced.
0.028674 - It is assumed that to each attribute m ∈ M is assigned a weight wm such that 0 ⩽ wm ⩽ 1.
0.104167 - Such weight can be thought as measuring the significance of the attribute from 0 (no relevance) to 1 (absolute relevance).
0.081481 - The weight should be defined by the user based on the semantic load of the attribute, i.e.
0.101852 - based on its importance to the model represented by the formal context.
0.051282 - The similarity between two objects g, h ∈ G will also be expressed by a real number between 0 (completely dissimilar) and 1 (completely indistinguishable), so defined: where, μ(g, h, m) = wm, if (g, m) ∈ I ↔ (h, m) ∈ I; 0, otherwise.
0.062222 - That is, the similarity between two objects g and h is given by the weighted sum of the weights of attributes in which both objects agree with each other (both have them or both do not have them).
0.074074 - From the similarity matrix it is possible to build an algorithm that makes clusters of similar objects, so that two objects can be in the same group only if they have a certain degree of “similarity”.
0.118056 - Accordingly, it is suggested the use of a similarity index ϵ, and a maximum number of similar elements α, both defined by the user.
0.087719 - Two objects g, h ∈ G will be considered similar and therefore are likely to stay in the same group if and only if sim(i, j) ⩾ ϵ.
0.101445 - The number α is just one more cutting option for the user: it specifies that each group can contain at most α elements.
0.045977 - In Dias and Vieira (2010) an algorithm was proposed and it will be considered in this work.
0.176435 - Let γ be the set of all clusters of objects constructed from G by the JBOS approach.
0.085470 - In the reduced formal context each set H ∈ γ will be considered as an object and such object will have the attributes of M which are common to all objects in H. That is, the attributes of H will be those in ⋂{g′∣g ∈ H}, where the operator of derivation is applied from I.
0.094057 - Therefore, by the JBOS method the reduced formal context (Gr, Mr, Ir) is such that: Gr = γ; Mr = ⋃{⋂{g′∣g ∈ H}∣H ∈ γ}; and (H, m) ∈ Ir if, and only if, m ∈ ⋂ {g′∣g ∈ H}.
0.063492 - As case study the cold rolling process was chosen.
0.098765 - In Zárate and Bittencout (2008) the representation neural of the rolling process was discussed and its results will be considered in this work.
0.091826 - One motivation for this case study refers to the fact that in the siderurgic industry the number of experts is smaller and the knowledge about the process should be passed to less experiment operators in such a simple and symbolic form, so that they understand the functioning principles of these systems.
0.101852 - There are several effects produced by the disturbances in the rolling process.
0.120000 - The objective here is to show the approach FCANN as a new alternative to know the behavior of the cold rolling process analyzing the influence of the variables on the rolling load; however, using formal context reduced by JBOS approach.
0.052678 - Neural representation The theoretical models for the cold rolling process allow calculating the rolling load by unit width (P) of non-linear expressions represented by equation: ; where hiinput thickness; ho output thickness; tb back tension; tf front tension; average yield stress; μ friction coefficient.
0.060606 - This equation involves non-linear relations that usually lead to a non-linear expression of difficult analytical and/or numerical solution.
0.071895 - Such is the case of the Alexander’s model (Alexander, 1972) considered as one of the most complete in the rolling theory.
0.044444 - That model requires a significant computational effort, which prevents its use in on-line control and supervision systems.
0.148807 - To obtain a database for the training and validation, the Alexander’s Model was used.
0.031192 - The nominal operational condition for the rolling process was defined as: • hi = 5.0 (mm) • ho = 3.6 (mm) • μ = 0.12 • tb = 89.22 (N/mm2) • tf = 4.325 (N/mm2) • y = 256.325 + 468.187 ϵ0.4275 • • P = 875,310 (N/mm2) Data for the material and the rolling mill were defined as: • E = 200054 (N/mm2) • v = 0.330 Poisson’s ratio of the strip • W = 500.0 (mm) • R = 292.1 (mm) • M = 4903.300 (N/mm) • g = 1.846 (mm) • Si = 250.556 (N/mm2) • Si = S0 = 534.900 (N/mm2).
0.074074 - Thus, the variables were varied as: hi = ± 8%, ho = ± 3%, μ = ± 20%, tb = ± 30%, tf = ± 30% and, as suggested in Bryant (1973).
0.054106 - Ten different values were chosen for each variable, resulting in 106 data sets.
0.077295 - The rolling load (P) was calculated through the Alexander’s model.
0.107280 - The representation of the cold rolling process through ANN was treated in Zárate and Bittencout (2008).
0.097749 - The neural network used correspond to a multi-layer, feedforward and totally connected with N = 6 inputs and M = 1 output, where the number of neurons in the hidden layer was chosen as 13 (2N + 1), as proposed in Hecht-Nielsen (1990) and Nielsen (1987).
0.100358 - As axon transfer function, the log-sigmoid function was chosen and the network was trained through back-propagation algorithm.
0.070117 - As inputs of the network, the input thickness (hi), the output thickness (ho), the friction coefficient (μ), the front tension (tf), the back tension (tb) and the average yield stress were considered; and as output, the rolling load (P) was considered resulted in: .
0.080460 - The following procedure was adopted to normalize the input data before using them in the network structure: (a) In order to improve convergence of the ANN training process, the normalization interval [0, 1] was reduced to [0.2, 0.8] (Altincay & Demirekler, 2002; Tarca & Cooke, 2005).
0.068687 - (b) The data were normalized through the following formula: Ln = (Lo − Lmin)/(Lmax − Lmin) and Lo = Ln∗Lmax + (1 − Ln)∗Lmin; where Ln is the normalized value, Lo is the value to normalize, Lmin and Lmax are minimum and maximum variable values, respectively.
0.026936 - (c) Lmin and Lmax were computed as follows: Lmin = (4∗ LimInf − LimSup)/3 and Lmax = (LimInf − 0.8∗Lmin)/0.2.
0.103704 - To obtain a representative set of the process to be the training set, in Zárate et al.
0.111111 - (2006) it is suggested some techniques to build small and better-defined training sets, maintaining the capacity of generalization of the network.
0.113810 - Among these, the expression n = (z/e)2 × (f × (1 − f)) has been borrowed from statistics area and has been used in this work to calculate a suitable size for the training set.
0.081871 - Where n is the size of the sample, z is the reliance level, e is the error around the average and f is the population proportion.
0.045977 - Fixing z in 90% (z = 1.645), f in 0.5 and e in 3%, n = 752.
0.111640 - It is important to emphasize that the value e is not the maximum error expected for the network.
0.111111 - The following procedure has been adopted to build trainings sets: (1) For each variable of the network, records containing the maximum and the minimum values have been selected.
0.105820 - (2) The operation point of the process (i.e.
0.137931 - the most representative variables in the data set) has been chosen and added to the training set.
0.069444 - (3) The remaining elements have been randomly selected, with the purpose of reaching the size n of the training set.
0.158025 - For the training process 752 data are used.
0.102694 - An error of 98 N/mm2 was applied for the Rolling load.
0.083333 - In the validation process all data that were not seen in the training process were used, that is 999,248.
0.153846 - The training showed satisfatory results and can be considered acceptable to represent the process.
0.115226 - Table 2 show statistical information of the error obtained in the training process and validation.
0.130359 - Because the goal of capturing process knowledge learned by the neural network, we chose a training and validation process to explore the maximum capacity of generalization of the network.
0.068376 - Therefore, we carried out the validation process with a sample of data as comprehensive.
0.000000 - Table 2.
0.069444 - Training and validation results.
0.000000 - Training Validation Data set 752 999,248 Minimum error 0.045 1.94E−05 Maximum error 96.831 159.965 Average error 15.576 15.42 Standard deviation error 9.4123 9.3477 5.2.
0.126455 - Extraction of knowledge by the FCANN method After a suitable training process, the FCANN method can be applied.
0.062986 - Using two data per variable and two discretization intervals,2 which results in a synthetic database with 64 records, an implications base, with 874 rules was obtained.
0.151884 - For the rule extraction the algorithm find implication (Carpineto & Romano, 2004) was used.
0.084656 - An example of these rules is shown to follow: .
0.111376 - As mentioned, when the number of data per variable or discretization intervals are increased further details are extracted from the ANN; however, this type of analysis is not always possible.
0.192400 - Table 3 the number of extracted formal concepts and rules for certain configurations of data per variable and discretization intervals can be observed.
0.158088 - Note that for 4 data per variable and intervals of discretization, the number of formal concepts extracted was greater than 29,000.
0.065844 - Also note that the visual analysis of a diagram of this magnitude is not feasible.
0.113947 - In relation to the set of rules, note that 4 data per variable and intervals of discretization, the number of rules obtained, again, is very high, making it difficult to use.
0.199914 - Note also that increasing the number of input variables and output of the neural network also affects the number of extracted formal concepts and rules.
0.000000 - Table 3.
0.217965 - Number of formal concepts and rules, (concepts, rules), obtained for the exploited variable data settings and discretization.
0.105522 - Discretization Data per variable 2 3 4 2 874–720 1324–459 1597–351 3 946–621 7077–3164 8460–2581 4 862–693 7254–3627 29,387–11,730 In Table 3 notice that even using only two data per variable and interval discretization the number of formal concepts and rules is high, thus making the process of knowledge extraction expensive.
0.164799 - Extraction of knowledge using reduced formal contexts from JBOS approach Analyzing a formal context, it is possible to observe that the number of formal concepts and rules is influenced by the number of objects G, the number of attributes M and the distribution of the incidence I (Dias, 2010).
0.124146 - Among these parameters, normally, the number of objects has a higher quantity.3 As discussed, an approach capable of reducing the dimensionality of formal contexts reducing the number of objects is the JBOS.
0.101506 - The application of the JBOS methodology demands prior knowledge of the semantic load of the attributes of the formal context, in other words it is necessary to determine a weight for each attribute of the formal context, which represents its importance in the process analysis.
0.135115 - The relation of the variables importance used in modeling the neural network, proposed by Zárate and Bittencout (2008), was considered.
0.100853 - Note that for the load P is the most important variable and the front tension tf less relevant variable.
0.193859 - Based on the relevance of the variables used in the neural model it is possible to generate an equivalent importance to the attributes and apply the JBOS approach to reduce the formal context extracted from the ANN by the FCANN method.
0.126984 - Thus, we applied the following set of weights: and .
0.115885 - Each attribute generated by the discretization process received the variable weight of corresponding neural network.
0.140605 - Applying a maximum limit of objects per group α = 10 and varying the threshold cutoff ϵ, it can be seen in Table 4 the number of objects ∣G∣, of formal concepts ∣C∣ and of rules ∣R∣ obtained from the formal context extracted from the neural network, using 2 data per variables and 2 discretization intervals.
0.000000 - Table 4.
0.212366 - Number of objects ∣G∣, formal concepts ∣C∣ and rules ∣R∣ for the ϵ variations extracted from the neural network with two data per variable and two discretization intervals.
0.000000 - ϵ ∣G∣ ∣C∣ ∣R∣ 1 64 874 720 0.9 63 860 702 0.8 57 743 623 0.7 53 691 591 0.6 39 348 406 0.5 29 207 271 0.4 23 107 175 0.3 18 48 91 0.2 13 23 65 0.1 11 17 58 5.3.1.
0.121365 - Extracting qualitative knowledge of the rolling process using reduced formal context As discussed in Section 3 section, it was possible to analyze, by the method FCANN, a subset of variables learned by the neural network, thus obtaining the qualitative relationship4 between the variables.
0.102872 - In Zárate and Dias (2009) the load behavior P and output thickness ho for variations ±20% of the friction coefficient μ was obtained by the FCANN method.
0.152021 - In this section, this same behavior will be confirmed; however, by using reduced conceptual lattices by the JBOS approach.
0.102053 - As it will be observed, even after the process of reducing the complexity of the formal context it will still be possible to observe the qualitative behavior of the rolling process learned by the neural network.
0.084565 - The process of analyzing the correlation between a subset of variables related by the neural network starts at applying constant values on the variables which are not analyzed during the process of building the synthetic data base for the operation of the neural network (Section 3).
0.086957 - In Table 5, the set of applied values can be observed.
0.092141 - Analyses were conducted to rolling load P and output thickness ho, for variations of friction coefficient (minimum, maximum and average are observed in the last line of Table 5).
0.000000 - Table 5.
0.119658 - Constant values applied during the analyses process of the variables subset learned by the neural network.
0.075299 - Average hi (mm) 5 tb (N/mm2) 43,245 tf (N/mm2) 891,905 4,594,045 Average Minimum Maximum μ 0.12 0.069 0.0144 In Table 6, the formal context (G, M, I) extracted by the FCANN method for the relationship between the load P and the output thickness ho applying a minimum friction coefficient μ is presented.
0.144199 - In the formal context G are the distinct operation points of the neural networks and M the variables discretized in 15 intervals.
0.086957 - Note that the formal context objects have few characteristics in common.
0.064103 - However, it can be seen the formation of two groups of objects, the first consisting of objects which the rolling load is in the interval P[5] and a group in which the load is in the interval P[6].
0.000000 - Table 6.
0.051013 - Formal context extracted by the FCANN method for the relationship between load P and output thickness ho applying the minimum friction coefficient μ. ho1 ho2 ho3 ho4 ho5 ho6 ho7 ho8 ho9 ho9 ho10 ho12 ho 13 ho14 ho15 P5 P6 0 x x 16 x x 32 x x 48 x x 64 x x 80 x x 96 x x 112 x x 128 x x 144 x x 160 x x 176 x x 192 x x 208 x x 224 x x In the Fig 2 one can observe the concept lattice originated from a formal concept from the Table 6, which was obtained by FCANN method for the relationship between the rolling load P and the output thickness ho applying a minimum friction coefficient μ without reducing the complexity.
0.061931 - It is observed in the lattice, as verified in the formal context, the formation of two distinct groups: the first relating the lowest rolling load P with the largest output thickness values ho and the second relating the highest rolling load P with the lowest output thickness values ho.
0.098448 - This behavior demonstrates the relationship between the variables learned by the neural network, which reflects the actual behavior of the rolling process.
0.107440 - Conceptual lattice obtained for the relationship between load P and thickness… Fig 2.
0.125000 - Conceptual lattice obtained for the relationship between load P and thickness output ho applying the minimum friction coefficient μ without reduction of complexity.
0.142685 - In Table 7, the formal context reduced by the JBOS approach (ϵ = 0.879 and α = 2) obtained for the relationship between rolling load P and output thickness ho, applying a minimum friction coefficient μ, can be observed.
0.111111 - The resulting formal context has not presented the same objects or attributes.
0.000000 - Objects or attributes with these features, i.e.
0.017778 - identical, can be removed without structural loss of conceptual lattice (Ganter & Wille, 1999).
0.076628 - We can note in the reduced formal context the existence of a small group of objects ({0} and {16}) formed by the load of greater value P[6] and a group of objects ({32, 48} and {224}) formed by the load of minor value P[5].
0.000000 - Table 7.
0.137956 - Formal Context reduced by the JBOS approach (ϵ = 0.879 and α = 2) obtained for the relationship between load P and output thickness ho applying a minimum friction coefficient μ after the removal of identical objects and attributes.
0.105135 - ho1 ho2 ho15 P5 P6 16 x x 32 48 x 224 x x 0 x x The Fig 3 presents the conceptual lattice reduced by the JBOS approach using ϵ = 0.879 and α = 2 obtained for the relationship between load P and output thickness ho applying a minimum friction coefficient μ, from the formal context shown in Table 7.
0.118519 - Again, note the formation of groups of objects as seen in the context and the original concept lattice.
0.024691 - Reduced conceptual Lattice (ϵ=0 Fig 3.
0.115055 - Reduced conceptual Lattice (ϵ = 0.879 and α = 2) obtained for the relationship between load P and output thickness ho applying the minimum friction coefficient μ.
0.090896 - Whereas the Fig 4 presents a reduced conceptual lattice (ϵ = 0.879 and α = 2) obtained for the relationship between load P and output thickness ho applying an average friction coefficient μ and Fig 5 the equivalent reduced concept lattice.
0.088164 - Finally, Fig 6 presents a reduced conceptual lattice (ϵ = 0.879 and α = 2) obtained for the relationship between load P and output thickness ho applying a maximum friction coefficient μ and Fig 7 the equivalent reduced conceptual lattice.
0.089119 - Notice that, as to the minimal friction change, the changes for the maximum and average values present, the formation of groups which show the relationship between the load and the thickness of output.
0.141849 - Furthermore, in the process of formal context reduction, and subsequent construction of the reduced lattices, the relationship between variables was preserved.
0.125217 - Concept lattice obtained for the relationship between load P and output… Fig 4.
0.111520 - Concept lattice obtained for the relationship between load P and output thickness ho applying an average friction coefficient μ without reduction of complexity.
0.049383 - Reduced concept lattice (ϵ=0 Fig 5.
0.111474 - Reduced concept lattice (ϵ = 0.879 and α = 2) obtained for the relationship between load P and the output thickness ho applying an average friction coefficient μ.
0.120370 - Conceptual lattice obtained for the relationship between load P and the output… Fig 6.
0.117647 - Conceptual lattice obtained for the relationship between load P and the output thickness ho applying a maximum friction coefficient μ without reduction of the complexity.
0.049383 - Reduced concept lattice (ϵ=0 Fig 7.
0.111474 - Reduced concept lattice (ϵ = 0.879 and α = 2) obtained for the relationship between load P and the output thickness ho applying a maximum friction coefficient μ.
0.202381 - With the set of reduced formal concepts and the equivalents it is also possible to extract rules that show the qualitative behavior of the process learned by the neural network.
0.115942 - In Fig 8 a sketch of the load behavior P to variations of ±20% in the friction coefficient μ and a set of rules obtained from the original lattice and the reduced can be observed.
0.104167 - Note that, the friction coefficient variations cause variations in rolling load that are reflected in the obtained set of rules.
0.114478 - For-20% in the friction coefficient μ, the rules indicate a rolling load in the interval of P[5] and P[6].
0.119658 - The average (medium) friction coefficient μ resulted in a rolling load between P[6] and P[7].
0.122358 - Finally, a change of +20% in the friction coefficient μ resulted in a load between P[7] and P[8].
0.094017 - Behavior of load P to variations of ±20 % in the friction coefficient μ Fig 8.
0.101852 - Behavior of load P to variations of ±20 % in the friction coefficient μ.
0.137931 - Also in the sketch shown in Fig 8 the reduced sets of extracted rules can be observed.
0.099617 - Note that, as in the original set of rules, the behavior of the process can be observed.
0.153021 - Furthermore, the number of rules is significantly smaller, which results in a much clearer and easier analysis of the process behavior learned by the neural network.
0.076389 - Many real-world problems involve a high complexity in its domain and the understanding these processes can be very complex.
0.080000 - An alternative is to study them through symbolic representations of cause and effect.
0.072039 - Artificial intelligence techniques have been proposed to this end, among them, artificial neural networks stand out.
0.143468 - In recent years, many studies have shown the ability to use neural networks for different applications.
0.103175 - They are capable of relating the variables involved in a process from a set of data.
0.053333 - However, little or no information is provided to understand how results were achieved.
0.162097 - In this scenario, obtaining good results using neural networks, but with inability to explain the results, several strategies for extracting the knowledge learned by neural networks emerged, among them, the FCANN method.
0.098551 - However, this approach is incapable of dealing with neural networks with many variables.
0.137368 - Moreover, even neural network with a few variables, depending on the level of detail explored, the number of relations extracted from the neural network can be impractical.
0.172920 - Thus, this study aimed to train a neural network to learn the process of cold rolling steel and apply the FCANN method to extract quantitative and qualitative understanding of the neural network.
0.148583 - However, we applied techniques to reduce complexity in the concept lattice and in the formal context created by the FCANN method.
0.143369 - Providing then, greater scalability and quality of the method, thereby extracting a more relevant knowledge and easy to analyze.
0.106996 - However, there was no loss in ability to understand the process learned by the network.
0.230801 - Even applying the JBOS approach for a formal context reduction, it was possible to understand the process learned by the neural network.
0.145711 - It was observed for the variation ±20% in the friction coefficient μ the relationship between the load P and the output thickness ho through a small number of rules and formal concepts.
0.182644 - The ability to reduce, summarize and synthesize the knowledge extracted from the neural network is relevant because it presents to the end-user knowledge easy to understand.
0.176413 - As a future work, it is intended to apply other approaches to reduce the formal context and concept lattice, for example, the clustering of formal concepts.
0.125463 - Additionally, it is intended to explore the ability of neural networks to provide the qualitative behavior of other industrial processes.
0.074074 - 1 All line diagrams in this paper were drawn using the Conexp software (Yevtushenko, 2000).
0.083627 - 2 The number of cut points for the discretization process is still an open question, whereas the number of data per variable should be enough to fill all the formal context, thus avoiding the loss of their representation or the creation of repeated objects.
0.061870 - 3 In real formal contexts, it is often observed a much larger number of objects than attributes.
0.105820 - 4 Concerning the quality or nature of the process.

[Frase 5] Thus, this paper addresses the application of the JBOS approach to extracted reduced knowledge from the formal contexts extracted by FCANN from the neural network.
[Frase 6] Thus, providing a small number of formal concepts and rules for the final user, without losing the ability to understand the process learned by the network.
[Frase 201] Based on the relevance of the variables used in the neural model it is possible to generate an equivalent importance to the attributes and apply the JBOS approach to reduce the formal context extracted from the ANN by the FCANN method.
[Frase 276] The ability to reduce, summarize and synthesize the knowledge extracted from the neural network is relevant because it presents to the end-user knowledge easy to understand.
[Frase 277] As a future work, it is intended to apply other approaches to reduce the formal context and concept lattice, for example, the clustering of formal concepts.
